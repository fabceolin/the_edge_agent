# Risk Profile: Story TEA-LTM-015

**Date:** 2025-01-18
**Reviewer:** Quinn (Test Architect)

---

## Executive Summary

- **Total Risks Identified:** 12
- **Critical Risks:** 1
- **High Risks:** 3
- **Medium Risks:** 5
- **Low Risks:** 3
- **Risk Score:** 53/100 (calculated)

This story introduces a complex distributed system with multiple failure modes. The combination of PostgreSQL catalog, cloud blob storage, and Parquet indexes creates a sophisticated architecture with significant data consistency and operational risks.

---

## Risk Matrix

| Risk ID  | Description | Probability | Impact | Score | Priority |
|----------|-------------|-------------|--------|-------|----------|
| DATA-001 | Blob-catalog consistency failure | High (3) | High (3) | 9 | **Critical** |
| DATA-002 | Orphan blob accumulation | Medium (2) | High (3) | 6 | High |
| TECH-001 | Closure table corruption on concurrent moves | Medium (2) | High (3) | 6 | High |
| SEC-001 | Tenant isolation bypass | Low (1) | High (3) | 3 | Low |
| PERF-001 | Index compaction blocking writes | High (3) | Medium (2) | 6 | High |
| PERF-002 | Metadata cache stale reads | Medium (2) | Medium (2) | 4 | Medium |
| OPS-001 | Multi-cloud storage configuration errors | Medium (2) | Medium (2) | 4 | Medium |
| OPS-002 | Connection pool exhaustion under load | Medium (2) | Medium (2) | 4 | Medium |
| TECH-002 | DuckDB in-memory pressure at scale | Medium (2) | Medium (2) | 4 | Medium |
| TECH-003 | fsspec provider compatibility variance | Medium (2) | Medium (2) | 4 | Medium |
| DATA-003 | Parquet delta file explosion | Low (1) | Medium (2) | 2 | Low |
| BUS-001 | Incomplete migration path from existing backends | Low (1) | Medium (2) | 2 | Low |

---

## Critical Risks Requiring Immediate Attention

### 1. DATA-001: Blob-Catalog Consistency Failure

**Score: 9 (Critical)**

**Probability: High** - The two-phase write pattern (blob first, then catalog) is inherently susceptible to partial failures. Network interruptions, process crashes, and timeout scenarios all create windows where blob exists without catalog entry or vice versa.

**Impact: High** - Data can become orphaned (wasting storage) or catalog can reference non-existent blobs (data loss perception). At 100GB+ scale with many writes, orphans accumulate quickly.

**Affected Components:**
- `HierarchicalLTMBackend.store()`
- `orphan_cleanup.py`
- PostgreSQL catalog tables

**Mitigation:**
| Strategy | Action |
|----------|--------|
| Preventive | Implement write-ahead logging before blob write |
| Detective | Add reconciliation job comparing catalog entries to storage |
| Corrective | Orphan cleanup job (AC-18) with aggressive monitoring |

**Testing Focus:**
- Chaos testing: Kill process between blob write and catalog update
- Network partition testing: Disconnect storage mid-write
- Verify orphan cleanup actually removes stale blobs

---

## High Risks

### 2. DATA-002: Orphan Blob Accumulation

**Score: 6 (High)**

**Probability: Medium** - Every partial failure creates an orphan. High-write workloads will accumulate orphans faster than cleanup can process.

**Impact: High** - Storage costs grow unbounded. At enterprise scale (100GB+ per tenant), this can lead to significant cloud billing surprises.

**Mitigation:**
- Tune `max_age_seconds` appropriately (default 1 hour may be too long)
- Monitor orphan count as a key metric
- Alert on orphan accumulation rate

---

### 3. TECH-001: Closure Table Corruption on Concurrent Moves

**Score: 6 (High)**

**Probability: Medium** - TEA-LTM-014 introduced `move_entity()`. Concurrent moves affecting overlapping subtrees can corrupt the closure table if not properly serialized.

**Impact: High** - Corrupted closure table means incorrect hierarchy queries, potential data exposure across tenants, and expensive manual repair.

**Affected Components:**
- `EntityHierarchy.move_entity()`
- `ltm_entity_closure` table

**Mitigation:**
- Advisory locks or `SELECT FOR UPDATE` on affected subtree during move
- Integration tests with concurrent move operations
- Audit log of all move operations

---

### 4. PERF-001: Index Compaction Blocking Writes

**Score: 6 (High)**

**Probability: High** - The delta file pattern means compaction must eventually run. At high write rates, compaction may fall behind or cause write latency spikes when acquiring locks.

**Impact: Medium** - Write latency increases during compaction. If compaction falls behind, reads become slower (scanning many delta files).

**Mitigation:**
- Implement background compaction with low-priority scheduling
- Add circuit breaker: pause compaction if write latency exceeds threshold
- Monitor delta file count per path

---

## Medium Risks

### 5. PERF-002: Metadata Cache Stale Reads

**Score: 4 (Medium)**

**Probability: Medium** - With 600-second TTL, recently deleted or moved entries may still appear in query results.

**Impact: Medium** - Stale data in multi-user scenarios causes confusion but not data loss.

**Mitigation:**
- Document eventual consistency behavior
- Provide cache invalidation API for critical operations

---

### 6. OPS-001: Multi-Cloud Storage Configuration Errors

**Score: 4 (Medium)**

**Probability: Medium** - Supporting GCS, S3, Azure, and local storage via fsspec means many configuration permutations to get wrong.

**Impact: Medium** - Misconfigured storage leads to write failures and potential data loss if pointed to wrong bucket.

**Mitigation:**
- Validate storage URI format on backend initialization
- Add connection test with health check endpoint
- Clear error messages for common misconfigurations

---

### 7. OPS-002: Connection Pool Exhaustion Under Load

**Score: 4 (Medium)**

**Probability: Medium** - With `pool_size: 10` default and parallel reads (`threads: 8`), high-concurrency workloads may exhaust connections.

**Impact: Medium** - Write/read failures under load, degraded user experience.

**Mitigation:**
- Right-size pool based on expected concurrency
- Add connection acquisition timeout with clear error
- Monitor pool utilization

---

### 8. TECH-002: DuckDB In-Memory Pressure at Scale

**Score: 4 (Medium)**

**Probability: Medium** - DuckDB is used in-memory for Parquet queries. Large indexes or many concurrent queries may exhaust memory.

**Impact: Medium** - Query failures or process crashes under memory pressure.

**Mitigation:**
- Consider persistent DuckDB database for large deployments
- Add memory limits and spill-to-disk configuration
- Document memory requirements

---

### 9. TECH-003: fsspec Provider Compatibility Variance

**Score: 4 (Medium)**

**Probability: Medium** - Different fsspec backends (gcsfs, s3fs, local) have subtle behavioral differences in atomicity guarantees, error codes, and metadata handling.

**Impact: Medium** - Code that works on GCS may fail on S3 or vice versa.

**Mitigation:**
- Abstract provider-specific behavior behind consistent interface
- Integration test matrix across all supported providers
- Document known provider-specific limitations

---

## Low Risks

### 10. SEC-001: Tenant Isolation Bypass

**Score: 3 (Low)**

**Probability: Low** - Isolation is enforced via closure table queries. Bypass requires SQL injection or logic error in access control integration (AC-12).

**Impact: High** - Cross-tenant data exposure is a severe security incident.

**Mitigation:**
- Parameterized queries only (no string interpolation)
- Security-focused code review on `allowed_ancestors` validation
- Penetration testing with multi-tenant scenarios

---

### 11. DATA-003: Parquet Delta File Explosion

**Score: 2 (Low)**

**Probability: Low** - With `max_deltas: 100` threshold, compaction should prevent runaway growth.

**Impact: Medium** - Many delta files slow reads significantly.

**Mitigation:**
- Monitor delta file count
- Auto-trigger emergency compaction above critical threshold

---

### 12. BUS-001: Incomplete Migration Path from Existing Backends

**Score: 2 (Low)**

**Probability: Low** - Story focuses on new backend, migration may be deferred.

**Impact: Medium** - Users stuck on old backends cannot adopt new features.

**Mitigation:**
- Document migration path for sqlite → hierarchical
- Provide migration utility (separate story)

---

## Risk Distribution

### By Category

| Category | Count | Critical | High | Medium | Low |
|----------|-------|----------|------|--------|-----|
| Data | 3 | 1 | 1 | 0 | 1 |
| Technical | 3 | 0 | 1 | 2 | 0 |
| Performance | 2 | 0 | 1 | 1 | 0 |
| Security | 1 | 0 | 0 | 0 | 1 |
| Operational | 2 | 0 | 0 | 2 | 0 |
| Business | 1 | 0 | 0 | 0 | 1 |

### By Component

| Component | Risk Count |
|-----------|------------|
| HierarchicalLTMBackend | 6 |
| IndexManager (Parquet) | 3 |
| PostgreSQL Catalog | 3 |
| EntityHierarchy | 2 |
| Blob Storage (fsspec) | 4 |

---

## Detailed Risk Register

| ID | Category | Title | Prob | Impact | Score | Strategy | Owner | Timeline |
|----|----------|-------|------|--------|-------|----------|-------|----------|
| DATA-001 | Data | Blob-catalog consistency | 3 | 3 | 9 | Preventive | Dev | Before deployment |
| DATA-002 | Data | Orphan accumulation | 2 | 3 | 6 | Detective | Dev | Before deployment |
| TECH-001 | Technical | Concurrent move corruption | 2 | 3 | 6 | Preventive | Dev | Before deployment |
| PERF-001 | Performance | Compaction blocking | 3 | 2 | 6 | Corrective | Dev | Before deployment |
| PERF-002 | Performance | Stale cache reads | 2 | 2 | 4 | Detective | Dev | Before deployment |
| OPS-001 | Operational | Storage config errors | 2 | 2 | 4 | Preventive | Dev | Before deployment |
| OPS-002 | Operational | Connection pool exhaustion | 2 | 2 | 4 | Detective | Dev | Before deployment |
| TECH-002 | Technical | DuckDB memory pressure | 2 | 2 | 4 | Corrective | Dev | After MVP |
| TECH-003 | Technical | fsspec compatibility | 2 | 2 | 4 | Preventive | Dev | Before deployment |
| SEC-001 | Security | Tenant isolation bypass | 1 | 3 | 3 | Preventive | Dev | Before deployment |
| DATA-003 | Data | Delta file explosion | 1 | 2 | 2 | Detective | Dev | After MVP |
| BUS-001 | Business | Migration path | 1 | 2 | 2 | Corrective | Dev | Future story |

---

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Tests

| Test | Risk ID | Type | Description |
|------|---------|------|-------------|
| CHAOS-001 | DATA-001 | Chaos | Kill process between blob and catalog write |
| CHAOS-002 | DATA-001 | Chaos | Network partition during store operation |
| INT-003 | DATA-001 | Integration | Verify catalog not updated on blob failure |
| INT-004 | DATA-001 | Integration | Verify orphan logged on catalog failure |
| INT-009 | DATA-002 | Integration | Orphan cleanup removes old blobs |

### Priority 2: High Risk Tests

| Test | Risk ID | Type | Description |
|------|---------|------|-------------|
| CONC-001 | TECH-001 | Concurrency | Parallel move_entity on overlapping subtrees |
| PERF-003 | PERF-001 | Performance | Write latency during compaction |
| LOAD-001 | PERF-001 | Load | High write rate delta accumulation |

### Priority 3: Medium/Low Risk Tests

| Test | Risk ID | Type | Description |
|------|---------|------|-------------|
| CACHE-001 | PERF-002 | Integration | Stale read after delete within TTL |
| MULTI-001 | OPS-001 | Integration | Invalid storage URI rejected at init |
| POOL-001 | OPS-002 | Load | Pool exhaustion under concurrent load |
| SEC-001 | SEC-001 | Security | Attempted cross-tenant query blocked |

---

## Risk Acceptance Criteria

### Must Fix Before Production

- [ ] DATA-001: Blob-catalog consistency (Critical)
- [ ] DATA-002: Orphan cleanup must be functional
- [ ] TECH-001: Concurrent move serialization
- [ ] PERF-001: Compaction must not block writes excessively
- [ ] SEC-001: Tenant isolation enforcement verified

### Can Deploy with Mitigation

- [ ] PERF-002: Document eventual consistency (TTL-based cache)
- [ ] OPS-001: Validation at startup with clear error messages
- [ ] OPS-002: Monitor and alert on pool utilization

### Accepted Risks

- DATA-003: Delta file explosion (monitoring in place)
- BUS-001: Migration path (separate story planned)
- TECH-002: DuckDB memory (documented requirements)

---

## Monitoring Requirements

Post-deployment monitoring for:

| Metric | Risk | Alert Threshold |
|--------|------|-----------------|
| Orphan blob count | DATA-001, DATA-002 | > 1000 orphans |
| Orphan accumulation rate | DATA-001, DATA-002 | > 100/hour |
| Delta files per path | PERF-001, DATA-003 | > 50 |
| Write latency p99 | PERF-001 | > 500ms |
| Connection pool utilization | OPS-002 | > 80% |
| DuckDB memory usage | TECH-002 | > 80% of limit |
| Cross-tenant query attempts | SEC-001 | Any occurrence |

---

## Risk Review Triggers

Review and update risk profile when:

- [ ] Storage provider added (GCS → S3 → Azure)
- [ ] New hierarchy levels introduced
- [ ] Scale exceeds 100GB per tenant
- [ ] Concurrency requirements change
- [ ] Security audit findings
- [ ] Performance regression reported

---

## Integration with Quality Gates

Based on risk analysis:

- **Gate Recommendation:** CONCERNS
- **Rationale:** 1 critical risk (DATA-001 score 9) requires mitigation before production, but story includes AC-14 through AC-18 addressing this. Gate passes conditionally on those ACs being fully tested.
