# Test Design: Story 44.3 - Experiment Framework

Date: 2026-01-13
Designer: Quinn (Test Architect)

## Test Strategy Overview

- Total test scenarios: 42
- Unit tests: 24 (57%)
- Integration tests: 14 (33%)
- E2E tests: 4 (10%)
- Priority distribution: P0: 14, P1: 18, P2: 10

## Test Scenarios by Acceptance Criteria

### AC1: Experiment Runner - `run_experiment()` function executes agent against dataset

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-INT-001 | Integration | P0 | `run_experiment()` executes agent for each dataset item | Core functionality |
| 44.3-INT-002 | Integration | P0 | Experiment completes with correct item count | Completeness check |
| 44.3-INT-003 | Integration | P1 | Large dataset (100+ items) executes successfully | Scale validation |
| 44.3-UNIT-001 | Unit | P1 | `ExperimentConfig` validates required fields | Config validation |
| 44.3-UNIT-002 | Unit | P2 | Empty dataset produces empty result (no error) | Edge case |

### AC2: Dataset Loading - Load test datasets from JSON fixtures

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-UNIT-003 | Unit | P0 | `Dataset::from_json_file()` loads valid JSON file | Core file loading |
| 44.3-UNIT-004 | Unit | P0 | `Dataset::from_json_str()` parses inline JSON string | Core string parsing |
| 44.3-UNIT-005 | Unit | P0 | Nested JSON structures preserved in `input` field | Complex data support |
| 44.3-UNIT-006 | Unit | P1 | Missing `expected_output` defaults gracefully | Optional field handling |
| 44.3-UNIT-007 | Unit | P1 | `metadata` field parsed as HashMap | Metadata support |
| 44.3-UNIT-008 | Unit | P1 | Invalid JSON returns descriptive error | Error messaging |
| 44.3-UNIT-009 | Unit | P2 | Large JSON file (1MB+) loads without OOM | Memory safety |
| 44.3-UNIT-010 | Unit | P2 | Unicode in JSON fields handled correctly | Internationalization |

### AC3: Custom Metrics - `Metric` trait allows pluggable scoring

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-UNIT-011 | Unit | P0 | Custom `Metric` implementation callable by runner | Extensibility |
| 44.3-UNIT-012 | Unit | P0 | `Metric::score()` receives output and expected | Correct parameters |
| 44.3-UNIT-013 | Unit | P1 | `MetricResult` with `score`, `passed`, `details` | Result structure |
| 44.3-UNIT-014 | Unit | P1 | Multiple metrics applied to single item | Multi-metric support |
| 44.3-UNIT-015 | Unit | P2 | Metric returning NaN handled gracefully | Edge case |

### AC4: Built-in Metrics - ExactMatch, ContainsMatch, NumericTolerance

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-UNIT-016 | Unit | P0 | `ExactMatch` returns 1.0 for identical values | Core metric |
| 44.3-UNIT-017 | Unit | P0 | `ExactMatch` returns 0.0 for different values | Core metric |
| 44.3-UNIT-018 | Unit | P0 | `ContainsMatch` returns 1.0 when output contains expected | Core metric |
| 44.3-UNIT-019 | Unit | P0 | `ContainsMatch` case-sensitive option works | Config option |
| 44.3-UNIT-020 | Unit | P1 | `NumericTolerance` returns 1.0 within tolerance | Numeric comparison |
| 44.3-UNIT-021 | Unit | P1 | `NumericTolerance` returns 0.0 outside tolerance | Numeric comparison |
| 44.3-UNIT-022 | Unit | P1 | `JsonPathMatch` extracts and compares nested values | Path-based matching |
| 44.3-UNIT-023 | Unit | P2 | `ExactMatch` handles null/undefined values | Edge case |
| 44.3-UNIT-024 | Unit | P2 | `ContainsMatch` handles empty strings | Edge case |

### AC5: Result Aggregation - Per-item scores and aggregate statistics

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-UNIT-025 | Unit | P0 | Mean score calculated correctly across items | Core aggregation |
| 44.3-UNIT-026 | Unit | P0 | Min/max scores captured correctly | Range tracking |
| 44.3-UNIT-027 | Unit | P1 | Pass rate calculated (% >= threshold) | Quality metric |
| 44.3-UNIT-028 | Unit | P1 | Per-item timing recorded | Performance tracking |
| 44.3-UNIT-029 | Unit | P1 | Total duration reflects actual execution time | Duration accuracy |
| 44.3-INT-004 | Integration | P1 | Aggregates match manual calculation | Calculation verification |
| 44.3-UNIT-030 | Unit | P2 | Single item produces valid aggregates | Edge case |

### AC6: Error Handling - Single item failures don't crash experiment

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-INT-005 | Integration | P0 | Failed item captured in `errors` list | Error capture |
| 44.3-INT-006 | Integration | P0 | Experiment continues after item failure | Resilience |
| 44.3-INT-007 | Integration | P1 | Error message includes item index | Debugging support |
| 44.3-INT-008 | Integration | P1 | `failed_items` count accurate in aggregates | Count accuracy |
| 44.3-INT-009 | Integration | P2 | All items fail still produces valid result | Total failure scenario |

### AC7: Opik Integration - Experiments traced when enabled

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-INT-010 | Integration | P1 | `opik_enabled: true` creates experiment trace | Opik integration |
| 44.3-INT-011 | Integration | P1 | Each item execution appears as child span | Span hierarchy |
| 44.3-INT-012 | Integration | P1 | Metric scores included in span metadata | Metric visibility |
| 44.3-INT-013 | Integration | P2 | `opik_enabled: false` produces no traces | Disable option |
| 44.3-E2E-001 | E2E | P2 | Real Opik experiment visible in dashboard | Full integration |

### AC8: Progress Callback - Optional callback for progress reporting

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-INT-014 | Integration | P1 | Progress callback invoked for each item | Callback firing |
| 44.3-UNIT-031 | Unit | P1 | Callback receives (current, total) counts | Correct parameters |
| 44.3-UNIT-032 | Unit | P2 | No callback (None) doesn't cause error | Optional handling |
| 44.3-UNIT-033 | Unit | P2 | Callback exception doesn't crash experiment | Isolation |

### AC9: Async Support - Works with async agent execution

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-INT-015 | Integration | P0 | `run_experiment` is async and awaitable | Async signature |
| 44.3-INT-016 | Integration | P1 | Async agent execution completes correctly | Async flow |
| 44.3-E2E-002 | E2E | P1 | Real async YAML agent evaluates successfully | Full async test |
| 44.3-E2E-003 | E2E | P2 | Parallel item execution (if supported) | Concurrency |

## Metric Implementation Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 44.3-E2E-004 | E2E | P1 | Python-Rust metric parity (same results) | Cross-runtime consistency |

## Risk Coverage

| Risk | Mitigating Tests |
|------|------------------|
| Incorrect aggregate calculations | 44.3-UNIT-025-030, 44.3-INT-004 |
| Experiment crashes on single failure | 44.3-INT-005-009 |
| Built-in metrics produce wrong scores | 44.3-UNIT-016-024 |
| Dataset loading fails silently | 44.3-UNIT-003-010 |
| Async execution hangs | 44.3-INT-015-016, 44.3-E2E-002 |
| Opik integration breaks experiment | 44.3-INT-010-013 |
| Memory issues with large datasets | 44.3-UNIT-009, 44.3-INT-003 |

## Recommended Execution Order

1. **P0 Unit tests** - Metric correctness and dataset loading
2. **P0 Integration tests** - Runner core functionality
3. **P1 Unit tests** - Extended metric coverage
4. **P1 Integration tests** - Error handling and callbacks
5. **P2 tests** - Edge cases and polish
6. **E2E tests** - Full workflow validation

## Test Implementation Notes

### Unit Test Setup Pattern (Metrics)

```rust
#[cfg(test)]
mod metric_tests {
    use super::*;
    use serde_json::json;

    #[test]
    fn test_exact_match_identical_values() {
        let metric = ExactMatch;
        let output = json!({"answer": "Paris"});
        let expected = json!({"answer": "Paris"});

        let result = metric.score(&output, &expected);

        assert_eq!(result.score, 1.0);
        assert!(result.passed);
    }

    #[test]
    fn test_exact_match_different_values() {
        let metric = ExactMatch;
        let output = json!({"answer": "London"});
        let expected = json!({"answer": "Paris"});

        let result = metric.score(&output, &expected);

        assert_eq!(result.score, 0.0);
        assert!(!result.passed);
    }

    #[test]
    fn test_contains_match_case_insensitive() {
        let metric = ContainsMatch { case_sensitive: false };
        let output = json!("The answer is PARIS");
        let expected = json!("paris");

        let result = metric.score(&output, &expected);

        assert_eq!(result.score, 1.0);
    }

    #[test]
    fn test_numeric_tolerance_within_range() {
        let metric = NumericTolerance { tolerance: 0.01 };
        let output = json!(3.14159);
        let expected = json!(3.14);

        let result = metric.score(&output, &expected);

        assert_eq!(result.score, 1.0);
    }
}
```

### Integration Test Pattern (Experiment Runner)

```rust
#[tokio::test]
async fn test_experiment_runner_basic() {
    let dataset = Dataset::from_json_str(r#"{
        "name": "test",
        "items": [
            {"input": {"x": 1}, "expected_output": {"y": 2}},
            {"input": {"x": 2}, "expected_output": {"y": 4}}
        ]
    }"#).unwrap();

    // Mock agent that doubles input
    let config = ExperimentConfig {
        agent_yaml: mock_doubler_yaml(),
        dataset,
        metrics: vec![Box::new(ExactMatch)],
        progress_callback: None,
        opik_enabled: false,
    };

    let result = run_experiment(config).await.unwrap();

    assert_eq!(result.items.len(), 2);
    assert_eq!(result.aggregates.total_items, 2);
    assert_eq!(result.aggregates.failed_items, 0);
}
```

### Error Handling Test Pattern

```rust
#[tokio::test]
async fn test_experiment_continues_after_failure() {
    let dataset = Dataset::from_json_str(r#"{
        "name": "test",
        "items": [
            {"input": {"fail": true}, "expected_output": {}},
            {"input": {"fail": false}, "expected_output": {"result": "ok"}}
        ]
    }"#).unwrap();

    // Agent that fails on "fail: true"
    let config = ExperimentConfig {
        agent_yaml: mock_failing_agent_yaml(),
        dataset,
        metrics: vec![Box::new(ExactMatch)],
        progress_callback: None,
        opik_enabled: false,
    };

    let result = run_experiment(config).await.unwrap();

    assert_eq!(result.errors.len(), 1);
    assert_eq!(result.errors[0].0, 0); // First item failed
    assert_eq!(result.items.len(), 1); // Second item succeeded
    assert_eq!(result.aggregates.failed_items, 1);
}
```

### Dataset Loading Test Pattern

```rust
#[test]
fn test_dataset_from_json_file() {
    let temp_file = tempfile::NamedTempFile::new().unwrap();
    std::fs::write(temp_file.path(), r#"{
        "name": "file_test",
        "items": [
            {"input": {"q": "test"}, "expected_output": {"a": "answer"}}
        ]
    }"#).unwrap();

    let dataset = Dataset::from_json_file(temp_file.path()).unwrap();

    assert_eq!(dataset.name, "file_test");
    assert_eq!(dataset.items.len(), 1);
}
```

## Quality Checklist

- [x] Every AC has test coverage
- [x] Test levels are appropriate (unit for metrics, integration for runner)
- [x] No duplicate coverage across levels
- [x] Priorities align with business risk (core metrics = P0)
- [x] Test IDs follow naming convention (44.3-LEVEL-SEQ)
- [x] Scenarios are atomic and independent

## JSON Dataset Fixture Example

For integration/E2E tests, use this standard fixture format:

```json
{
  "name": "qa_test_dataset",
  "description": "Standard QA test cases for experiment framework",
  "items": [
    {
      "input": {"query": "What is 2+2?"},
      "expected_output": {"answer": "4"},
      "metadata": {"category": "math", "difficulty": "easy"}
    },
    {
      "input": {"query": "Capital of France?"},
      "expected_output": {"answer": "Paris"},
      "metadata": {"category": "geography", "difficulty": "easy"}
    },
    {
      "input": {"query": "Solve: 3x + 5 = 20"},
      "expected_output": {"answer": "x = 5"},
      "metadata": {"category": "math", "difficulty": "medium"}
    }
  ]
}
```

Save to: `rust/tests/fixtures/qa_test_dataset.json`
