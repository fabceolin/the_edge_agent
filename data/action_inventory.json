{
  "generated_at": "2026-01-26T02:56:16.882435+00:00",
  "git_commit": "8eb000e",
  "total_actions": 482,
  "modules": [
    {
      "file": "a2a_actions.py",
      "namespace": "a2a",
      "actions": [
        {
          "name": "a2a.send",
          "function": "a2a_send",
          "parameters": [
            {
              "name": "to",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "message",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "confirm",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "correlation_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Send a message to a specific agent.\n\nArgs:\n    state: Current workflow state\n    to: Recipient agent ID\n    message: Message with 'type' and 'payload' fields\n    confirm: Wait for delivery confirmation\n    correlation_id: Optional correlation ID for request/response\n    ttl: Time-to-live in seconds\n\nReturns:\n    Dict with 'a2a_message_sent' and optional 'a2a_message_id'\n\nExample YAML:\n    action: a2a.send\n    with:\n      to: coordinator\n      message:\n        type: status_update\n        payload:\n          progress: \"{{ state.progress }}\"",
          "line_number": 97
        },
        {
          "name": "a2a.receive",
          "function": "a2a_receive",
          "parameters": [
            {
              "name": "from_agents",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "'0s'"
            },
            {
              "name": "require_all",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Receive messages from agents.\n\nArgs:\n    state: Current workflow state\n    from_agents: Optional list of sender agent IDs to filter\n    type: Optional message type filter\n    timeout: Timeout string (e.g., \"30s\", \"5m\")\n    require_all: Wait for messages from ALL specified agents\n\nReturns:\n    Dict with 'a2a_messages' list\n\nExample YAML:\n    action: a2a.receive\n    with:\n      from: [worker-1, worker-2]\n      type: task_result\n      timeout: 30s\n      require_all: true",
          "line_number": 156
        },
        {
          "name": "a2a.broadcast",
          "function": "a2a_broadcast",
          "parameters": [
            {
              "name": "message",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "agent_type_filter",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Broadcast message to all agents in namespace.\n\nArgs:\n    state: Current workflow state\n    message: Message with 'type' and 'payload' fields\n    agent_type_filter: Optional filter by agent type\n    ttl: Time-to-live in seconds\n\nReturns:\n    Dict with 'a2a_broadcast_count'\n\nExample YAML:\n    action: a2a.broadcast\n    with:\n      message:\n        type: announcement\n        payload:\n          text: \"All hands meeting at 3pm\"\n      agent_type_filter: worker",
          "line_number": 216
        },
        {
          "name": "a2a.delegate",
          "function": "a2a_delegate",
          "parameters": [
            {
              "name": "to",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "'60s'"
            },
            {
              "name": "on_timeout",
              "type": "str",
              "required": false,
              "default": "'raise'"
            },
            {
              "name": "fallback",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delegate a task to another agent and wait for response.\n\nArgs:\n    state: Current workflow state\n    to: Delegate agent ID\n    task: Task with 'type' and other fields\n    timeout: Timeout string (e.g., \"60s\")\n    on_timeout: Strategy: 'raise', 'fallback_local', 'retry'\n    fallback: Fallback action configuration (for fallback_local)\n\nReturns:\n    Dict with 'a2a_delegation_result'\n\nExample YAML:\n    action: a2a.delegate\n    with:\n      to: search-specialist\n      task:\n        type: search\n        query: \"{{ state.query }}\"\n      timeout: 60s\n      on_timeout: fallback_local\n      fallback:\n        action: web.search\n        with:\n          query: \"{{ state.query }}\"",
          "line_number": 266
        },
        {
          "name": "a2a.state.get",
          "function": "a2a_state_get",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get a value from shared state.\n\nArgs:\n    state: Current workflow state\n    key: State key\n    default: Default value if key doesn't exist\n\nReturns:\n    Dict with 'a2a_shared_state' and 'a2a_state_version'\n\nExample YAML:\n    action: a2a.state.get\n    with:\n      key: team_progress\n      default: {completed: 0, total: 0}",
          "line_number": 392
        },
        {
          "name": "a2a.state.set",
          "function": "a2a_state_set",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "value",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "ttl",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "expected_version",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Set a value in shared state.\n\nArgs:\n    state: Current workflow state\n    key: State key\n    value: Value to store\n    ttl: Time-to-live in seconds\n    expected_version: Expected version for optimistic locking\n\nReturns:\n    Dict with 'a2a_state_version'\n\nExample YAML:\n    action: a2a.state.set\n    with:\n      key: team_progress\n      value:\n        completed: \"{{ state.completed_tasks }}\"\n        total: \"{{ state.total_tasks }}\"\n      ttl: 3600",
          "line_number": 426
        },
        {
          "name": "a2a.discover",
          "function": "a2a_discover",
          "parameters": [
            {
              "name": "capability",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "status",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Discover available agents in namespace.\n\nArgs:\n    state: Current workflow state\n    capability: Optional capability filter\n    agent_type: Optional type filter\n    status: Optional status filter\n\nReturns:\n    Dict with 'a2a_agents' list\n\nExample YAML:\n    action: a2a.discover\n    with:\n      capability: summarize",
          "line_number": 482
        },
        {
          "name": "a2a.register",
          "function": "a2a_register",
          "parameters": [
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "capabilities",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Register current agent for discovery and broadcasts.\n\nArgs:\n    state: Current workflow state\n    agent_id: Optional override for agent ID\n    capabilities: List of capabilities\n    agent_type: Agent type for categorization\n    metadata: Additional metadata\n\nReturns:\n    Dict with 'a2a_registered' and agent info\n\nExample YAML:\n    action: a2a.register\n    with:\n      capabilities: [search, summarize]\n      agent_type: worker",
          "line_number": 521
        },
        {
          "name": "a2a.unregister",
          "function": "a2a_unregister",
          "parameters": [
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Unregister current agent.\n\nArgs:\n    state: Current workflow state\n    agent_id: Optional override for agent ID\n\nReturns:\n    Dict with 'a2a_unregistered'",
          "line_number": 574
        },
        {
          "name": "a2a.heartbeat",
          "function": "a2a_heartbeat",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Send heartbeat to update last_seen timestamp.\n\nReturns:\n    Dict with 'a2a_heartbeat' timestamp",
          "line_number": 603
        },
        {
          "name": "actions.a2a_send",
          "function": "a2a_send",
          "parameters": [
            {
              "name": "to",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "message",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "confirm",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "correlation_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Send a message to a specific agent.\n\nArgs:\n    state: Current workflow state\n    to: Recipient agent ID\n    message: Message with 'type' and 'payload' fields\n    confirm: Wait for delivery confirmation\n    correlation_id: Optional correlation ID for request/response\n    ttl: Time-to-live in seconds\n\nReturns:\n    Dict with 'a2a_message_sent' and optional 'a2a_message_id'\n\nExample YAML:\n    action: a2a.send\n    with:\n      to: coordinator\n      message:\n        type: status_update\n        payload:\n          progress: \"{{ state.progress }}\"",
          "line_number": 97
        },
        {
          "name": "actions.a2a_receive",
          "function": "a2a_receive",
          "parameters": [
            {
              "name": "from_agents",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "'0s'"
            },
            {
              "name": "require_all",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Receive messages from agents.\n\nArgs:\n    state: Current workflow state\n    from_agents: Optional list of sender agent IDs to filter\n    type: Optional message type filter\n    timeout: Timeout string (e.g., \"30s\", \"5m\")\n    require_all: Wait for messages from ALL specified agents\n\nReturns:\n    Dict with 'a2a_messages' list\n\nExample YAML:\n    action: a2a.receive\n    with:\n      from: [worker-1, worker-2]\n      type: task_result\n      timeout: 30s\n      require_all: true",
          "line_number": 156
        },
        {
          "name": "actions.a2a_broadcast",
          "function": "a2a_broadcast",
          "parameters": [
            {
              "name": "message",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "agent_type_filter",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Broadcast message to all agents in namespace.\n\nArgs:\n    state: Current workflow state\n    message: Message with 'type' and 'payload' fields\n    agent_type_filter: Optional filter by agent type\n    ttl: Time-to-live in seconds\n\nReturns:\n    Dict with 'a2a_broadcast_count'\n\nExample YAML:\n    action: a2a.broadcast\n    with:\n      message:\n        type: announcement\n        payload:\n          text: \"All hands meeting at 3pm\"\n      agent_type_filter: worker",
          "line_number": 216
        },
        {
          "name": "actions.a2a_delegate",
          "function": "a2a_delegate",
          "parameters": [
            {
              "name": "to",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "'60s'"
            },
            {
              "name": "on_timeout",
              "type": "str",
              "required": false,
              "default": "'raise'"
            },
            {
              "name": "fallback",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delegate a task to another agent and wait for response.\n\nArgs:\n    state: Current workflow state\n    to: Delegate agent ID\n    task: Task with 'type' and other fields\n    timeout: Timeout string (e.g., \"60s\")\n    on_timeout: Strategy: 'raise', 'fallback_local', 'retry'\n    fallback: Fallback action configuration (for fallback_local)\n\nReturns:\n    Dict with 'a2a_delegation_result'\n\nExample YAML:\n    action: a2a.delegate\n    with:\n      to: search-specialist\n      task:\n        type: search\n        query: \"{{ state.query }}\"\n      timeout: 60s\n      on_timeout: fallback_local\n      fallback:\n        action: web.search\n        with:\n          query: \"{{ state.query }}\"",
          "line_number": 266
        },
        {
          "name": "actions.a2a_state_get",
          "function": "a2a_state_get",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get a value from shared state.\n\nArgs:\n    state: Current workflow state\n    key: State key\n    default: Default value if key doesn't exist\n\nReturns:\n    Dict with 'a2a_shared_state' and 'a2a_state_version'\n\nExample YAML:\n    action: a2a.state.get\n    with:\n      key: team_progress\n      default: {completed: 0, total: 0}",
          "line_number": 392
        },
        {
          "name": "actions.a2a_state_set",
          "function": "a2a_state_set",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "value",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "ttl",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "expected_version",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Set a value in shared state.\n\nArgs:\n    state: Current workflow state\n    key: State key\n    value: Value to store\n    ttl: Time-to-live in seconds\n    expected_version: Expected version for optimistic locking\n\nReturns:\n    Dict with 'a2a_state_version'\n\nExample YAML:\n    action: a2a.state.set\n    with:\n      key: team_progress\n      value:\n        completed: \"{{ state.completed_tasks }}\"\n        total: \"{{ state.total_tasks }}\"\n      ttl: 3600",
          "line_number": 426
        },
        {
          "name": "actions.a2a_discover",
          "function": "a2a_discover",
          "parameters": [
            {
              "name": "capability",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "status",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Discover available agents in namespace.\n\nArgs:\n    state: Current workflow state\n    capability: Optional capability filter\n    agent_type: Optional type filter\n    status: Optional status filter\n\nReturns:\n    Dict with 'a2a_agents' list\n\nExample YAML:\n    action: a2a.discover\n    with:\n      capability: summarize",
          "line_number": 482
        },
        {
          "name": "actions.a2a_register",
          "function": "a2a_register",
          "parameters": [
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "capabilities",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Register current agent for discovery and broadcasts.\n\nArgs:\n    state: Current workflow state\n    agent_id: Optional override for agent ID\n    capabilities: List of capabilities\n    agent_type: Agent type for categorization\n    metadata: Additional metadata\n\nReturns:\n    Dict with 'a2a_registered' and agent info\n\nExample YAML:\n    action: a2a.register\n    with:\n      capabilities: [search, summarize]\n      agent_type: worker",
          "line_number": 521
        },
        {
          "name": "actions.a2a_unregister",
          "function": "a2a_unregister",
          "parameters": [
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Unregister current agent.\n\nArgs:\n    state: Current workflow state\n    agent_id: Optional override for agent ID\n\nReturns:\n    Dict with 'a2a_unregistered'",
          "line_number": 574
        },
        {
          "name": "actions.a2a_heartbeat",
          "function": "a2a_heartbeat",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Send heartbeat to update last_seen timestamp.\n\nReturns:\n    Dict with 'a2a_heartbeat' timestamp",
          "line_number": 603
        }
      ]
    },
    {
      "file": "academic_actions.py",
      "namespace": "academic",
      "actions": [
        {
          "name": "academic.pubmed",
          "function": "academic_pubmed",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "max_results",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "sort_by",
              "type": "str",
              "required": false,
              "default": "'relevance'"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search PubMed database for scientific articles via NCBI E-utilities.\n\nUses the two-step process:\n1. esearch - Find PMIDs matching the query\n2. efetch - Retrieve article details for found PMIDs\n\nArgs:\n    state: Current workflow state\n    query: Search query string (PubMed query syntax supported)\n    max_results: Maximum number of results to return. Default: 5\n    sort_by: Sort order - \"relevance\" or \"date\". Default: \"relevance\"\n    timeout: Request timeout in seconds. Default: 30\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"pmid\": str,\n                \"title\": str,\n                \"authors\": List[str],\n                \"abstract\": str,\n                \"journal\": str,\n                \"pub_date\": str,\n                \"doi\": str,\n                \"url\": str\n            },\n            ...\n        ],\n        \"query\": str,\n        \"total_results\": int,\n        \"returned_results\": int\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_code\": str  # empty_query, network, rate_limit, timeout, api_error\n    }\n\nExample:\n    >>> result = academic_pubmed(\n    ...     state={},\n    ...     query=\"CRISPR gene editing\",\n    ...     max_results=10,\n    ...     sort_by=\"date\"\n    ... )\n    >>> if result['success']:\n    ...     for article in result['results']:\n    ...         print(f\"[{article['pmid']}] {article['title']}\")",
          "line_number": 157
        },
        {
          "name": "actions.academic_pubmed",
          "function": "academic_pubmed",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "max_results",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "sort_by",
              "type": "str",
              "required": false,
              "default": "'relevance'"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search PubMed database for scientific articles via NCBI E-utilities.\n\nUses the two-step process:\n1. esearch - Find PMIDs matching the query\n2. efetch - Retrieve article details for found PMIDs\n\nArgs:\n    state: Current workflow state\n    query: Search query string (PubMed query syntax supported)\n    max_results: Maximum number of results to return. Default: 5\n    sort_by: Sort order - \"relevance\" or \"date\". Default: \"relevance\"\n    timeout: Request timeout in seconds. Default: 30\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"pmid\": str,\n                \"title\": str,\n                \"authors\": List[str],\n                \"abstract\": str,\n                \"journal\": str,\n                \"pub_date\": str,\n                \"doi\": str,\n                \"url\": str\n            },\n            ...\n        ],\n        \"query\": str,\n        \"total_results\": int,\n        \"returned_results\": int\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_code\": str  # empty_query, network, rate_limit, timeout, api_error\n    }\n\nExample:\n    >>> result = academic_pubmed(\n    ...     state={},\n    ...     query=\"CRISPR gene editing\",\n    ...     max_results=10,\n    ...     sort_by=\"date\"\n    ... )\n    >>> if result['success']:\n    ...     for article in result['results']:\n    ...         print(f\"[{article['pmid']}] {article['title']}\")",
          "line_number": 157
        },
        {
          "name": "academic.arxiv",
          "function": "academic_arxiv",
          "parameters": [
            {
              "name": "query",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "arxiv_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_results",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "sort_by",
              "type": "str",
              "required": false,
              "default": "'relevance'"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search ArXiv preprint server for papers.\n\nSupports two modes:\n1. Search by query string\n2. Direct lookup by ArXiv ID\n\nArgs:\n    state: Current workflow state\n    query: Search query string (ArXiv query syntax supported)\n    arxiv_id: ArXiv paper ID for direct lookup (e.g., \"2301.00001\")\n    max_results: Maximum number of results to return. Default: 5\n    sort_by: Sort order - \"relevance\" or \"date\". Default: \"relevance\"\n    timeout: Request timeout in seconds. Default: 30\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"arxiv_id\": str,\n                \"title\": str,\n                \"authors\": List[str],\n                \"abstract\": str,\n                \"categories\": List[str],\n                \"published\": str,\n                \"updated\": str,\n                \"pdf_url\": str\n            },\n            ...\n        ],\n        \"query\": str,\n        \"total_results\": int\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_code\": str  # empty_query, network, rate_limit, timeout, api_error\n    }\n\nExample:\n    >>> # Search by query\n    >>> result = academic_arxiv(\n    ...     state={},\n    ...     query=\"large language models\",\n    ...     max_results=5,\n    ...     sort_by=\"date\"\n    ... )\n    >>> if result['success']:\n    ...     for paper in result['results']:\n    ...         print(f\"[{paper['arxiv_id']}] {paper['title']}\")\n\n    >>> # Direct lookup by ID\n    >>> result = academic_arxiv(\n    ...     state={},\n    ...     arxiv_id=\"2301.00001\"\n    ... )",
          "line_number": 534
        },
        {
          "name": "actions.academic_arxiv",
          "function": "academic_arxiv",
          "parameters": [
            {
              "name": "query",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "arxiv_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_results",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "sort_by",
              "type": "str",
              "required": false,
              "default": "'relevance'"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search ArXiv preprint server for papers.\n\nSupports two modes:\n1. Search by query string\n2. Direct lookup by ArXiv ID\n\nArgs:\n    state: Current workflow state\n    query: Search query string (ArXiv query syntax supported)\n    arxiv_id: ArXiv paper ID for direct lookup (e.g., \"2301.00001\")\n    max_results: Maximum number of results to return. Default: 5\n    sort_by: Sort order - \"relevance\" or \"date\". Default: \"relevance\"\n    timeout: Request timeout in seconds. Default: 30\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"arxiv_id\": str,\n                \"title\": str,\n                \"authors\": List[str],\n                \"abstract\": str,\n                \"categories\": List[str],\n                \"published\": str,\n                \"updated\": str,\n                \"pdf_url\": str\n            },\n            ...\n        ],\n        \"query\": str,\n        \"total_results\": int\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_code\": str  # empty_query, network, rate_limit, timeout, api_error\n    }\n\nExample:\n    >>> # Search by query\n    >>> result = academic_arxiv(\n    ...     state={},\n    ...     query=\"large language models\",\n    ...     max_results=5,\n    ...     sort_by=\"date\"\n    ... )\n    >>> if result['success']:\n    ...     for paper in result['results']:\n    ...         print(f\"[{paper['arxiv_id']}] {paper['title']}\")\n\n    >>> # Direct lookup by ID\n    >>> result = academic_arxiv(\n    ...     state={},\n    ...     arxiv_id=\"2301.00001\"\n    ... )",
          "line_number": 534
        },
        {
          "name": "academic.crossref",
          "function": "academic_crossref",
          "parameters": [
            {
              "name": "doi",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "query",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_results",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "mailto",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Query CrossRef API for DOI metadata or search by query string.\n\nSupports two modes:\n1. Direct DOI lookup via `https://api.crossref.org/works/{doi}`\n2. Search by query via `https://api.crossref.org/works?query={query}`\n\nArgs:\n    state: Current workflow state\n    doi: DOI for direct lookup (e.g., \"10.1038/nature12373\")\n    query: Search query string for text search\n    max_results: Maximum number of results to return. Default: 5\n    timeout: Request timeout in seconds. Default: 30\n    mailto: Email for polite pool access (recommended for higher rate limits)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"doi\": str,\n                \"title\": str,\n                \"authors\": List[str],\n                \"abstract\": str,\n                \"container_title\": str,  # journal/book name\n                \"published_date\": str,\n                \"type\": str,  # journal-article, book-chapter, etc.\n                \"url\": str\n            },\n            ...\n        ],\n        \"query\": str,\n        \"total_results\": int\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_code\": str  # empty_query, not_found, network, rate_limit_exhausted, timeout, api_error\n    }\n\nExample:\n    >>> # Direct DOI lookup\n    >>> result = academic_crossref(\n    ...     state={},\n    ...     doi=\"10.1038/nature12373\"\n    ... )\n    >>> if result['success']:\n    ...     print(f\"Title: {result['results'][0]['title']}\")\n\n    >>> # Search by query\n    >>> result = academic_crossref(\n    ...     state={},\n    ...     query=\"machine learning cancer\",\n    ...     max_results=10,\n    ...     mailto=\"your@email.com\"\n    ... )",
          "line_number": 841
        },
        {
          "name": "actions.academic_crossref",
          "function": "academic_crossref",
          "parameters": [
            {
              "name": "doi",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "query",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_results",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "mailto",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Query CrossRef API for DOI metadata or search by query string.\n\nSupports two modes:\n1. Direct DOI lookup via `https://api.crossref.org/works/{doi}`\n2. Search by query via `https://api.crossref.org/works?query={query}`\n\nArgs:\n    state: Current workflow state\n    doi: DOI for direct lookup (e.g., \"10.1038/nature12373\")\n    query: Search query string for text search\n    max_results: Maximum number of results to return. Default: 5\n    timeout: Request timeout in seconds. Default: 30\n    mailto: Email for polite pool access (recommended for higher rate limits)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"doi\": str,\n                \"title\": str,\n                \"authors\": List[str],\n                \"abstract\": str,\n                \"container_title\": str,  # journal/book name\n                \"published_date\": str,\n                \"type\": str,  # journal-article, book-chapter, etc.\n                \"url\": str\n            },\n            ...\n        ],\n        \"query\": str,\n        \"total_results\": int\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_code\": str  # empty_query, not_found, network, rate_limit_exhausted, timeout, api_error\n    }\n\nExample:\n    >>> # Direct DOI lookup\n    >>> result = academic_crossref(\n    ...     state={},\n    ...     doi=\"10.1038/nature12373\"\n    ... )\n    >>> if result['success']:\n    ...     print(f\"Title: {result['results'][0]['title']}\")\n\n    >>> # Search by query\n    >>> result = academic_crossref(\n    ...     state={},\n    ...     query=\"machine learning cancer\",\n    ...     max_results=10,\n    ...     mailto=\"your@email.com\"\n    ... )",
          "line_number": 841
        }
      ]
    },
    {
      "file": "agent_actions.py",
      "namespace": "agent",
      "actions": [
        {
          "name": "agent.dispatch",
          "function": "agent_dispatch",
          "parameters": [
            {
              "name": "agent",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_retries",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Dispatch a task to a single named agent.\n\nArgs:\n    state: Current workflow state\n    agent: Name of agent to dispatch to (from settings.agents)\n    task: Task description (supports Jinja2 templating)\n    timeout: Override agent timeout (seconds)\n    max_retries: Override agent retry count\n    **kwargs: Additional parameters for LLM call\n\nReturns:\n    {\n        \"response\": str,\n        \"content\": str,  # Alias for response\n        \"agent\": str,\n        \"success\": bool,\n        \"attempts\": int,\n        \"elapsed_ms\": float\n    }",
          "line_number": 442
        },
        {
          "name": "agent.parallel",
          "function": "agent_parallel",
          "parameters": [
            {
              "name": "agents",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "aggregation",
              "type": "str",
              "required": false,
              "default": "'collect'"
            },
            {
              "name": "max_concurrent",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "consensus_threshold",
              "type": "float",
              "required": false,
              "default": "0.5"
            },
            {
              "name": "consensus_max_rounds",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Dispatch same task to multiple agents in parallel.\n\nArgs:\n    state: Current workflow state\n    agents: List of agent names to dispatch to\n    task: Task description (supports Jinja2 templating)\n    aggregation: Aggregation strategy: collect, vote, consensus, first\n    max_concurrent: Maximum concurrent agent calls\n    timeout: Override agent timeout (seconds)\n    consensus_threshold: Agreement threshold for consensus mode (0.0-1.0)\n    consensus_max_rounds: Max retry rounds for consensus mode\n    **kwargs: Additional parameters for LLM calls\n\nReturns:\n    Aggregated result based on aggregation strategy",
          "line_number": 527
        },
        {
          "name": "agent.sequential",
          "function": "agent_sequential",
          "parameters": [
            {
              "name": "agents",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "tasks",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "transform",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "early_exit_on_failure",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "output_key",
              "type": "str",
              "required": false,
              "default": "'sequential_result'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chain multiple agents where output feeds into next agent's input.\n\nArgs:\n    state: Current workflow state\n    agents: List of agent names to chain\n    task: Single task template for all agents (uses {{ previous_response }})\n    tasks: Per-agent task templates (must match agents length)\n    transform: Optional transformation template applied between agents\n    early_exit_on_failure: Stop chain on first failure\n    output_key: Key to store final result in state\n    **kwargs: Additional parameters for LLM calls\n\nReturns:\n    {\n        \"result\": final_response,\n        \"chain\": [{\"agent\": str, \"response\": str}, ...],\n        \"success\": bool\n    }",
          "line_number": 660
        },
        {
          "name": "agent.coordinate",
          "function": "agent_coordinate",
          "parameters": [
            {
              "name": "leader",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "workers",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "validation_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_rounds",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "parallel_workers",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Coordinator pattern with leader agent dispatching to workers.\n\nThe leader:\n1. Analyzes the task and generates subtasks for workers\n2. Dispatches subtasks to worker agents\n3. Aggregates and validates worker results\n4. Re-dispatches on validation failure\n\nArgs:\n    state: Current workflow state\n    leader: Name of leader/coordinator agent\n    workers: List of worker agent names\n    task: Main task description\n    validation_prompt: Template for leader to validate results\n    max_rounds: Maximum coordination rounds\n    parallel_workers: Execute workers in parallel (default: True)\n    **kwargs: Additional parameters for LLM calls\n\nReturns:\n    {\n        \"result\": final_result,\n        \"rounds\": int,\n        \"worker_results\": [...],\n        \"success\": bool\n    }",
          "line_number": 783
        },
        {
          "name": "agent.crewai_delegate",
          "function": "agent_crewai_delegate",
          "parameters": [
            {
              "name": "agents",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "process",
              "type": "str",
              "required": false,
              "default": "'sequential'"
            },
            {
              "name": "backend",
              "type": "str",
              "required": false,
              "default": "'native'"
            },
            {
              "name": "fallback_to_native",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "verbose",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delegate to CrewAI for complex multi-agent workflows.\n\nArgs:\n    state: Current workflow state\n    agents: List of agent names to use\n    task: Task description\n    process: CrewAI process type: sequential, hierarchical\n    backend: Execution backend: crewai, native\n    fallback_to_native: Use native TEA if CrewAI unavailable\n    verbose: Enable verbose logging\n    **kwargs: Additional CrewAI parameters\n\nReturns:\n    {\n        \"result\": str,\n        \"backend_used\": str,\n        \"success\": bool\n    }",
          "line_number": 954
        },
        {
          "name": "actions.agent_dispatch",
          "function": "agent_dispatch",
          "parameters": [
            {
              "name": "agent",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_retries",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Dispatch a task to a single named agent.\n\nArgs:\n    state: Current workflow state\n    agent: Name of agent to dispatch to (from settings.agents)\n    task: Task description (supports Jinja2 templating)\n    timeout: Override agent timeout (seconds)\n    max_retries: Override agent retry count\n    **kwargs: Additional parameters for LLM call\n\nReturns:\n    {\n        \"response\": str,\n        \"content\": str,  # Alias for response\n        \"agent\": str,\n        \"success\": bool,\n        \"attempts\": int,\n        \"elapsed_ms\": float\n    }",
          "line_number": 442
        },
        {
          "name": "actions.agent_parallel",
          "function": "agent_parallel",
          "parameters": [
            {
              "name": "agents",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "aggregation",
              "type": "str",
              "required": false,
              "default": "'collect'"
            },
            {
              "name": "max_concurrent",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "consensus_threshold",
              "type": "float",
              "required": false,
              "default": "0.5"
            },
            {
              "name": "consensus_max_rounds",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Dispatch same task to multiple agents in parallel.\n\nArgs:\n    state: Current workflow state\n    agents: List of agent names to dispatch to\n    task: Task description (supports Jinja2 templating)\n    aggregation: Aggregation strategy: collect, vote, consensus, first\n    max_concurrent: Maximum concurrent agent calls\n    timeout: Override agent timeout (seconds)\n    consensus_threshold: Agreement threshold for consensus mode (0.0-1.0)\n    consensus_max_rounds: Max retry rounds for consensus mode\n    **kwargs: Additional parameters for LLM calls\n\nReturns:\n    Aggregated result based on aggregation strategy",
          "line_number": 527
        },
        {
          "name": "actions.agent_sequential",
          "function": "agent_sequential",
          "parameters": [
            {
              "name": "agents",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "tasks",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "transform",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "early_exit_on_failure",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "output_key",
              "type": "str",
              "required": false,
              "default": "'sequential_result'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chain multiple agents where output feeds into next agent's input.\n\nArgs:\n    state: Current workflow state\n    agents: List of agent names to chain\n    task: Single task template for all agents (uses {{ previous_response }})\n    tasks: Per-agent task templates (must match agents length)\n    transform: Optional transformation template applied between agents\n    early_exit_on_failure: Stop chain on first failure\n    output_key: Key to store final result in state\n    **kwargs: Additional parameters for LLM calls\n\nReturns:\n    {\n        \"result\": final_response,\n        \"chain\": [{\"agent\": str, \"response\": str}, ...],\n        \"success\": bool\n    }",
          "line_number": 660
        },
        {
          "name": "actions.agent_coordinate",
          "function": "agent_coordinate",
          "parameters": [
            {
              "name": "leader",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "workers",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "validation_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_rounds",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "parallel_workers",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Coordinator pattern with leader agent dispatching to workers.\n\nThe leader:\n1. Analyzes the task and generates subtasks for workers\n2. Dispatches subtasks to worker agents\n3. Aggregates and validates worker results\n4. Re-dispatches on validation failure\n\nArgs:\n    state: Current workflow state\n    leader: Name of leader/coordinator agent\n    workers: List of worker agent names\n    task: Main task description\n    validation_prompt: Template for leader to validate results\n    max_rounds: Maximum coordination rounds\n    parallel_workers: Execute workers in parallel (default: True)\n    **kwargs: Additional parameters for LLM calls\n\nReturns:\n    {\n        \"result\": final_result,\n        \"rounds\": int,\n        \"worker_results\": [...],\n        \"success\": bool\n    }",
          "line_number": 783
        },
        {
          "name": "actions.agent_crewai_delegate",
          "function": "agent_crewai_delegate",
          "parameters": [
            {
              "name": "agents",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "process",
              "type": "str",
              "required": false,
              "default": "'sequential'"
            },
            {
              "name": "backend",
              "type": "str",
              "required": false,
              "default": "'native'"
            },
            {
              "name": "fallback_to_native",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "verbose",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delegate to CrewAI for complex multi-agent workflows.\n\nArgs:\n    state: Current workflow state\n    agents: List of agent names to use\n    task: Task description\n    process: CrewAI process type: sequential, hierarchical\n    backend: Execution backend: crewai, native\n    fallback_to_native: Use native TEA if CrewAI unavailable\n    verbose: Enable verbose logging\n    **kwargs: Additional CrewAI parameters\n\nReturns:\n    {\n        \"result\": str,\n        \"backend_used\": str,\n        \"success\": bool\n    }",
          "line_number": 954
        }
      ]
    },
    {
      "file": "auth_actions.py",
      "namespace": "auth",
      "actions": [
        {
          "name": "auth.verify",
          "function": "auth_verify",
          "parameters": [
            {
              "name": "token",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "headers",
              "type": "Optional[Dict[str, str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Verify an authentication token.\n\nThis action performs explicit token verification within a workflow.\nUseful when auth middleware is disabled (inject_user: false) or\nwhen verifying tokens from different sources.\n\nArgs:\n    state: Current workflow state\n    token: Token to verify (if None, extracts from headers)\n    headers: Request headers for token extraction\n    provider: Provider type override (firebase, jwt, api_key)\n\nReturns:\n    Dict with:\n        - success: bool\n        - user: UserInfo dict if success\n        - error: str if failure\n\nExample YAML:\n    - uses: auth.verify\n      with:\n        token: \"{{ state.custom_token }}\"\n      output: auth_result\n\n    - name: check_auth\n      run: |\n        if state[\"auth_result\"][\"success\"]:\n            return {\"user_id\": state[\"auth_result\"][\"user\"][\"uid\"]}\n        else:\n            return {\"error\": state[\"auth_result\"][\"error\"]}",
          "line_number": 44
        },
        {
          "name": "actions.auth_verify",
          "function": "auth_verify",
          "parameters": [
            {
              "name": "token",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "headers",
              "type": "Optional[Dict[str, str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Verify an authentication token.\n\nThis action performs explicit token verification within a workflow.\nUseful when auth middleware is disabled (inject_user: false) or\nwhen verifying tokens from different sources.\n\nArgs:\n    state: Current workflow state\n    token: Token to verify (if None, extracts from headers)\n    headers: Request headers for token extraction\n    provider: Provider type override (firebase, jwt, api_key)\n\nReturns:\n    Dict with:\n        - success: bool\n        - user: UserInfo dict if success\n        - error: str if failure\n\nExample YAML:\n    - uses: auth.verify\n      with:\n        token: \"{{ state.custom_token }}\"\n      output: auth_result\n\n    - name: check_auth\n      run: |\n        if state[\"auth_result\"][\"success\"]:\n            return {\"user_id\": state[\"auth_result\"][\"user\"][\"uid\"]}\n        else:\n            return {\"error\": state[\"auth_result\"][\"error\"]}",
          "line_number": 44
        },
        {
          "name": "auth.get_user",
          "function": "auth_get_user",
          "parameters": [
            {
              "name": "uid",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get full user profile by UID.\n\nThis action fetches the complete user profile from the auth provider.\nFor Firebase, this retrieves user data from Firebase Auth.\nFor JWT providers, this returns minimal info.\n\nArgs:\n    state: Current workflow state\n    uid: User ID to look up (if None, uses __user__.uid from state)\n\nReturns:\n    Dict with:\n        - success: bool\n        - user: Full UserInfo dict if success\n        - error: str if failure\n\nExample YAML:\n    - uses: auth.get_user\n      with:\n        uid: \"{{ state.__user__.uid }}\"\n      output: full_profile\n\n    - name: use_profile\n      run: |\n        profile = state[\"full_profile\"]\n        if profile[\"success\"]:\n            return {\"name\": profile[\"user\"][\"name\"]}",
          "line_number": 183
        },
        {
          "name": "actions.auth_get_user",
          "function": "auth_get_user",
          "parameters": [
            {
              "name": "uid",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get full user profile by UID.\n\nThis action fetches the complete user profile from the auth provider.\nFor Firebase, this retrieves user data from Firebase Auth.\nFor JWT providers, this returns minimal info.\n\nArgs:\n    state: Current workflow state\n    uid: User ID to look up (if None, uses __user__.uid from state)\n\nReturns:\n    Dict with:\n        - success: bool\n        - user: Full UserInfo dict if success\n        - error: str if failure\n\nExample YAML:\n    - uses: auth.get_user\n      with:\n        uid: \"{{ state.__user__.uid }}\"\n      output: full_profile\n\n    - name: use_profile\n      run: |\n        profile = state[\"full_profile\"]\n        if profile[\"success\"]:\n            return {\"name\": profile[\"user\"][\"name\"]}",
          "line_number": 183
        }
      ]
    },
    {
      "file": "bmad_actions.py",
      "namespace": "bmad",
      "actions": [
        {
          "name": "bmad.parse_story",
          "function": "bmad_parse_story",
          "parameters": [
            {
              "name": "content",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "validate_template_flag",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "template_path",
              "type": "str",
              "required": false,
              "default": "'.bmad-core/templates/story-tmpl.yaml'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Parse a BMad story file into structured data.\n\nUses regex-based parsing to extract BMad-specific sections including\ntasks, acceptance criteria, status, and completion metrics.\n\nArgs:\n    state: Current state dictionary\n    content: Raw markdown content of the story\n    validate_template_flag: If True, validate against BMad template\n    template_path: Path to story template for validation\n\nReturns:\n    {\n        \"story_id\": str,                 # Story identifier\n        \"title\": str,                    # Story title\n        \"status\": str,                   # Current status\n        \"acceptance_criteria\": List,     # Numbered AC items\n        \"tasks\": List,                   # Task tree with subtasks\n        \"completion\": {                  # Completion metrics\n            \"total\": int,\n            \"completed\": int,\n            \"percentage\": float\n        },\n        \"validation_errors\": List[str],  # If validate_template=True\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure\n\nExample:\n    >>> result = bmad_parse_story(\n    ...     state={},\n    ...     content=\"# Story TEA-TEST-001.1: Test\\n\\n## Status\\nDraft\\n...\"\n    ... )\n    >>> assert result['story_id'] == 'TEA-TEST-001.1'\n    >>> assert result['status'] == 'Draft'",
          "line_number": 257
        },
        {
          "name": "bmad_parse_story",
          "function": "bmad_parse_story",
          "parameters": [
            {
              "name": "content",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "validate_template_flag",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "template_path",
              "type": "str",
              "required": false,
              "default": "'.bmad-core/templates/story-tmpl.yaml'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Parse a BMad story file into structured data.\n\nUses regex-based parsing to extract BMad-specific sections including\ntasks, acceptance criteria, status, and completion metrics.\n\nArgs:\n    state: Current state dictionary\n    content: Raw markdown content of the story\n    validate_template_flag: If True, validate against BMad template\n    template_path: Path to story template for validation\n\nReturns:\n    {\n        \"story_id\": str,                 # Story identifier\n        \"title\": str,                    # Story title\n        \"status\": str,                   # Current status\n        \"acceptance_criteria\": List,     # Numbered AC items\n        \"tasks\": List,                   # Task tree with subtasks\n        \"completion\": {                  # Completion metrics\n            \"total\": int,\n            \"completed\": int,\n            \"percentage\": float\n        },\n        \"validation_errors\": List[str],  # If validate_template=True\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure\n\nExample:\n    >>> result = bmad_parse_story(\n    ...     state={},\n    ...     content=\"# Story TEA-TEST-001.1: Test\\n\\n## Status\\nDraft\\n...\"\n    ... )\n    >>> assert result['story_id'] == 'TEA-TEST-001.1'\n    >>> assert result['status'] == 'Draft'",
          "line_number": 257
        }
      ]
    },
    {
      "file": "cache_actions.py",
      "namespace": "cache",
      "actions": [
        {
          "name": "cache.wrap",
          "function": "cache_wrap",
          "parameters": [
            {
              "name": "action",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "args",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "key_strategy",
              "type": "str",
              "required": false,
              "default": "'args'"
            },
            {
              "name": "key_source",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl_days",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl_hours",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl_seconds",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "skip_cache",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "cache_enabled",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "cleanup_probability",
              "type": "float",
              "required": false,
              "default": "0.05"
            },
            {
              "name": "cleanup_limit",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "key_prefix",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Wrap any action with automatic caching.\n\nChecks cache before execution, stores result after successful execution.\nUses Long-Term Memory (LTM) for persistence.\n\nCache keys are automatically prefixed with the agent name (TEA-BUILTIN-010.1):\n- Format: cache:{agent_name}:{user_key}\n- Agent name resolved from settings.name, YAML name field, or filename\n- Use key_prefix=False to disable automatic prefixing\n\nArgs:\n    state: Current state dictionary\n    action: The action to wrap (e.g., 'llamaextract.extract', 'llm.call')\n    args: Arguments to pass to the wrapped action\n    key: Cache key or Jinja expression result. If not provided, uses key_strategy.\n    key_strategy: Strategy for key generation: 'sha256', 'args', 'custom', 'file_content'\n    key_source: For file_content/sha256, the argument name containing source value\n    ttl_days: Cache TTL in days (default: 60)\n    ttl_hours: Cache TTL in hours (overrides ttl_days)\n    ttl_seconds: Cache TTL in seconds (overrides ttl_hours and ttl_days)\n    skip_cache: Bypass cache lookup, force fresh execution\n    cache_enabled: Enable/disable caching entirely (default: True)\n    cleanup_probability: Probability of running cleanup after cache miss (0.0-1.0)\n    cleanup_limit: Maximum expired entries to delete per cleanup run\n    key_prefix: Auto-prefix keys with agent name (default: True, TEA-BUILTIN-010.1)\n\nReturns:\n    Action result with cache metadata:\n    - success: bool\n    - result: The wrapped action's result\n    - _cache_hit: bool - Whether result came from cache\n    - _cache_key: str - The cache key used\n    - _cache_created_at: str - ISO timestamp (if cache hit)\n    - error: str - Error message if failed",
          "line_number": 226
        },
        {
          "name": "actions.cache_wrap",
          "function": "cache_wrap",
          "parameters": [
            {
              "name": "action",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "args",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "key_strategy",
              "type": "str",
              "required": false,
              "default": "'args'"
            },
            {
              "name": "key_source",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl_days",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl_hours",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl_seconds",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "skip_cache",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "cache_enabled",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "cleanup_probability",
              "type": "float",
              "required": false,
              "default": "0.05"
            },
            {
              "name": "cleanup_limit",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "key_prefix",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Wrap any action with automatic caching.\n\nChecks cache before execution, stores result after successful execution.\nUses Long-Term Memory (LTM) for persistence.\n\nCache keys are automatically prefixed with the agent name (TEA-BUILTIN-010.1):\n- Format: cache:{agent_name}:{user_key}\n- Agent name resolved from settings.name, YAML name field, or filename\n- Use key_prefix=False to disable automatic prefixing\n\nArgs:\n    state: Current state dictionary\n    action: The action to wrap (e.g., 'llamaextract.extract', 'llm.call')\n    args: Arguments to pass to the wrapped action\n    key: Cache key or Jinja expression result. If not provided, uses key_strategy.\n    key_strategy: Strategy for key generation: 'sha256', 'args', 'custom', 'file_content'\n    key_source: For file_content/sha256, the argument name containing source value\n    ttl_days: Cache TTL in days (default: 60)\n    ttl_hours: Cache TTL in hours (overrides ttl_days)\n    ttl_seconds: Cache TTL in seconds (overrides ttl_hours and ttl_days)\n    skip_cache: Bypass cache lookup, force fresh execution\n    cache_enabled: Enable/disable caching entirely (default: True)\n    cleanup_probability: Probability of running cleanup after cache miss (0.0-1.0)\n    cleanup_limit: Maximum expired entries to delete per cleanup run\n    key_prefix: Auto-prefix keys with agent name (default: True, TEA-BUILTIN-010.1)\n\nReturns:\n    Action result with cache metadata:\n    - success: bool\n    - result: The wrapped action's result\n    - _cache_hit: bool - Whether result came from cache\n    - _cache_key: str - The cache key used\n    - _cache_created_at: str - ISO timestamp (if cache hit)\n    - error: str - Error message if failed",
          "line_number": 226
        },
        {
          "name": "cache.get",
          "function": "cache_get",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "include_metadata",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Retrieve cached value by key without executing any action.\n\nUseful for debugging and cache inspection.\n\nArgs:\n    state: Current state dictionary\n    key: Cache key to retrieve\n    include_metadata: Include cache metadata in response\n\nReturns:\n    - success: bool\n    - found: bool - Whether entry exists\n    - value: any - Cached value\n    - metadata: dict - If include_metadata=True\n    - expired: bool - Whether entry has expired",
          "line_number": 394
        },
        {
          "name": "actions.cache_get",
          "function": "cache_get",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "include_metadata",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Retrieve cached value by key without executing any action.\n\nUseful for debugging and cache inspection.\n\nArgs:\n    state: Current state dictionary\n    key: Cache key to retrieve\n    include_metadata: Include cache metadata in response\n\nReturns:\n    - success: bool\n    - found: bool - Whether entry exists\n    - value: any - Cached value\n    - metadata: dict - If include_metadata=True\n    - expired: bool - Whether entry has expired",
          "line_number": 394
        },
        {
          "name": "cache.invalidate",
          "function": "cache_invalidate",
          "parameters": [
            {
              "name": "key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "pattern",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata_filter",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Invalidate (delete) cached entries by exact key or pattern.\n\nArgs:\n    state: Current state dictionary\n    key: Exact cache key to invalidate\n    pattern: Key pattern for bulk invalidation (uses LTM search)\n    metadata_filter: Metadata filter for selective invalidation\n\nReturns:\n    - success: bool\n    - deleted_count: int\n    - deleted_keys: list[str]",
          "line_number": 471
        },
        {
          "name": "actions.cache_invalidate",
          "function": "cache_invalidate",
          "parameters": [
            {
              "name": "key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "pattern",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata_filter",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Invalidate (delete) cached entries by exact key or pattern.\n\nArgs:\n    state: Current state dictionary\n    key: Exact cache key to invalidate\n    pattern: Key pattern for bulk invalidation (uses LTM search)\n    metadata_filter: Metadata filter for selective invalidation\n\nReturns:\n    - success: bool\n    - deleted_count: int\n    - deleted_keys: list[str]",
          "line_number": 471
        },
        {
          "name": "storage.hash",
          "function": "storage_hash",
          "parameters": [
            {
              "name": "path",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "algorithm",
              "type": "str",
              "required": false,
              "default": "'sha256'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compute hash of file content from any URI.\n\nSupports local files, S3, GCS, Azure via fsspec.\n\nArgs:\n    state: Current state dictionary\n    path: File path or URI (file://, s3://, gs://, az://)\n    algorithm: Hash algorithm: 'sha256', 'md5', 'blake2b'\n\nReturns:\n    - success: bool\n    - hash: str - Hex-encoded hash\n    - algorithm: str\n    - size_bytes: int - File size\n    - path: str\n    - error: str - If failed",
          "line_number": 561
        },
        {
          "name": "actions.storage_hash",
          "function": "storage_hash",
          "parameters": [
            {
              "name": "path",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "algorithm",
              "type": "str",
              "required": false,
              "default": "'sha256'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compute hash of file content from any URI.\n\nSupports local files, S3, GCS, Azure via fsspec.\n\nArgs:\n    state: Current state dictionary\n    path: File path or URI (file://, s3://, gs://, az://)\n    algorithm: Hash algorithm: 'sha256', 'md5', 'blake2b'\n\nReturns:\n    - success: bool\n    - hash: str - Hex-encoded hash\n    - algorithm: str\n    - size_bytes: int - File size\n    - path: str\n    - error: str - If failed",
          "line_number": 561
        }
      ]
    },
    {
      "file": "catalog_actions.py",
      "namespace": "catalog",
      "actions": [
        {
          "name": "catalog.register_table",
          "function": "catalog_register_table",
          "parameters": [
            {
              "name": "name",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "type",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "primary_key",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "source_collection",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "parquet_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Register a new table in the DuckLake catalog.\n\nTEA Custom Action: catalog.register_table\n\nArgs:\n    state: Current agent state (must contain metadata_store or _metadata_store)\n    name: Table name (used as document ID)\n    type: Table type - \"memory\" or \"tabular\"\n    schema: Column definitions for tabular tables {\"column\": \"TYPE\", ...}\n    primary_key: List of key columns for tabular tables\n    source_collection: Source collection for memory tables\n    parquet_path: GCS path to Parquet file\n    **kwargs: May contain metadata_store for dependency injection\n\nReturns:\n    Dict with success, table_name, snapshot_id, or error",
          "line_number": 118
        },
        {
          "name": "catalog.get_table",
          "function": "catalog_get_table",
          "parameters": [
            {
              "name": "name",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get table metadata from the catalog.\n\nTEA Custom Action: catalog.get_table\n\nArgs:\n    state: Current agent state\n    name: Table name\n\nReturns:\n    Dict with success and table data, or error",
          "line_number": 253
        },
        {
          "name": "catalog.list_tables",
          "function": "catalog_list_tables",
          "parameters": [
            {
              "name": "type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List tables in the catalog with optional filtering.\n\nTEA Custom Action: catalog.list_tables\n\nArgs:\n    state: Current agent state\n    type: Optional filter by table type (\"memory\" or \"tabular\")\n    limit: Maximum number of results\n\nReturns:\n    Dict with success and tables list",
          "line_number": 302
        },
        {
          "name": "catalog.track_file",
          "function": "catalog_track_file",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "path",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "content_hash",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "byte_size",
              "type": "int",
              "required": true,
              "default": null
            },
            {
              "name": "row_count",
              "type": "int",
              "required": true,
              "default": null
            },
            {
              "name": "type",
              "type": "str",
              "required": false,
              "default": "FILE_TYPE_PARQUET"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Track a Parquet or delta file in the catalog.\n\nTEA Custom Action: catalog.track_file\n\nArgs:\n    state: Current agent state\n    table: Table name this file belongs to\n    path: Full GCS URI (e.g., gs://bucket/path/file.parquet)\n    content_hash: SHA-256 hash of file content\n    byte_size: File size in bytes\n    row_count: Number of rows in the file\n    type: File type - \"parquet\" or \"delta\"\n\nReturns:\n    Dict with success and file_id",
          "line_number": 361
        },
        {
          "name": "catalog.get_file",
          "function": "catalog_get_file",
          "parameters": [
            {
              "name": "file_id",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get file metadata from the catalog.\n\nTEA Custom Action: catalog.get_file\n\nArgs:\n    state: Current agent state\n    file_id: File document ID\n\nReturns:\n    Dict with success and file data",
          "line_number": 448
        },
        {
          "name": "catalog.list_files",
          "function": "catalog_list_files",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List files for a table with optional filtering.\n\nTEA Custom Action: catalog.list_files\n\nArgs:\n    state: Current agent state\n    table: Table name\n    type: Optional filter by file type\n    limit: Maximum number of results\n\nReturns:\n    Dict with success and files list",
          "line_number": 497
        },
        {
          "name": "catalog.create_snapshot",
          "function": "catalog_create_snapshot",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "parquet_file_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "source_doc_ids",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "row_count",
              "type": "int",
              "required": false,
              "default": "0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create a point-in-time snapshot for a table.\n\nTEA Custom Action: catalog.create_snapshot\n\nArgs:\n    state: Current agent state\n    table: Table name\n    parquet_file_id: Reference to ducklake_files document\n    source_doc_ids: List of agent_memory doc IDs\n    row_count: Total rows in this snapshot\n\nReturns:\n    Dict with success and snapshot_id",
          "line_number": 563
        },
        {
          "name": "catalog.get_latest_snapshot",
          "function": "catalog_get_latest_snapshot",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get the most recent snapshot for a table.\n\nTEA Custom Action: catalog.get_latest_snapshot\n\nArgs:\n    state: Current agent state\n    table: Table name\n\nReturns:\n    Dict with success and snapshot data",
          "line_number": 631
        },
        {
          "name": "catalog.list_snapshots",
          "function": "catalog_list_snapshots",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List snapshots for a table.\n\nTEA Custom Action: catalog.list_snapshots\n\nArgs:\n    state: Current agent state\n    table: Table name\n    limit: Maximum number of results\n\nReturns:\n    Dict with success and snapshots list",
          "line_number": 689
        },
        {
          "name": "catalog.get_changed_files",
          "function": "catalog_get_changed_files",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "since_snapshot_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get files that changed since a snapshot.\n\nTEA Custom Action: catalog.get_changed_files\n\nFor memory tables: queries agent_memory for docs where:\n- synced_at is null (never synced), OR\n- doc not in snapshot's source_doc_ids\n\nFor tabular tables: queries ducklake_files for files created after snapshot.\n\nArgs:\n    state: Current agent state\n    table: Table name\n    since_snapshot_id: Optional snapshot ID to compare against\n\nReturns:\n    Dict with success and list of changed files/docs",
          "line_number": 749
        }
      ]
    },
    {
      "file": "cloud_memory_actions.py",
      "namespace": "cloud_memory",
      "actions": [
        {
          "name": "memory.cloud_store",
          "function": "memory_cloud_store",
          "parameters": [
            {
              "name": "path",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "content",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "content_type",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "anchors",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Dict[str, Any]",
              "required": false,
              "default": "None"
            },
            {
              "name": "skip_embedding",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Store an artifact in cloud storage with metadata and embedding.\n\nTEA Custom Action: memory.cloud_store\n\nArgs:\n    state: Current agent state (must contain project_id)\n    path: Relative path for the file\n    content: Content to store\n    content_type: Optional explicit content type (yaml, json, md)\n    anchors: List of anchor tags for searching\n    metadata: Additional metadata to store\n    skip_embedding: If True, skip embedding generation\n    **kwargs: May contain metadata_store and blob_storage\n\nReturns:\n    Dict with success, file_path, doc_id, or error",
          "line_number": 284
        },
        {
          "name": "memory.cloud_retrieve",
          "function": "memory_cloud_retrieve",
          "parameters": [
            {
              "name": "path",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "doc_id",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "parse",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Retrieve an artifact from cloud storage.\n\nTEA Custom Action: memory.cloud_retrieve\n\nArgs:\n    state: Current agent state (must contain project_id)\n    path: Relative path of the file (optional if doc_id provided)\n    doc_id: Document ID (optional if path provided)\n    parse: If True, auto-parse YAML/JSON content\n\nReturns:\n    Dict with success, content, metadata, or error",
          "line_number": 460
        },
        {
          "name": "memory.cloud_list",
          "function": "memory_cloud_list",
          "parameters": [
            {
              "name": "prefix",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "anchors",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "status",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "content_type",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "offset",
              "type": "int",
              "required": false,
              "default": "0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List artifacts with filtering.\n\nTEA Custom Action: memory.cloud_list\n\nArgs:\n    state: Current agent state (must contain project_id)\n    prefix: Filter by path prefix\n    anchors: Filter by anchors (OR logic)\n    status: Filter by status (active, archived, draft)\n    content_type: Filter by content type\n    limit: Maximum results (default 100)\n    offset: Skip first N results\n\nReturns:\n    Dict with success and files list",
          "line_number": 587
        },
        {
          "name": "memory.manifest_update",
          "function": "memory_manifest_update",
          "parameters": [
            {
              "name": "doc_id",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "anchors",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "anchors_add",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "anchors_remove",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "status",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "summary",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Update metadata only (not file content).\n\nTEA Custom Action: memory.manifest_update\n\nArgs:\n    state: Current agent state (must contain project_id)\n    doc_id: Document ID to update\n    anchors: Replace all anchors (if provided)\n    anchors_add: Anchors to add\n    anchors_remove: Anchors to remove\n    status: New status value\n    summary: New summary text\n\nReturns:\n    Dict with success status",
          "line_number": 689
        },
        {
          "name": "memory.manifest_search",
          "function": "memory_manifest_search",
          "parameters": [
            {
              "name": "anchor",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "anchors_any",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "status",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "offset",
              "type": "int",
              "required": false,
              "default": "0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search documents by anchors.\n\nTEA Custom Action: memory.manifest_search\n\nArgs:\n    state: Current agent state (must contain project_id)\n    anchor: Single anchor for exact match\n    anchors_any: List of anchors for OR search\n    status: Filter by status\n    limit: Maximum results\n    offset: Pagination offset\n\nReturns:\n    Dict with success and matching documents",
          "line_number": 772
        }
      ]
    },
    {
      "file": "code_actions.py",
      "namespace": "code",
      "actions": [
        {
          "name": "code.execute",
          "function": "code_execute",
          "parameters": [
            {
              "name": "code",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "float",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "max_output_bytes",
              "type": "int",
              "required": false,
              "default": "65536"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute Python code in a RestrictedPython sandbox.\n\nThis action provides sandboxed code execution with:\n- Whitelist-only builtins (safe math, types, iteration)\n- No imports, file access, or network access\n- Timeout enforcement\n- Output capture (stdout/stderr)\n- Output size limits\n\nArgs:\n    state: Current state dictionary\n    code: Python source code to execute\n    timeout: Maximum execution time in seconds (default: 30)\n    max_output_bytes: Maximum output size in bytes (default: 64KB)\n\nReturns:\n    {\n        \"success\": bool,\n        \"stdout\": str,\n        \"stderr\": str,\n        \"return_value\": any,\n        \"error\": Optional[str],\n        \"execution_time_ms\": float\n    }\n\nExample:\n    >>> result = code_execute({}, code=\"x = 1 + 2; result = x * 10\")\n    >>> result['return_value']  # 30\n\nSecurity Notes:\n    - Set `result` variable to return a value\n    - Use print() for stdout output\n    - Imports are blocked\n    - File/network/system access blocked",
          "line_number": 542
        },
        {
          "name": "actions.code_execute",
          "function": "code_execute",
          "parameters": [
            {
              "name": "code",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "float",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "max_output_bytes",
              "type": "int",
              "required": false,
              "default": "65536"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute Python code in a RestrictedPython sandbox.\n\nThis action provides sandboxed code execution with:\n- Whitelist-only builtins (safe math, types, iteration)\n- No imports, file access, or network access\n- Timeout enforcement\n- Output capture (stdout/stderr)\n- Output size limits\n\nArgs:\n    state: Current state dictionary\n    code: Python source code to execute\n    timeout: Maximum execution time in seconds (default: 30)\n    max_output_bytes: Maximum output size in bytes (default: 64KB)\n\nReturns:\n    {\n        \"success\": bool,\n        \"stdout\": str,\n        \"stderr\": str,\n        \"return_value\": any,\n        \"error\": Optional[str],\n        \"execution_time_ms\": float\n    }\n\nExample:\n    >>> result = code_execute({}, code=\"x = 1 + 2; result = x * 10\")\n    >>> result['return_value']  # 30\n\nSecurity Notes:\n    - Set `result` variable to return a value\n    - Use print() for stdout output\n    - Imports are blocked\n    - File/network/system access blocked",
          "line_number": 542
        },
        {
          "name": "code.sandbox",
          "function": "code_sandbox",
          "parameters": [
            {
              "name": "action",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "sandbox_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "code",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "float",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "max_output_bytes",
              "type": "int",
              "required": false,
              "default": "65536"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Manage persistent sandbox sessions for multi-step code execution.\n\nThis action allows creating sandbox sessions that persist state\nbetween executions. Variables defined in one execution are\navailable in subsequent executions within the same session.\n\nArgs:\n    state: Current state dictionary\n    action: One of \"create\", \"execute\", \"destroy\", \"list\"\n    sandbox_id: Session ID (required for execute/destroy)\n    code: Python code (required for execute)\n    timeout: Execution timeout in seconds (default: 30)\n    max_output_bytes: Max output size in bytes (default: 64KB)\n\nReturns:\n    For action=\"create\":\n        {\"sandbox_id\": str, \"created\": True, \"success\": True}\n\n    For action=\"execute\":\n        {\"success\": bool, \"stdout\": str, \"stderr\": str,\n         \"return_value\": any, \"error\": str, \"execution_time_ms\": float}\n\n    For action=\"destroy\":\n        {\"destroyed\": bool, \"sandbox_id\": str, \"success\": True}\n\n    For action=\"list\":\n        {\"sandboxes\": List[str], \"count\": int, \"success\": True}\n\nExample:\n    >>> # Create session\n    >>> result = code_sandbox({}, action=\"create\")\n    >>> sid = result['sandbox_id']\n    >>>\n    >>> # Execute with state persistence\n    >>> code_sandbox({}, action=\"execute\", sandbox_id=sid, code=\"x = 10\")\n    >>> result = code_sandbox({}, action=\"execute\", sandbox_id=sid, code=\"result = x + 5\")\n    >>> result['return_value']  # 15\n    >>>\n    >>> # Cleanup\n    >>> code_sandbox({}, action=\"destroy\", sandbox_id=sid)",
          "line_number": 606
        },
        {
          "name": "actions.code_sandbox",
          "function": "code_sandbox",
          "parameters": [
            {
              "name": "action",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "sandbox_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "code",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "float",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "max_output_bytes",
              "type": "int",
              "required": false,
              "default": "65536"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Manage persistent sandbox sessions for multi-step code execution.\n\nThis action allows creating sandbox sessions that persist state\nbetween executions. Variables defined in one execution are\navailable in subsequent executions within the same session.\n\nArgs:\n    state: Current state dictionary\n    action: One of \"create\", \"execute\", \"destroy\", \"list\"\n    sandbox_id: Session ID (required for execute/destroy)\n    code: Python code (required for execute)\n    timeout: Execution timeout in seconds (default: 30)\n    max_output_bytes: Max output size in bytes (default: 64KB)\n\nReturns:\n    For action=\"create\":\n        {\"sandbox_id\": str, \"created\": True, \"success\": True}\n\n    For action=\"execute\":\n        {\"success\": bool, \"stdout\": str, \"stderr\": str,\n         \"return_value\": any, \"error\": str, \"execution_time_ms\": float}\n\n    For action=\"destroy\":\n        {\"destroyed\": bool, \"sandbox_id\": str, \"success\": True}\n\n    For action=\"list\":\n        {\"sandboxes\": List[str], \"count\": int, \"success\": True}\n\nExample:\n    >>> # Create session\n    >>> result = code_sandbox({}, action=\"create\")\n    >>> sid = result['sandbox_id']\n    >>>\n    >>> # Execute with state persistence\n    >>> code_sandbox({}, action=\"execute\", sandbox_id=sid, code=\"x = 10\")\n    >>> result = code_sandbox({}, action=\"execute\", sandbox_id=sid, code=\"result = x + 5\")\n    >>> result['return_value']  # 15\n    >>>\n    >>> # Cleanup\n    >>> code_sandbox({}, action=\"destroy\", sandbox_id=sid)",
          "line_number": 606
        }
      ]
    },
    {
      "file": "context_actions.py",
      "namespace": "context",
      "actions": [
        {
          "name": "context.assemble",
          "function": "context_assemble",
          "parameters": [
            {
              "name": "config",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "query",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "variables",
              "type": "Optional[Dict[str, str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Assemble context from configured layers with relevance ranking.\n\nTEA Custom Action: context.assemble\n\nArgs:\n    state: Current agent state (must contain project_id)\n    config: Layer configuration:\n        - max_tokens: int (default 8000)\n        - layers: [{scope, entity_id?, directory?, year?, month?}, ...]\n        - tabular: [{table, sql}, ...] (optional)\n    query: Search query for relevance ranking (optional)\n    variables: Variable substitution dict\n    **kwargs: Additional arguments\n        cloud_list_fn: memory.cloud_list function (optional)\n        cloud_retrieve_fn: memory.cloud_retrieve function (optional)\n        data_query_fn: data.query function (optional)\n        vector_search_fn: memory.vector_search function (optional)\n        config_get_scopes_fn: config.get_scopes function (optional)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"context\": str,  # Assembled context string\n        \"tokens\": int,   # Total token count\n        \"sources\": [{path, scope, tokens, score}, ...],\n        \"tabular\": [{table, rows}, ...],\n        \"truncated\": bool\n    }\n\n    On error:\n    {\n        \"success\": False,\n        \"error\": str\n    }\n\nExample:\n    context.assemble(\n        config={\n            \"max_tokens\": 8000,\n            \"layers\": [\n                {\"scope\": \"app\"},\n                {\"scope\": \"firm\", \"entity_id\": \"firm123\"},\n                {\"scope\": \"directory\", \"directory\": \"chambers\", \"year\": \"2024\", \"month\": \"12\"}\n            ],\n            \"tabular\": [\n                {\"table\": \"firm_scores\", \"sql\": \"SELECT * FROM data WHERE firm_id = '{{firm_id}}'\"}\n            ]\n        },\n        query=\"What are the key matters for this firm?\",\n        variables={\"firm_id\": \"firm123\", \"year\": \"2024\", \"month\": \"12\"}\n    )",
          "line_number": 578
        }
      ]
    },
    {
      "file": "core_actions.py",
      "namespace": "core",
      "actions": [
        {
          "name": "http.get",
          "function": "http_get",
          "parameters": [
            {
              "name": "url",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "headers",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Make HTTP GET request.",
          "line_number": 137
        },
        {
          "name": "actions.http_get",
          "function": "http_get",
          "parameters": [
            {
              "name": "url",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "headers",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Make HTTP GET request.",
          "line_number": 137
        },
        {
          "name": "http.post",
          "function": "http_post",
          "parameters": [
            {
              "name": "url",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "json",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "headers",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Make HTTP POST request.",
          "line_number": 144
        },
        {
          "name": "actions.http_post",
          "function": "http_post",
          "parameters": [
            {
              "name": "url",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "json",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "headers",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Make HTTP POST request.",
          "line_number": 144
        },
        {
          "name": "file.write",
          "function": "file_write",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "content",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "stream",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Write content to a file (local or remote via fsspec).\n\nSupports local paths, file:// URIs, and remote URIs (s3://, gs://, az://, etc.).\nAutomatically creates parent directories for both local and remote paths.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., '/tmp/file.txt', 's3://bucket/key')\n    content: Content to write (string)\n    stream: If True and content is an iterator, stream write (default: False)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'path': str, 'success': True} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 157
        },
        {
          "name": "actions.file_write",
          "function": "file_write",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "content",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "stream",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Write content to a file (local or remote via fsspec).\n\nSupports local paths, file:// URIs, and remote URIs (s3://, gs://, az://, etc.).\nAutomatically creates parent directories for both local and remote paths.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., '/tmp/file.txt', 's3://bucket/key')\n    content: Content to write (string)\n    stream: If True and content is an iterator, stream write (default: False)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'path': str, 'success': True} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 157
        },
        {
          "name": "file.read",
          "function": "file_read",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "stream",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "encoding",
              "type": "Any",
              "required": false,
              "default": "'utf-8'"
            },
            {
              "name": "cache",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Read content from a file (local or remote via fsspec).\n\nSupports local paths, file:// URIs, and remote URIs (s3://, gs://, az://, etc.).\nSupports fsspec's caching via protocol chaining (e.g., 'simplecache::s3://...').\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., '/tmp/file.txt', 's3://bucket/key')\n    stream: If True, returns a file-like object instead of content (default: False)\n    encoding: Text encoding (default: 'utf-8')\n    cache: Caching mode - 'simple', 'file', 'block', or None (default: None)\n           - 'simple': Cache entire file locally (simplecache::)\n           - 'file': Cache to specific temp directory (filecache::)\n           - 'block': Cache in blocks for partial reads (blockcache::)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'content': str, 'success': True} on success (when stream=False)\n    {'file': file-like, 'success': True} on success (when stream=True)\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 220
        },
        {
          "name": "actions.file_read",
          "function": "file_read",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "stream",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "encoding",
              "type": "Any",
              "required": false,
              "default": "'utf-8'"
            },
            {
              "name": "cache",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Read content from a file (local or remote via fsspec).\n\nSupports local paths, file:// URIs, and remote URIs (s3://, gs://, az://, etc.).\nSupports fsspec's caching via protocol chaining (e.g., 'simplecache::s3://...').\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., '/tmp/file.txt', 's3://bucket/key')\n    stream: If True, returns a file-like object instead of content (default: False)\n    encoding: Text encoding (default: 'utf-8')\n    cache: Caching mode - 'simple', 'file', 'block', or None (default: None)\n           - 'simple': Cache entire file locally (simplecache::)\n           - 'file': Cache to specific temp directory (filecache::)\n           - 'block': Cache in blocks for partial reads (blockcache::)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'content': str, 'success': True} on success (when stream=False)\n    {'file': file-like, 'success': True} on success (when stream=True)\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 220
        },
        {
          "name": "actions.notify",
          "function": "notify",
          "parameters": [
            {
              "name": "channel",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "message",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Send a notification.",
          "line_number": 293
        },
        {
          "name": "notify",
          "function": "notify",
          "parameters": [
            {
              "name": "channel",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "message",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Send a notification.",
          "line_number": 293
        },
        {
          "name": "checkpoint.save",
          "function": "checkpoint_save",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "graph",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "node",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Save checkpoint to specified path.\n\nArgs:\n    state: Current state dictionary\n    path: File path where checkpoint will be saved\n    graph: StateGraph instance (injected via context)\n    node: Current node name (injected via context)\n    config: Current config dict (injected via context)\n\nReturns:\n    {\"checkpoint_path\": str, \"saved\": True} on success\n    {\"checkpoint_path\": str, \"saved\": False, \"error\": str} on failure",
          "line_number": 302
        },
        {
          "name": "checkpoint.load",
          "function": "checkpoint_load",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Load checkpoint from specified path.\n\nArgs:\n    state: Current state (for template processing, not used)\n    path: File path to checkpoint\n\nReturns:\n    {\n        \"checkpoint_state\": dict,\n        \"checkpoint_node\": str,\n        \"checkpoint_config\": dict,\n        \"checkpoint_timestamp\": float,\n        \"checkpoint_version\": str\n    }\n    Or {\"error\": str} on failure",
          "line_number": 354
        }
      ]
    },
    {
      "file": "data_actions.py",
      "namespace": "data",
      "actions": [
        {
          "name": "json.parse",
          "function": "json_parse",
          "parameters": [
            {
              "name": "text",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "strict",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Parse a JSON string into a Python object.\n\nArgs:\n    state: Current state dictionary\n    text: JSON string to parse\n    strict: If True, use standard JSON parsing. If False, allow\n           trailing commas and comments (best effort)\n    default: Default value to return on parse error (only used\n            when strict=False)\n\nReturns:\n    {\"data\": any, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"parse\"} on failure",
          "line_number": 57
        },
        {
          "name": "json_parse",
          "function": "json_parse",
          "parameters": [
            {
              "name": "text",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "strict",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Parse a JSON string into a Python object.\n\nArgs:\n    state: Current state dictionary\n    text: JSON string to parse\n    strict: If True, use standard JSON parsing. If False, allow\n           trailing commas and comments (best effort)\n    default: Default value to return on parse error (only used\n            when strict=False)\n\nReturns:\n    {\"data\": any, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"parse\"} on failure",
          "line_number": 57
        },
        {
          "name": "json.transform",
          "function": "json_transform",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "expression",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "engine_type",
              "type": "Any",
              "required": false,
              "default": "'jmespath'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Transform data using JMESPath or JSONPath expressions.\n\nArgs:\n    state: Current state dictionary\n    data: Data to transform (dict or list)\n    expression: JMESPath or JSONPath expression string\n    engine_type: \"jmespath\" (default) or \"jsonpath\"\n\nReturns:\n    {\"result\": any, \"expression\": str, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"transform\"} on failure\n\nExamples:\n    expression: \"user.name\" -> extracts nested value\n    expression: \"[?status=='active'].name\" -> filters array\n    expression: \"{names: [].name, count: length(@)}\" -> projects fields",
          "line_number": 112
        },
        {
          "name": "json_transform",
          "function": "json_transform",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "expression",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "engine_type",
              "type": "Any",
              "required": false,
              "default": "'jmespath'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Transform data using JMESPath or JSONPath expressions.\n\nArgs:\n    state: Current state dictionary\n    data: Data to transform (dict or list)\n    expression: JMESPath or JSONPath expression string\n    engine_type: \"jmespath\" (default) or \"jsonpath\"\n\nReturns:\n    {\"result\": any, \"expression\": str, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"transform\"} on failure\n\nExamples:\n    expression: \"user.name\" -> extracts nested value\n    expression: \"[?status=='active'].name\" -> filters array\n    expression: \"{names: [].name, count: length(@)}\" -> projects fields",
          "line_number": 112
        },
        {
          "name": "json.stringify",
          "function": "json_stringify",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "indent",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "sort_keys",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Convert a Python object to a JSON string.\n\nArgs:\n    state: Current state dictionary\n    data: Python object to serialize\n    indent: Indentation level for pretty printing (None for compact)\n    sort_keys: Sort dictionary keys alphabetically\n\nReturns:\n    {\"text\": str, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"serialize\"} on failure",
          "line_number": 222
        },
        {
          "name": "json_stringify",
          "function": "json_stringify",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "indent",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "sort_keys",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Convert a Python object to a JSON string.\n\nArgs:\n    state: Current state dictionary\n    data: Python object to serialize\n    indent: Indentation level for pretty printing (None for compact)\n    sort_keys: Sort dictionary keys alphabetically\n\nReturns:\n    {\"text\": str, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"serialize\"} on failure",
          "line_number": 222
        },
        {
          "name": "csv.parse",
          "function": "csv_parse",
          "parameters": [
            {
              "name": "text",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "path",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "delimiter",
              "type": "Any",
              "required": false,
              "default": "','"
            },
            {
              "name": "has_header",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Parse CSV data from text or file.\n\nArgs:\n    state: Current state dictionary\n    text: CSV string to parse (mutually exclusive with path)\n    path: File path to read CSV from (mutually exclusive with text)\n    delimiter: Field delimiter character (default: \",\")\n    has_header: If True, first row is treated as headers\n\nReturns:\n    {\n        \"data\": List[dict] | List[List],\n        \"headers\": Optional[List[str]],\n        \"row_count\": int,\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False, \"error_type\": \"parse\"|\"io\"} on failure",
          "line_number": 250
        },
        {
          "name": "csv_parse",
          "function": "csv_parse",
          "parameters": [
            {
              "name": "text",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "path",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "delimiter",
              "type": "Any",
              "required": false,
              "default": "','"
            },
            {
              "name": "has_header",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Parse CSV data from text or file.\n\nArgs:\n    state: Current state dictionary\n    text: CSV string to parse (mutually exclusive with path)\n    path: File path to read CSV from (mutually exclusive with text)\n    delimiter: Field delimiter character (default: \",\")\n    has_header: If True, first row is treated as headers\n\nReturns:\n    {\n        \"data\": List[dict] | List[List],\n        \"headers\": Optional[List[str]],\n        \"row_count\": int,\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False, \"error_type\": \"parse\"|\"io\"} on failure",
          "line_number": 250
        },
        {
          "name": "csv.stringify",
          "function": "csv_stringify",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "headers",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "delimiter",
              "type": "Any",
              "required": false,
              "default": "','"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Convert a list of dicts or list of lists to a CSV string.\n\nArgs:\n    state: Current state dictionary\n    data: List of dicts or list of lists to convert\n    headers: Column headers (auto-detected from dict keys if not provided)\n    delimiter: Field delimiter character (default: \",\")\n\nReturns:\n    {\"text\": str, \"row_count\": int, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"serialize\"} on failure",
          "line_number": 359
        },
        {
          "name": "csv_stringify",
          "function": "csv_stringify",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "headers",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "delimiter",
              "type": "Any",
              "required": false,
              "default": "','"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Convert a list of dicts or list of lists to a CSV string.\n\nArgs:\n    state: Current state dictionary\n    data: List of dicts or list of lists to convert\n    headers: Column headers (auto-detected from dict keys if not provided)\n    delimiter: Field delimiter character (default: \",\")\n\nReturns:\n    {\"text\": str, \"row_count\": int, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"serialize\"} on failure",
          "line_number": 359
        },
        {
          "name": "data.validate",
          "function": "data_validate",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Validate data against a JSON Schema.\n\nArgs:\n    state: Current state dictionary\n    data: Data to validate\n    schema: JSON Schema to validate against\n\nReturns:\n    {\n        \"valid\": bool,\n        \"errors\": List[{\"path\": str, \"message\": str}],\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False, \"error_type\": \"validate\"} on failure",
          "line_number": 438
        },
        {
          "name": "data_validate",
          "function": "data_validate",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Validate data against a JSON Schema.\n\nArgs:\n    state: Current state dictionary\n    data: Data to validate\n    schema: JSON Schema to validate against\n\nReturns:\n    {\n        \"valid\": bool,\n        \"errors\": List[{\"path\": str, \"message\": str}],\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False, \"error_type\": \"validate\"} on failure",
          "line_number": 438
        },
        {
          "name": "data.merge",
          "function": "data_merge",
          "parameters": [
            {
              "name": "sources",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "strategy",
              "type": "Any",
              "required": false,
              "default": "'deep'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Merge multiple dictionaries/objects.\n\nArgs:\n    state: Current state dictionary\n    sources: List of dictionaries to merge\n    strategy: Merge strategy - \"shallow\", \"deep\", or \"replace\"\n        - shallow: Only merge top-level keys\n        - deep: Recursively merge nested dictionaries\n        - replace: Later sources completely replace earlier ones\n\nReturns:\n    {\"result\": dict, \"source_count\": int, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"merge\"} on failure",
          "line_number": 512
        },
        {
          "name": "data_merge",
          "function": "data_merge",
          "parameters": [
            {
              "name": "sources",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "strategy",
              "type": "Any",
              "required": false,
              "default": "'deep'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Merge multiple dictionaries/objects.\n\nArgs:\n    state: Current state dictionary\n    sources: List of dictionaries to merge\n    strategy: Merge strategy - \"shallow\", \"deep\", or \"replace\"\n        - shallow: Only merge top-level keys\n        - deep: Recursively merge nested dictionaries\n        - replace: Later sources completely replace earlier ones\n\nReturns:\n    {\"result\": dict, \"source_count\": int, \"success\": True} on success\n    {\"error\": str, \"success\": False, \"error_type\": \"merge\"} on failure",
          "line_number": 512
        },
        {
          "name": "data.filter",
          "function": "data_filter",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "predicate",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Filter list items using predicate expressions.\n\nArgs:\n    state: Current state dictionary\n    data: List to filter\n    predicate: Single predicate dict or list of predicates (AND logic)\n        Each predicate: {\"field\": str, \"op\": str, \"value\": any}\n        Supported ops: eq, ne, gt, gte, lt, lte, in, not_in, contains, startswith, endswith\n\nReturns:\n    {\n        \"result\": List,\n        \"original_count\": int,\n        \"filtered_count\": int,\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False, \"error_type\": \"filter\"} on failure\n\nExamples:\n    predicate: {\"field\": \"status\", \"op\": \"eq\", \"value\": \"active\"}\n    predicate: [\n        {\"field\": \"status\", \"op\": \"eq\", \"value\": \"active\"},\n        {\"field\": \"role\", \"op\": \"in\", \"value\": [\"admin\", \"moderator\"]}\n    ]",
          "line_number": 602
        },
        {
          "name": "data_filter",
          "function": "data_filter",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "predicate",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Filter list items using predicate expressions.\n\nArgs:\n    state: Current state dictionary\n    data: List to filter\n    predicate: Single predicate dict or list of predicates (AND logic)\n        Each predicate: {\"field\": str, \"op\": str, \"value\": any}\n        Supported ops: eq, ne, gt, gte, lt, lte, in, not_in, contains, startswith, endswith\n\nReturns:\n    {\n        \"result\": List,\n        \"original_count\": int,\n        \"filtered_count\": int,\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False, \"error_type\": \"filter\"} on failure\n\nExamples:\n    predicate: {\"field\": \"status\", \"op\": \"eq\", \"value\": \"active\"}\n    predicate: [\n        {\"field\": \"status\", \"op\": \"eq\", \"value\": \"active\"},\n        {\"field\": \"role\", \"op\": \"in\", \"value\": [\"admin\", \"moderator\"]}\n    ]",
          "line_number": 602
        }
      ]
    },
    {
      "file": "data_tabular_actions.py",
      "namespace": "data_tabular",
      "actions": [
        {
          "name": "data.create_table",
          "function": "data_create_table",
          "parameters": [
            {
              "name": "name",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Dict[str, str]",
              "required": true,
              "default": null
            },
            {
              "name": "primary_key",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Register a new tabular table in the catalog.\n\nTEA Custom Action: data.create_table\n\nArgs:\n    state: Current agent state\n    name: Table name (e.g., \"firm_scores\")\n    schema: Column definitions {\"column_name\": \"type\", ...}\n            Types: \"string\", \"integer\", \"float\", \"boolean\", \"timestamp\"\n    primary_key: List of columns forming the primary key\n\nReturns:\n    Dict with success, table, schema, primary_key, or error\n\nExample:\n    data.create_table(\n        name=\"firm_scores\",\n        schema={\"firm_id\": \"string\", \"score\": \"float\", \"category\": \"string\"},\n        primary_key=[\"firm_id\"]\n    )",
          "line_number": 223
        },
        {
          "name": "data.insert",
          "function": "data_insert",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "rows",
              "type": "List[Dict[str, Any]]",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Insert rows into a tabular table.\n\nTEA Custom Action: data.insert\n\nSmall batches (<1KB) are inlined in metadata store.\nLarge batches create Parquet files in blob storage.\n\nArgs:\n    state: Current agent state\n    table: Table name\n    rows: List of row dicts matching table schema\n\nReturns:\n    Dict with success, table, row_count, storage type, or error\n\nExample:\n    data.insert(\n        table=\"firm_scores\",\n        rows=[\n            {\"firm_id\": \"f1\", \"score\": 85.5, \"category\": \"A\"},\n            {\"firm_id\": \"f2\", \"score\": 92.0, \"category\": \"A\"}\n        ]\n    )",
          "line_number": 315
        },
        {
          "name": "data.update",
          "function": "data_update",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "where",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "updates",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Update rows matching WHERE clause.\n\nTEA Custom Action: data.update\n\nCreates new versioned records (append-only, does not modify original).\n\nArgs:\n    state: Current agent state\n    table: Table name\n    where: Primary key values to match {\"pk_col\": value, ...}\n    updates: Column values to update {\"col\": new_value, ...}\n\nReturns:\n    Dict with success, table, row_count, or error\n\nExample:\n    data.update(\n        table=\"firm_scores\",\n        where={\"firm_id\": \"f1\"},\n        updates={\"score\": 90.0, \"category\": \"A+\"}\n    )",
          "line_number": 544
        },
        {
          "name": "data.delete",
          "function": "data_delete",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "where",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delete rows matching WHERE clause.\n\nTEA Custom Action: data.delete\n\nCreates tombstone record (data preserved for audit).\n\nArgs:\n    state: Current agent state\n    table: Table name\n    where: Primary key values to match {\"pk_col\": value, ...}\n\nReturns:\n    Dict with success, table, row_count, or error\n\nExample:\n    data.delete(\n        table=\"firm_scores\",\n        where={\"firm_id\": \"f1\"}\n    )",
          "line_number": 660
        },
        {
          "name": "data.query",
          "function": "data_query",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "sql",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Query tabular data with SQL.\n\nTEA Custom Action: data.query\n\nMerges Parquet files + inlined data with Last-Write-Wins.\nExcludes tombstoned rows.\n\nArgs:\n    state: Current agent state\n    table: Table name\n    sql: SQL query (table aliased as 'data')\n         Example: \"SELECT * FROM data WHERE score > 80\"\n\nReturns:\n    Dict with success, table, rows, row_count, or error\n\nExample:\n    data.query(\n        table=\"firm_scores\",\n        sql=\"SELECT * FROM data WHERE score > 80 ORDER BY score DESC\"\n    )",
          "line_number": 750
        },
        {
          "name": "data.consolidate",
          "function": "data_consolidate",
          "parameters": [
            {
              "name": "table",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Full compaction: merge N Parquet files + inlined -> 1 Parquet file.\n\nTEA Custom Action: data.consolidate\n\nUse when:\n- Preparing for archival\n- Query performance optimization needed\n- Reducing file count and metadata documents\n\nArgs:\n    state: Current agent state\n    table: Table name to consolidate\n\nReturns:\n    Dict with consolidation statistics\n\nExample:\n    data.consolidate(table=\"firm_scores\")",
          "line_number": 1008
        }
      ]
    },
    {
      "file": "dspy_actions.py",
      "namespace": "dspy",
      "actions": [
        {
          "name": "reason.dspy.cot",
          "function": "reason_dspy_cot",
          "parameters": [
            {
              "name": "problem",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "signature",
              "type": "str",
              "required": false,
              "default": "'question -> thinking, answer'"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "few_shot_examples",
              "type": "Optional[List[Dict]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "compiled_key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chain-of-Thought reasoning using DSPy ChainOfThought module.\n\nWraps DSPy's ChainOfThought for model-agnostic CoT prompting with\ncompiled/optimized prompts. Falls back to native reason.cot when\nDSPy is unavailable.\n\nArgs:\n    state: Current state dictionary\n    problem: The problem or question to reason about\n    signature: DSPy signature string (default: \"question -> thinking, answer\")\n    model: LLM model to use (default: from settings or gpt-4)\n    few_shot_examples: Optional list of example dicts\n    temperature: LLM temperature (default: 0.7)\n    compiled_key: Key for pre-compiled module to use\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"thinking\": str,           # Chain-of-thought reasoning\n        \"answer\": any,             # Final answer\n        \"reasoning_trace\": list,   # Trace for observability\n        \"dspy_module\": str,        # \"ChainOfThought\" or \"native_fallback\"\n        \"model\": str\n    }\n    Or fallback to native reason.cot result if DSPy unavailable",
          "line_number": 82
        },
        {
          "name": "actions.reason_dspy_cot",
          "function": "reason_dspy_cot",
          "parameters": [
            {
              "name": "problem",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "signature",
              "type": "str",
              "required": false,
              "default": "'question -> thinking, answer'"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "few_shot_examples",
              "type": "Optional[List[Dict]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "compiled_key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chain-of-Thought reasoning using DSPy ChainOfThought module.\n\nWraps DSPy's ChainOfThought for model-agnostic CoT prompting with\ncompiled/optimized prompts. Falls back to native reason.cot when\nDSPy is unavailable.\n\nArgs:\n    state: Current state dictionary\n    problem: The problem or question to reason about\n    signature: DSPy signature string (default: \"question -> thinking, answer\")\n    model: LLM model to use (default: from settings or gpt-4)\n    few_shot_examples: Optional list of example dicts\n    temperature: LLM temperature (default: 0.7)\n    compiled_key: Key for pre-compiled module to use\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"thinking\": str,           # Chain-of-thought reasoning\n        \"answer\": any,             # Final answer\n        \"reasoning_trace\": list,   # Trace for observability\n        \"dspy_module\": str,        # \"ChainOfThought\" or \"native_fallback\"\n        \"model\": str\n    }\n    Or fallback to native reason.cot result if DSPy unavailable",
          "line_number": 82
        },
        {
          "name": "reason.dspy.react",
          "function": "reason_dspy_react",
          "parameters": [
            {
              "name": "goal",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "signature",
              "type": "str",
              "required": false,
              "default": "'goal -> result'"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "tools",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_steps",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "ReAct reasoning using DSPy ReAct module.\n\nWraps DSPy's ReAct for tool-using agents with compiled prompts.\nIntegrates with TEA tool bridges (MCP, CrewAI, LangChain).\nFalls back to native reason.react when DSPy is unavailable.\n\nArgs:\n    state: Current state dictionary\n    goal: The goal to achieve\n    signature: DSPy signature string (default: \"goal -> result\")\n    model: LLM model to use\n    tools: List of tool/action names to make available\n    max_steps: Maximum reasoning steps (default: 10)\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"steps\": list,             # Action-observation trace\n        \"final_answer\": any,       # Final result\n        \"reasoning_trace\": list,   # Trace for observability\n        \"dspy_module\": str,        # \"ReAct\" or \"native_fallback\"\n        \"model\": str,\n        \"total_steps\": int\n    }",
          "line_number": 260
        },
        {
          "name": "actions.reason_dspy_react",
          "function": "reason_dspy_react",
          "parameters": [
            {
              "name": "goal",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "signature",
              "type": "str",
              "required": false,
              "default": "'goal -> result'"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "tools",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_steps",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "ReAct reasoning using DSPy ReAct module.\n\nWraps DSPy's ReAct for tool-using agents with compiled prompts.\nIntegrates with TEA tool bridges (MCP, CrewAI, LangChain).\nFalls back to native reason.react when DSPy is unavailable.\n\nArgs:\n    state: Current state dictionary\n    goal: The goal to achieve\n    signature: DSPy signature string (default: \"goal -> result\")\n    model: LLM model to use\n    tools: List of tool/action names to make available\n    max_steps: Maximum reasoning steps (default: 10)\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"steps\": list,             # Action-observation trace\n        \"final_answer\": any,       # Final result\n        \"reasoning_trace\": list,   # Trace for observability\n        \"dspy_module\": str,        # \"ReAct\" or \"native_fallback\"\n        \"model\": str,\n        \"total_steps\": int\n    }",
          "line_number": 260
        },
        {
          "name": "reason.dspy.compile",
          "function": "reason_dspy_compile",
          "parameters": [
            {
              "name": "module_type",
              "type": "str",
              "required": false,
              "default": "'cot'"
            },
            {
              "name": "signature",
              "type": "str",
              "required": false,
              "default": "'question -> answer'"
            },
            {
              "name": "training_data",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "teleprompter",
              "type": "str",
              "required": false,
              "default": "'BootstrapFewShot'"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metric",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "output_key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compile a DSPy module with teleprompter for optimized prompts.\n\nUses DSPy's compilation capabilities to optimize prompts based on\ntraining examples. Compiled prompts can be persisted and reused.\n\nArgs:\n    state: Current state dictionary\n    module_type: Type of module to compile (\"cot\", \"react\", \"predict\")\n    signature: DSPy signature string\n    training_data: List of example dicts for optimization (or from state)\n    teleprompter: Teleprompter to use (BootstrapFewShot, BootstrapFewShotWithRandomSearch, MIPRO)\n    model: Model for compilation\n    metric: Metric function name (\"exact_match\" or custom)\n    output_key: Key to store compiled module in state\n    **kwargs: Additional teleprompter parameters\n\nReturns:\n    {\n        \"compiled\": True,\n        \"module_type\": str,\n        \"signature\": str,\n        \"teleprompter\": str,\n        \"training_examples\": int,\n        \"module_key\": str,       # Key for retrieving compiled module\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} if compilation fails",
          "line_number": 467
        },
        {
          "name": "actions.reason_dspy_compile",
          "function": "reason_dspy_compile",
          "parameters": [
            {
              "name": "module_type",
              "type": "str",
              "required": false,
              "default": "'cot'"
            },
            {
              "name": "signature",
              "type": "str",
              "required": false,
              "default": "'question -> answer'"
            },
            {
              "name": "training_data",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "teleprompter",
              "type": "str",
              "required": false,
              "default": "'BootstrapFewShot'"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metric",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "output_key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compile a DSPy module with teleprompter for optimized prompts.\n\nUses DSPy's compilation capabilities to optimize prompts based on\ntraining examples. Compiled prompts can be persisted and reused.\n\nArgs:\n    state: Current state dictionary\n    module_type: Type of module to compile (\"cot\", \"react\", \"predict\")\n    signature: DSPy signature string\n    training_data: List of example dicts for optimization (or from state)\n    teleprompter: Teleprompter to use (BootstrapFewShot, BootstrapFewShotWithRandomSearch, MIPRO)\n    model: Model for compilation\n    metric: Metric function name (\"exact_match\" or custom)\n    output_key: Key to store compiled module in state\n    **kwargs: Additional teleprompter parameters\n\nReturns:\n    {\n        \"compiled\": True,\n        \"module_type\": str,\n        \"signature\": str,\n        \"teleprompter\": str,\n        \"training_examples\": int,\n        \"module_key\": str,       # Key for retrieving compiled module\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} if compilation fails",
          "line_number": 467
        },
        {
          "name": "reason.dspy.optimize",
          "function": "reason_dspy_optimize",
          "parameters": [
            {
              "name": "module_key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "module_type",
              "type": "str",
              "required": false,
              "default": "'cot'"
            },
            {
              "name": "signature",
              "type": "str",
              "required": false,
              "default": "'question -> answer'"
            },
            {
              "name": "training_data",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "validation_data",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metric",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Run optimization against a validation set.\n\nCompiles and optimizes a DSPy module, evaluating against validation\nexamples to find the best configuration.\n\nArgs:\n    state: Current state dictionary\n    module_key: Key of pre-compiled module to optimize (or create new)\n    module_type: Type of module to optimize (\"cot\", \"react\", \"predict\")\n    signature: DSPy signature string\n    training_data: Training examples (or from state)\n    validation_data: Validation examples (or from state)\n    metric: Metric function (\"exact_match\" or custom)\n    model: Model for optimization\n    **kwargs: Additional optimization parameters\n\nReturns:\n    {\n        \"success\": True,\n        \"train_score\": float,\n        \"val_score\": float,\n        \"training_examples\": int,\n        \"validation_examples\": int,\n        \"module_key\": str,\n        \"best_config\": dict\n    }\n    Or {\"error\": str, \"success\": False} if optimization fails",
          "line_number": 672
        },
        {
          "name": "actions.reason_dspy_optimize",
          "function": "reason_dspy_optimize",
          "parameters": [
            {
              "name": "module_key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "module_type",
              "type": "str",
              "required": false,
              "default": "'cot'"
            },
            {
              "name": "signature",
              "type": "str",
              "required": false,
              "default": "'question -> answer'"
            },
            {
              "name": "training_data",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "validation_data",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metric",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Run optimization against a validation set.\n\nCompiles and optimizes a DSPy module, evaluating against validation\nexamples to find the best configuration.\n\nArgs:\n    state: Current state dictionary\n    module_key: Key of pre-compiled module to optimize (or create new)\n    module_type: Type of module to optimize (\"cot\", \"react\", \"predict\")\n    signature: DSPy signature string\n    training_data: Training examples (or from state)\n    validation_data: Validation examples (or from state)\n    metric: Metric function (\"exact_match\" or custom)\n    model: Model for optimization\n    **kwargs: Additional optimization parameters\n\nReturns:\n    {\n        \"success\": True,\n        \"train_score\": float,\n        \"val_score\": float,\n        \"training_examples\": int,\n        \"validation_examples\": int,\n        \"module_key\": str,\n        \"best_config\": dict\n    }\n    Or {\"error\": str, \"success\": False} if optimization fails",
          "line_number": 672
        },
        {
          "name": "reason.dspy.list_compiled",
          "function": "reason_dspy_list_compiled",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List all compiled DSPy modules.\n\nReturns:\n    {\n        \"modules\": list of module keys,\n        \"details\": dict of key -> metadata\n    }",
          "line_number": 876
        },
        {
          "name": "actions.reason_dspy_list_compiled",
          "function": "reason_dspy_list_compiled",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List all compiled DSPy modules.\n\nReturns:\n    {\n        \"modules\": list of module keys,\n        \"details\": dict of key -> metadata\n    }",
          "line_number": 876
        },
        {
          "name": "reason.dspy.export",
          "function": "reason_dspy_export",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Export all compiled DSPy prompts for checkpoint persistence.\n\nReturns:\n    {\n        \"prompts\": dict of key -> prompt config,\n        \"count\": int\n    }",
          "line_number": 910
        },
        {
          "name": "actions.reason_dspy_export",
          "function": "reason_dspy_export",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Export all compiled DSPy prompts for checkpoint persistence.\n\nReturns:\n    {\n        \"prompts\": dict of key -> prompt config,\n        \"count\": int\n    }",
          "line_number": 910
        },
        {
          "name": "reason.dspy.import",
          "function": "reason_dspy_import",
          "parameters": [
            {
              "name": "prompts",
              "type": "Optional[Dict[str, Dict]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Import compiled DSPy prompts from checkpoint persistence.\n\nArgs:\n    state: Current state dictionary\n    prompts: Dictionary of key -> prompt config (or from state)\n\nReturns:\n    {\n        \"imported\": int,\n        \"keys\": list of imported keys\n    }",
          "line_number": 932
        },
        {
          "name": "actions.reason_dspy_import",
          "function": "reason_dspy_import",
          "parameters": [
            {
              "name": "prompts",
              "type": "Optional[Dict[str, Dict]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Import compiled DSPy prompts from checkpoint persistence.\n\nArgs:\n    state: Current state dictionary\n    prompts: Dictionary of key -> prompt config (or from state)\n\nReturns:\n    {\n        \"imported\": int,\n        \"keys\": list of imported keys\n    }",
          "line_number": 932
        }
      ]
    },
    {
      "file": "error_actions.py",
      "namespace": "error",
      "actions": [
        {
          "name": "error.is_retryable",
          "function": "error_is_retryable",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 309
        },
        {
          "name": "error.clear",
          "function": "error_clear",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 310
        },
        {
          "name": "error.get",
          "function": "error_get",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 311
        },
        {
          "name": "error.has",
          "function": "error_has",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 312
        },
        {
          "name": "error.type",
          "function": "error_type",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 313
        },
        {
          "name": "error.retry",
          "function": "error_retry",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 314
        },
        {
          "name": "error.respond",
          "function": "error_respond",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 315
        },
        {
          "name": "actions.error_is_retryable",
          "function": "error_is_retryable",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 318
        },
        {
          "name": "actions.error_clear",
          "function": "error_clear",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 319
        },
        {
          "name": "actions.error_get",
          "function": "error_get",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 320
        },
        {
          "name": "actions.error_has",
          "function": "error_has",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 321
        },
        {
          "name": "actions.error_type",
          "function": "error_type",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 322
        },
        {
          "name": "actions.error_retry",
          "function": "error_retry",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 323
        },
        {
          "name": "actions.error_respond",
          "function": "error_respond",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 324
        }
      ]
    },
    {
      "file": "firestore_actions.py",
      "namespace": "firestore",
      "actions": [
        {
          "name": "firestore.get",
          "function": "wrapped_get",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 916
        },
        {
          "name": "firestore.set",
          "function": "wrapped_set",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 917
        },
        {
          "name": "firestore.query",
          "function": "wrapped_query",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 918
        },
        {
          "name": "firestore.delete",
          "function": "wrapped_delete",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 919
        },
        {
          "name": "firestore.batch",
          "function": "wrapped_batch",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 920
        },
        {
          "name": "actions.firestore_get",
          "function": "wrapped_get",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 923
        },
        {
          "name": "actions.firestore_set",
          "function": "wrapped_set",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 924
        },
        {
          "name": "actions.firestore_query",
          "function": "wrapped_query",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 925
        },
        {
          "name": "actions.firestore_delete",
          "function": "wrapped_delete",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 926
        },
        {
          "name": "actions.firestore_batch",
          "function": "wrapped_batch",
          "parameters": [],
          "return_type": "dict",
          "docstring": null,
          "line_number": 927
        }
      ]
    },
    {
      "file": "git_actions.py",
      "namespace": "git",
      "actions": [
        {
          "name": "git.worktree_create",
          "function": "_git_worktree_create",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 653
        },
        {
          "name": "git.worktree_remove",
          "function": "_git_worktree_remove",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 656
        },
        {
          "name": "git.worktree_merge",
          "function": "_git_worktree_merge",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 659
        },
        {
          "name": "git.worktree_list",
          "function": "_git_worktree_list",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 662
        },
        {
          "name": "git.current_branch",
          "function": "_git_current_branch",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 665
        },
        {
          "name": "git.status",
          "function": "_git_status",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 668
        },
        {
          "name": "actions.git_worktree_create",
          "function": "_git_worktree_create",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 653
        },
        {
          "name": "actions.git_worktree_remove",
          "function": "_git_worktree_remove",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 656
        },
        {
          "name": "actions.git_worktree_merge",
          "function": "_git_worktree_merge",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 659
        },
        {
          "name": "actions.git_worktree_list",
          "function": "_git_worktree_list",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 662
        },
        {
          "name": "actions.git_current_branch",
          "function": "_git_current_branch",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 665
        },
        {
          "name": "actions.git_status",
          "function": "_git_status",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": null,
          "line_number": 668
        }
      ]
    },
    {
      "file": "github_actions.py",
      "namespace": "github",
      "actions": [
        {
          "name": "github.list_issues",
          "function": "github_list_issues",
          "parameters": [
            {
              "name": "repo",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "labels",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "issue_state",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "milestone",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "assignee",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "creator",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mentioned",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "sort",
              "type": "str",
              "required": false,
              "default": "'created'"
            },
            {
              "name": "direction",
              "type": "str",
              "required": false,
              "default": "'desc'"
            },
            {
              "name": "since",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "per_page",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "page",
              "type": "int",
              "required": false,
              "default": "1"
            },
            {
              "name": "auto_paginate",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "max_pages",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "parse_body",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List issues from a GitHub repository.\n\nFetches issues from the GitHub REST API with optional filtering by labels,\nstate, milestone, and more. Can automatically extract task checkboxes from\nissue bodies using the markdown.parse action.\n\nArgs:\n    state: Current workflow state\n    repo: Repository in \"owner/repo\" format\n    labels: List of label names to filter by (comma-separated in API)\n    issue_state: Issue state filter: \"open\", \"closed\", or \"all\". Default: \"open\"\n    milestone: Milestone number, \"*\" for any, or \"none\"\n    assignee: Filter by assignee username, \"*\" for any, or \"none\"\n    creator: Filter by issue creator username\n    mentioned: Filter by mentioned username\n    sort: Sort field: \"created\", \"updated\", \"comments\". Default: \"created\"\n    direction: Sort direction: \"asc\" or \"desc\". Default: \"desc\"\n    since: Only issues updated after this ISO 8601 timestamp\n    per_page: Results per page (1-100). Default: 30\n    page: Page number for pagination. Default: 1\n    auto_paginate: Automatically fetch all pages up to max_pages. Default: False\n    max_pages: Maximum pages to fetch when auto_paginate is True. Default: 10\n    parse_body: Extract task checkboxes from issue body. Default: True\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"issues\": [\n            {\n                \"number\": int,\n                \"title\": str,\n                \"body\": str,\n                \"state\": str,\n                \"labels\": List[str],\n                \"assignees\": List[str],\n                \"milestone\": Optional[str],\n                \"created_at\": str,\n                \"updated_at\": str,\n                \"html_url\": str,\n                \"tasks\": List[dict]  # If parse_body=True\n            },\n            ...\n        ],\n        \"total_count\": int,\n        \"page\": int,\n        \"per_page\": int,\n        \"has_more\": bool\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str  # \"configuration\", \"authentication\", \"rate_limit\",\n                           # \"not_found\", \"api_error\", \"network\"\n    }\n\nExample YAML:\n    nodes:\n      - name: fetch_issues\n        uses: github.list_issues\n        with:\n          repo: \"owner/repo\"\n          labels: [\"ready-for-dev\", \"bug\"]\n          issue_state: open\n          parse_body: true\n        output: issues_result",
          "line_number": 132
        },
        {
          "name": "actions.github_list_issues",
          "function": "github_list_issues",
          "parameters": [
            {
              "name": "repo",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "labels",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "issue_state",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "milestone",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "assignee",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "creator",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mentioned",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "sort",
              "type": "str",
              "required": false,
              "default": "'created'"
            },
            {
              "name": "direction",
              "type": "str",
              "required": false,
              "default": "'desc'"
            },
            {
              "name": "since",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "per_page",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "page",
              "type": "int",
              "required": false,
              "default": "1"
            },
            {
              "name": "auto_paginate",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "max_pages",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "parse_body",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List issues from a GitHub repository.\n\nFetches issues from the GitHub REST API with optional filtering by labels,\nstate, milestone, and more. Can automatically extract task checkboxes from\nissue bodies using the markdown.parse action.\n\nArgs:\n    state: Current workflow state\n    repo: Repository in \"owner/repo\" format\n    labels: List of label names to filter by (comma-separated in API)\n    issue_state: Issue state filter: \"open\", \"closed\", or \"all\". Default: \"open\"\n    milestone: Milestone number, \"*\" for any, or \"none\"\n    assignee: Filter by assignee username, \"*\" for any, or \"none\"\n    creator: Filter by issue creator username\n    mentioned: Filter by mentioned username\n    sort: Sort field: \"created\", \"updated\", \"comments\". Default: \"created\"\n    direction: Sort direction: \"asc\" or \"desc\". Default: \"desc\"\n    since: Only issues updated after this ISO 8601 timestamp\n    per_page: Results per page (1-100). Default: 30\n    page: Page number for pagination. Default: 1\n    auto_paginate: Automatically fetch all pages up to max_pages. Default: False\n    max_pages: Maximum pages to fetch when auto_paginate is True. Default: 10\n    parse_body: Extract task checkboxes from issue body. Default: True\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"issues\": [\n            {\n                \"number\": int,\n                \"title\": str,\n                \"body\": str,\n                \"state\": str,\n                \"labels\": List[str],\n                \"assignees\": List[str],\n                \"milestone\": Optional[str],\n                \"created_at\": str,\n                \"updated_at\": str,\n                \"html_url\": str,\n                \"tasks\": List[dict]  # If parse_body=True\n            },\n            ...\n        ],\n        \"total_count\": int,\n        \"page\": int,\n        \"per_page\": int,\n        \"has_more\": bool\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str  # \"configuration\", \"authentication\", \"rate_limit\",\n                           # \"not_found\", \"api_error\", \"network\"\n    }\n\nExample YAML:\n    nodes:\n      - name: fetch_issues\n        uses: github.list_issues\n        with:\n          repo: \"owner/repo\"\n          labels: [\"ready-for-dev\", \"bug\"]\n          issue_state: open\n          parse_body: true\n        output: issues_result",
          "line_number": 132
        },
        {
          "name": "github.create_issue",
          "function": "github_create_issue",
          "parameters": [
            {
              "name": "repo",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "title",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "body",
              "type": "str",
              "required": false,
              "default": "''"
            },
            {
              "name": "labels",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "assignees",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "milestone",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create a new GitHub issue.\n\nArgs:\n    state: Current workflow state\n    repo: Repository in \"owner/repo\" format\n    title: Issue title (required)\n    body: Issue body (markdown supported)\n    labels: List of label names to apply\n    assignees: List of usernames to assign\n    milestone: Milestone number to associate\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"number\": int,\n        \"url\": str,\n        \"html_url\": str,\n        \"state\": str\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample YAML:\n    nodes:\n      - name: create_bug_report\n        uses: github.create_issue\n        with:\n          repo: \"owner/repo\"\n          title: \"Bug: {{ state.error_message }}\"\n          body: |\n            ## Error Report\n            **Version:** {{ state.version }}\n          labels: [\"bug\", \"auto-reported\"]\n        output: created_issue",
          "line_number": 512
        },
        {
          "name": "actions.github_create_issue",
          "function": "github_create_issue",
          "parameters": [
            {
              "name": "repo",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "title",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "body",
              "type": "str",
              "required": false,
              "default": "''"
            },
            {
              "name": "labels",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "assignees",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "milestone",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create a new GitHub issue.\n\nArgs:\n    state: Current workflow state\n    repo: Repository in \"owner/repo\" format\n    title: Issue title (required)\n    body: Issue body (markdown supported)\n    labels: List of label names to apply\n    assignees: List of usernames to assign\n    milestone: Milestone number to associate\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"number\": int,\n        \"url\": str,\n        \"html_url\": str,\n        \"state\": str\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample YAML:\n    nodes:\n      - name: create_bug_report\n        uses: github.create_issue\n        with:\n          repo: \"owner/repo\"\n          title: \"Bug: {{ state.error_message }}\"\n          body: |\n            ## Error Report\n            **Version:** {{ state.version }}\n          labels: [\"bug\", \"auto-reported\"]\n        output: created_issue",
          "line_number": 512
        },
        {
          "name": "github.update_issue",
          "function": "github_update_issue",
          "parameters": [
            {
              "name": "repo",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "issue_number",
              "type": "int",
              "required": true,
              "default": null
            },
            {
              "name": "issue_state",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "title",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "body",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "labels",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "labels_mode",
              "type": "str",
              "required": false,
              "default": "'replace'"
            },
            {
              "name": "assignees",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "milestone",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "comment",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Update an existing GitHub issue.\n\nArgs:\n    state: Current workflow state\n    repo: Repository in \"owner/repo\" format\n    issue_number: Issue number to update\n    issue_state: New state: \"open\" or \"closed\"\n    title: New issue title\n    body: New issue body\n    labels: Labels to apply\n    labels_mode: How to apply labels: \"replace\", \"add\", or \"remove\"\n    assignees: Assignees to set\n    milestone: Milestone number (use 0 to clear)\n    comment: Comment to add to the issue\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"number\": int,\n        \"state\": str,\n        \"updated\": True,\n        \"comment_id\": int  # If comment was added\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample YAML:\n    nodes:\n      - name: close_resolved\n        uses: github.update_issue\n        with:\n          repo: \"owner/repo\"\n          issue_number: \"{{ state.issue_number }}\"\n          issue_state: closed\n          comment: \"Fixed in {{ state.version }}\"\n          labels: [\"resolved\"]\n          labels_mode: add",
          "line_number": 739
        },
        {
          "name": "actions.github_update_issue",
          "function": "github_update_issue",
          "parameters": [
            {
              "name": "repo",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "issue_number",
              "type": "int",
              "required": true,
              "default": null
            },
            {
              "name": "issue_state",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "title",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "body",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "labels",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "labels_mode",
              "type": "str",
              "required": false,
              "default": "'replace'"
            },
            {
              "name": "assignees",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "milestone",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "comment",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Update an existing GitHub issue.\n\nArgs:\n    state: Current workflow state\n    repo: Repository in \"owner/repo\" format\n    issue_number: Issue number to update\n    issue_state: New state: \"open\" or \"closed\"\n    title: New issue title\n    body: New issue body\n    labels: Labels to apply\n    labels_mode: How to apply labels: \"replace\", \"add\", or \"remove\"\n    assignees: Assignees to set\n    milestone: Milestone number (use 0 to clear)\n    comment: Comment to add to the issue\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"number\": int,\n        \"state\": str,\n        \"updated\": True,\n        \"comment_id\": int  # If comment was added\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample YAML:\n    nodes:\n      - name: close_resolved\n        uses: github.update_issue\n        with:\n          repo: \"owner/repo\"\n          issue_number: \"{{ state.issue_number }}\"\n          issue_state: closed\n          comment: \"Fixed in {{ state.version }}\"\n          labels: [\"resolved\"]\n          labels_mode: add",
          "line_number": 739
        },
        {
          "name": "github.search_issues",
          "function": "github_search_issues",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "repo",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "issue_state",
              "type": "str",
              "required": false,
              "default": "'open'"
            },
            {
              "name": "labels",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "author",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "assignee",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "sort",
              "type": "str",
              "required": false,
              "default": "'best-match'"
            },
            {
              "name": "order",
              "type": "str",
              "required": false,
              "default": "'desc'"
            },
            {
              "name": "per_page",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "page",
              "type": "int",
              "required": false,
              "default": "1"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search GitHub issues using GitHub search syntax.\n\nArgs:\n    state: Current workflow state\n    query: Search query string (GitHub search syntax)\n    repo: Limit to specific repository \"owner/repo\"\n    issue_state: Filter by state: \"open\", \"closed\", or \"all\"\n    labels: Filter by labels\n    author: Filter by author username\n    assignee: Filter by assignee username\n    sort: Sort field: \"best-match\", \"created\", \"updated\", \"comments\"\n    order: Sort order: \"asc\" or \"desc\"\n    per_page: Results per page (1-100). Default: 30\n    page: Page number for pagination. Default: 1\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"total_count\": int,\n        \"items\": [\n            {\n                \"number\": int,\n                \"title\": str,\n                \"url\": str,\n                \"html_url\": str,\n                \"score\": float,  # Relevance score\n                \"labels\": List[str],\n                \"state\": str,\n                \"created_at\": str,\n                \"updated_at\": str\n            },\n            ...\n        ]\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample YAML:\n    nodes:\n      - name: find_similar\n        uses: github.search_issues\n        with:\n          repo: \"owner/repo\"\n          query: \"{{ state.error_signature }} in:body\"\n          labels: [\"bug\"]\n          issue_state: all\n        output: similar_issues",
          "line_number": 1030
        },
        {
          "name": "actions.github_search_issues",
          "function": "github_search_issues",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "repo",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "issue_state",
              "type": "str",
              "required": false,
              "default": "'open'"
            },
            {
              "name": "labels",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "author",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "assignee",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "sort",
              "type": "str",
              "required": false,
              "default": "'best-match'"
            },
            {
              "name": "order",
              "type": "str",
              "required": false,
              "default": "'desc'"
            },
            {
              "name": "per_page",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "page",
              "type": "int",
              "required": false,
              "default": "1"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search GitHub issues using GitHub search syntax.\n\nArgs:\n    state: Current workflow state\n    query: Search query string (GitHub search syntax)\n    repo: Limit to specific repository \"owner/repo\"\n    issue_state: Filter by state: \"open\", \"closed\", or \"all\"\n    labels: Filter by labels\n    author: Filter by author username\n    assignee: Filter by assignee username\n    sort: Sort field: \"best-match\", \"created\", \"updated\", \"comments\"\n    order: Sort order: \"asc\" or \"desc\"\n    per_page: Results per page (1-100). Default: 30\n    page: Page number for pagination. Default: 1\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"total_count\": int,\n        \"items\": [\n            {\n                \"number\": int,\n                \"title\": str,\n                \"url\": str,\n                \"html_url\": str,\n                \"score\": float,  # Relevance score\n                \"labels\": List[str],\n                \"state\": str,\n                \"created_at\": str,\n                \"updated_at\": str\n            },\n            ...\n        ]\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample YAML:\n    nodes:\n      - name: find_similar\n        uses: github.search_issues\n        with:\n          repo: \"owner/repo\"\n          query: \"{{ state.error_signature }} in:body\"\n          labels: [\"bug\"]\n          issue_state: all\n        output: similar_issues",
          "line_number": 1030
        }
      ]
    },
    {
      "file": "graph_actions.py",
      "namespace": "graph",
      "actions": [
        {
          "name": "graph.store_entity",
          "function": "graph_store_entity",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "entity_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "properties",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "text",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "embed",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Store an entity (node) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: Unique identifier for the entity\n    entity_type: Type/label of the entity (e.g., \"Person\", \"Document\")\n    properties: Optional properties dict\n    text: Optional text for embedding generation\n    embed: If True and text provided, generate embedding via embedding.create\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"type\": str, \"created\": bool, \"has_embedding\": bool}\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 102
        },
        {
          "name": "actions.graph_store_entity",
          "function": "graph_store_entity",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "entity_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "properties",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "text",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "embed",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Store an entity (node) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: Unique identifier for the entity\n    entity_type: Type/label of the entity (e.g., \"Person\", \"Document\")\n    properties: Optional properties dict\n    text: Optional text for embedding generation\n    embed: If True and text provided, generate embedding via embedding.create\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"type\": str, \"created\": bool, \"has_embedding\": bool}\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 102
        },
        {
          "name": "graph.store_relation",
          "function": "graph_store_relation",
          "parameters": [
            {
              "name": "from_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relation_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "properties",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Store a relation (edge) between two entities.\n\nArgs:\n    state: Current state dictionary\n    from_entity: Source entity ID\n    to_entity: Target entity ID\n    relation_type: Type of the relationship (e.g., \"KNOWS\", \"MENTIONS\")\n    properties: Optional properties dict\n\nReturns:\n    {\"success\": True, \"from\": str, \"to\": str, \"type\": str, \"created\": bool}\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 156
        },
        {
          "name": "actions.graph_store_relation",
          "function": "graph_store_relation",
          "parameters": [
            {
              "name": "from_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relation_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "properties",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Store a relation (edge) between two entities.\n\nArgs:\n    state: Current state dictionary\n    from_entity: Source entity ID\n    to_entity: Target entity ID\n    relation_type: Type of the relationship (e.g., \"KNOWS\", \"MENTIONS\")\n    properties: Optional properties dict\n\nReturns:\n    {\"success\": True, \"from\": str, \"to\": str, \"type\": str, \"created\": bool}\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 156
        },
        {
          "name": "graph.query",
          "function": "graph_query",
          "parameters": [
            {
              "name": "cypher",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "datalog",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "pgq",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "pattern",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "params",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "100"
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a Cypher/Datalog/SQL-PGQ query or pattern match.\n\nArgs:\n    state: Current state dictionary\n    cypher: Cypher query string (for KuzuBackend/Bighorn)\n    datalog: Datalog query string (for CozoBackend)\n    pgq: SQL/PGQ query string (for DuckPGQBackend)\n    pattern: Simplified pattern dict (alternative to raw queries)\n    params: Query parameters (substituted into query or Jinja2 template)\n    limit: Maximum results to return (default: 100)\n    timeout: Query timeout in seconds (not currently implemented)\n\nReturns:\n    {\"success\": True, \"results\": list, \"count\": int, \"query\": str}\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nNote:\n    Use 'cypher' for KuzuBackend, 'datalog' for CozoBackend,\n    'pgq' for DuckPGQBackend. The pattern parameter works with all backends.\n\nExample PGQ query:\n    FROM GRAPH_TABLE (knowledge_graph\n      MATCH (a:entities)-[r:relations]->(b:entities)\n      COLUMNS (a.id AS source, r.type AS relation, b.id AS target)\n    )\n    ORDER BY source\n    LIMIT 10",
          "line_number": 196
        },
        {
          "name": "actions.graph_query",
          "function": "graph_query",
          "parameters": [
            {
              "name": "cypher",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "datalog",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "pgq",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "pattern",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "params",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "100"
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a Cypher/Datalog/SQL-PGQ query or pattern match.\n\nArgs:\n    state: Current state dictionary\n    cypher: Cypher query string (for KuzuBackend/Bighorn)\n    datalog: Datalog query string (for CozoBackend)\n    pgq: SQL/PGQ query string (for DuckPGQBackend)\n    pattern: Simplified pattern dict (alternative to raw queries)\n    params: Query parameters (substituted into query or Jinja2 template)\n    limit: Maximum results to return (default: 100)\n    timeout: Query timeout in seconds (not currently implemented)\n\nReturns:\n    {\"success\": True, \"results\": list, \"count\": int, \"query\": str}\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nNote:\n    Use 'cypher' for KuzuBackend, 'datalog' for CozoBackend,\n    'pgq' for DuckPGQBackend. The pattern parameter works with all backends.\n\nExample PGQ query:\n    FROM GRAPH_TABLE (knowledge_graph\n      MATCH (a:entities)-[r:relations]->(b:entities)\n      COLUMNS (a.id AS source, r.type AS relation, b.id AS target)\n    )\n    ORDER BY source\n    LIMIT 10",
          "line_number": 196
        },
        {
          "name": "graph.retrieve_context",
          "function": "graph_retrieve_context",
          "parameters": [
            {
              "name": "query",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "embedding",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "entity_id",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "hops",
              "type": "Any",
              "required": false,
              "default": "2"
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "20"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Retrieve relevant subgraph context.\n\nCan be called with:\n- query: Text query (converted to embedding for HNSW search)\n- embedding: Direct embedding vector for HNSW search\n- entity_id: Start from entity and expand N hops\n\nArgs:\n    state: Current state dictionary\n    query: Text query for semantic search\n    embedding: Direct embedding vector\n    entity_id: Entity ID to start neighborhood expansion from\n    hops: Number of relationship hops to traverse (default: 2)\n    limit: Maximum entities to return (default: 20)\n\nReturns:\n    {\"success\": True, \"entities\": list, \"relations\": list, \"context_summary\": str}\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 274
        },
        {
          "name": "actions.graph_retrieve_context",
          "function": "graph_retrieve_context",
          "parameters": [
            {
              "name": "query",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "embedding",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "entity_id",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "hops",
              "type": "Any",
              "required": false,
              "default": "2"
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "20"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Retrieve relevant subgraph context.\n\nCan be called with:\n- query: Text query (converted to embedding for HNSW search)\n- embedding: Direct embedding vector for HNSW search\n- entity_id: Start from entity and expand N hops\n\nArgs:\n    state: Current state dictionary\n    query: Text query for semantic search\n    embedding: Direct embedding vector\n    entity_id: Entity ID to start neighborhood expansion from\n    hops: Number of relationship hops to traverse (default: 2)\n    limit: Maximum entities to return (default: 20)\n\nReturns:\n    {\"success\": True, \"entities\": list, \"relations\": list, \"context_summary\": str}\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 274
        },
        {
          "name": "graph.delete_entity",
          "function": "graph_delete_entity",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "detach",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete an entity (node) from the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: The unique identifier of the entity to delete\n    detach: If True (default), also delete all relationships connected\n           to this entity. If False, fail if relationships exist.\n\nReturns:\n    {\"success\": True, \"deleted\": True, \"entity_id\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 338
        },
        {
          "name": "actions.graph_delete_entity",
          "function": "graph_delete_entity",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "detach",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete an entity (node) from the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: The unique identifier of the entity to delete\n    detach: If True (default), also delete all relationships connected\n           to this entity. If False, fail if relationships exist.\n\nReturns:\n    {\"success\": True, \"deleted\": True, \"entity_id\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 338
        },
        {
          "name": "graph.delete_relation",
          "function": "graph_delete_relation",
          "parameters": [
            {
              "name": "from_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relation_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete a relation (edge) from the graph database.\n\nArgs:\n    state: Current state dictionary\n    from_entity: Source entity ID\n    to_entity: Target entity ID\n    relation_type: Type of the relationship to delete\n\nReturns:\n    {\"success\": True, \"deleted\": True}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 372
        },
        {
          "name": "actions.graph_delete_relation",
          "function": "graph_delete_relation",
          "parameters": [
            {
              "name": "from_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relation_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete a relation (edge) from the graph database.\n\nArgs:\n    state: Current state dictionary\n    from_entity: Source entity ID\n    to_entity: Target entity ID\n    relation_type: Type of the relationship to delete\n\nReturns:\n    {\"success\": True, \"deleted\": True}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 372
        },
        {
          "name": "graph.update_entity",
          "function": "graph_update_entity",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "properties",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "merge",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Update properties of an entity (node) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: The unique identifier of the entity to update\n    properties: Properties to set/merge\n    merge: If True (default), merge with existing properties.\n           If False, replace all properties.\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"properties\": dict}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 408
        },
        {
          "name": "actions.graph_update_entity",
          "function": "graph_update_entity",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "properties",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "merge",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Update properties of an entity (node) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: The unique identifier of the entity to update\n    properties: Properties to set/merge\n    merge: If True (default), merge with existing properties.\n           If False, replace all properties.\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"properties\": dict}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 408
        },
        {
          "name": "graph.update_relation",
          "function": "graph_update_relation",
          "parameters": [
            {
              "name": "from_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relation_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "properties",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "merge",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Update properties of a relation (edge) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    from_entity: Source entity ID\n    to_entity: Target entity ID\n    relation_type: Type of the relationship\n    properties: Properties to set/merge\n    merge: If True (default), merge with existing properties.\n\nReturns:\n    {\"success\": True, \"properties\": dict}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 450
        },
        {
          "name": "actions.graph_update_relation",
          "function": "graph_update_relation",
          "parameters": [
            {
              "name": "from_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relation_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "properties",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "merge",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Update properties of a relation (edge) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    from_entity: Source entity ID\n    to_entity: Target entity ID\n    relation_type: Type of the relationship\n    properties: Properties to set/merge\n    merge: If True (default), merge with existing properties.\n\nReturns:\n    {\"success\": True, \"properties\": dict}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 450
        },
        {
          "name": "graph.add_labels",
          "function": "graph_add_labels",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "labels",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Add labels to an entity (node) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: The unique identifier of the entity\n    labels: List of labels to add\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"labels_added\": list}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 499
        },
        {
          "name": "actions.graph_add_labels",
          "function": "graph_add_labels",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "labels",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Add labels to an entity (node) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: The unique identifier of the entity\n    labels: List of labels to add\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"labels_added\": list}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 499
        },
        {
          "name": "graph.remove_labels",
          "function": "graph_remove_labels",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "labels",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Remove labels from an entity (node) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: The unique identifier of the entity\n    labels: List of labels to remove\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"labels_removed\": list}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 537
        },
        {
          "name": "actions.graph_remove_labels",
          "function": "graph_remove_labels",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "labels",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Remove labels from an entity (node) in the graph database.\n\nArgs:\n    state: Current state dictionary\n    entity_id: The unique identifier of the entity\n    labels: List of labels to remove\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"labels_removed\": list}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 537
        },
        {
          "name": "graph.store_entities_batch",
          "function": "graph_store_entities_batch",
          "parameters": [
            {
              "name": "entities",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Bulk insert/update multiple entities in a single transaction.\n\nArgs:\n    state: Current state dictionary\n    entities: List of entity dictionaries, each with:\n        - entity_id: str (required)\n        - entity_type: str (required)\n        - properties: dict (optional)\n        - embedding: list[float] (optional)\n\nReturns:\n    {\"success\": True, \"processed_count\": int, \"created\": int, \"updated\": int}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 577
        },
        {
          "name": "actions.graph_store_entities_batch",
          "function": "graph_store_entities_batch",
          "parameters": [
            {
              "name": "entities",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Bulk insert/update multiple entities in a single transaction.\n\nArgs:\n    state: Current state dictionary\n    entities: List of entity dictionaries, each with:\n        - entity_id: str (required)\n        - entity_type: str (required)\n        - properties: dict (optional)\n        - embedding: list[float] (optional)\n\nReturns:\n    {\"success\": True, \"processed_count\": int, \"created\": int, \"updated\": int}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 577
        },
        {
          "name": "graph.store_relations_batch",
          "function": "graph_store_relations_batch",
          "parameters": [
            {
              "name": "relations",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Bulk create/update multiple relations in a single transaction.\n\nArgs:\n    state: Current state dictionary\n    relations: List of relation dictionaries, each with:\n        - from_entity: str (required)\n        - to_entity: str (required)\n        - relation_type: str (required)\n        - properties: dict (optional)\n\nReturns:\n    {\"success\": True, \"processed_count\": int, \"created\": int, \"updated\": int}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 611
        },
        {
          "name": "actions.graph_store_relations_batch",
          "function": "graph_store_relations_batch",
          "parameters": [
            {
              "name": "relations",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Bulk create/update multiple relations in a single transaction.\n\nArgs:\n    state: Current state dictionary\n    relations: List of relation dictionaries, each with:\n        - from_entity: str (required)\n        - to_entity: str (required)\n        - relation_type: str (required)\n        - properties: dict (optional)\n\nReturns:\n    {\"success\": True, \"processed_count\": int, \"created\": int, \"updated\": int}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 611
        },
        {
          "name": "graph.delete_entities_batch",
          "function": "graph_delete_entities_batch",
          "parameters": [
            {
              "name": "entity_ids",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "detach",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete multiple entities in a single transaction.\n\nArgs:\n    state: Current state dictionary\n    entity_ids: List of entity IDs to delete\n    detach: If True (default), also delete all relationships\n\nReturns:\n    {\"success\": True, \"deleted_count\": int, \"entity_ids\": list}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 645
        },
        {
          "name": "actions.graph_delete_entities_batch",
          "function": "graph_delete_entities_batch",
          "parameters": [
            {
              "name": "entity_ids",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "detach",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete multiple entities in a single transaction.\n\nArgs:\n    state: Current state dictionary\n    entity_ids: List of entity IDs to delete\n    detach: If True (default), also delete all relationships\n\nReturns:\n    {\"success\": True, \"deleted_count\": int, \"entity_ids\": list}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 645
        },
        {
          "name": "graph.merge_entity",
          "function": "graph_merge_entity",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "entity_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "on_create",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "on_match",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Conditional upsert with ON CREATE / ON MATCH semantics.\n\nArgs:\n    state: Current state dictionary\n    entity_id: Unique identifier for the entity\n    entity_type: Type/label of the entity\n    on_create: Properties to set only when creating (new entity)\n    on_match: Properties to set only when updating (existing entity)\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"created\": bool, \"properties\": dict}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 678
        },
        {
          "name": "actions.graph_merge_entity",
          "function": "graph_merge_entity",
          "parameters": [
            {
              "name": "entity_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "entity_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "on_create",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "on_match",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Conditional upsert with ON CREATE / ON MATCH semantics.\n\nArgs:\n    state: Current state dictionary\n    entity_id: Unique identifier for the entity\n    entity_type: Type/label of the entity\n    on_create: Properties to set only when creating (new entity)\n    on_match: Properties to set only when updating (existing entity)\n\nReturns:\n    {\"success\": True, \"entity_id\": str, \"created\": bool, \"properties\": dict}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 678
        },
        {
          "name": "graph.merge_relation",
          "function": "graph_merge_relation",
          "parameters": [
            {
              "name": "from_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relation_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "on_create",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "on_match",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Conditional upsert of a relation with ON CREATE / ON MATCH semantics.\n\nArgs:\n    state: Current state dictionary\n    from_entity: Source entity ID\n    to_entity: Target entity ID\n    relation_type: Type of the relationship\n    on_create: Properties to set only when creating (new relation)\n    on_match: Properties to set only when updating (existing relation)\n\nReturns:\n    {\"success\": True, \"created\": bool, \"properties\": dict}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 718
        },
        {
          "name": "actions.graph_merge_relation",
          "function": "graph_merge_relation",
          "parameters": [
            {
              "name": "from_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_entity",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relation_type",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "on_create",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "on_match",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Conditional upsert of a relation with ON CREATE / ON MATCH semantics.\n\nArgs:\n    state: Current state dictionary\n    from_entity: Source entity ID\n    to_entity: Target entity ID\n    relation_type: Type of the relationship\n    on_create: Properties to set only when creating (new relation)\n    on_match: Properties to set only when updating (existing relation)\n\nReturns:\n    {\"success\": True, \"created\": bool, \"properties\": dict}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 718
        },
        {
          "name": "graph.create",
          "function": "graph_create",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "vertex_tables",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "edge_tables",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Create a property graph from vertex and edge tables (DuckPGQ).\n\nThis action creates a SQL/PGQ property graph that can be queried\nusing the FROM GRAPH_TABLE syntax. Requires DuckPGQBackend.\n\nArgs:\n    state: Current state dictionary\n    name: Name of the property graph\n    vertex_tables: List of vertex table definitions:\n        [{\"name\": \"entities\", \"source\": \"path/to/entities.parquet\", \"key\": \"id\"}]\n    edge_tables: List of edge table definitions:\n        [{\"name\": \"relations\", \"source\": \"path/to/relations.parquet\",\n          \"source_key\": \"from_id\", \"destination_key\": \"to_id\",\n          \"references\": \"entities\"}]\n\nReturns:\n    {\"success\": True, \"graph\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: setup_graph\n      uses: graph.create\n      with:\n        name: knowledge_graph\n        vertex_tables:\n          - name: entities\n            source: \"s3://bucket/graph/entities.parquet\"\n            key: id\n        edge_tables:\n          - name: relations\n            source: \"s3://bucket/graph/relations.parquet\"\n            source_key: from_id\n            destination_key: to_id\n            references: entities",
          "line_number": 770
        },
        {
          "name": "actions.graph_create",
          "function": "graph_create",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "vertex_tables",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "edge_tables",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Create a property graph from vertex and edge tables (DuckPGQ).\n\nThis action creates a SQL/PGQ property graph that can be queried\nusing the FROM GRAPH_TABLE syntax. Requires DuckPGQBackend.\n\nArgs:\n    state: Current state dictionary\n    name: Name of the property graph\n    vertex_tables: List of vertex table definitions:\n        [{\"name\": \"entities\", \"source\": \"path/to/entities.parquet\", \"key\": \"id\"}]\n    edge_tables: List of edge table definitions:\n        [{\"name\": \"relations\", \"source\": \"path/to/relations.parquet\",\n          \"source_key\": \"from_id\", \"destination_key\": \"to_id\",\n          \"references\": \"entities\"}]\n\nReturns:\n    {\"success\": True, \"graph\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: setup_graph\n      uses: graph.create\n      with:\n        name: knowledge_graph\n        vertex_tables:\n          - name: entities\n            source: \"s3://bucket/graph/entities.parquet\"\n            key: id\n        edge_tables:\n          - name: relations\n            source: \"s3://bucket/graph/relations.parquet\"\n            source_key: from_id\n            destination_key: to_id\n            references: entities",
          "line_number": 770
        },
        {
          "name": "graph.drop",
          "function": "graph_drop",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Drop a property graph (DuckPGQ).\n\nArgs:\n    state: Current state dictionary\n    name: Name of the property graph to drop\n\nReturns:\n    {\"success\": True, \"graph\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 851
        },
        {
          "name": "actions.graph_drop",
          "function": "graph_drop",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Drop a property graph (DuckPGQ).\n\nArgs:\n    state: Current state dictionary\n    name: Name of the property graph to drop\n\nReturns:\n    {\"success\": True, \"graph\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 851
        },
        {
          "name": "graph.algorithm",
          "function": "graph_algorithm",
          "parameters": [
            {
              "name": "algorithm",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "graph",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "table",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "100"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run a graph algorithm (DuckPGQ).\n\nExecutes graph algorithms like PageRank, clustering coefficient,\nand connected components on a property graph.\n\nArgs:\n    state: Current state dictionary\n    algorithm: Algorithm name:\n        - \"pagerank\": PageRank centrality\n        - \"weakly_connected_component\" or \"wcc\": Find clusters\n        - \"local_clustering_coefficient\" or \"lcc\": Node connectivity\n    graph: Property graph name\n    table: Vertex table name to run algorithm on\n    limit: Maximum results to return (default: 100)\n\nReturns:\n    {\"success\": True, \"results\": list, \"count\": int, \"algorithm\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: compute_importance\n      uses: graph.algorithm\n      with:\n        algorithm: pagerank\n        graph: knowledge_graph\n        table: entities\n        limit: 100\n      output: important_entities",
          "line_number": 895
        },
        {
          "name": "actions.graph_algorithm",
          "function": "graph_algorithm",
          "parameters": [
            {
              "name": "algorithm",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "graph",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "table",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "100"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run a graph algorithm (DuckPGQ).\n\nExecutes graph algorithms like PageRank, clustering coefficient,\nand connected components on a property graph.\n\nArgs:\n    state: Current state dictionary\n    algorithm: Algorithm name:\n        - \"pagerank\": PageRank centrality\n        - \"weakly_connected_component\" or \"wcc\": Find clusters\n        - \"local_clustering_coefficient\" or \"lcc\": Node connectivity\n    graph: Property graph name\n    table: Vertex table name to run algorithm on\n    limit: Maximum results to return (default: 100)\n\nReturns:\n    {\"success\": True, \"results\": list, \"count\": int, \"algorithm\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: compute_importance\n      uses: graph.algorithm\n      with:\n        algorithm: pagerank\n        graph: knowledge_graph\n        table: entities\n        limit: 100\n      output: important_entities",
          "line_number": 895
        },
        {
          "name": "graph.shortest_path",
          "function": "graph_shortest_path",
          "parameters": [
            {
              "name": "graph",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "from_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "edge_table",
              "type": "Any",
              "required": false,
              "default": "'edges'"
            },
            {
              "name": "vertex_table",
              "type": "Any",
              "required": false,
              "default": "'vertices'"
            },
            {
              "name": "max_hops",
              "type": "Any",
              "required": false,
              "default": "10"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Find shortest path between two entities (DuckPGQ).\n\nUses SQL/PGQ ANY SHORTEST path query to find the shortest\npath between two entities in a property graph.\n\nArgs:\n    state: Current state dictionary\n    graph: Property graph name\n    from_id: Source entity ID\n    to_id: Target entity ID\n    edge_table: Edge table name/label (default: \"edges\")\n    vertex_table: Vertex table name/label (default: \"vertices\")\n    max_hops: Maximum path length (default: 10)\n\nReturns:\n    {\"success\": True, \"path\": list, \"hops\": int}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: find_path\n      uses: graph.shortest_path\n      with:\n        graph: knowledge_graph\n        from_id: \"{{ state.source_entity }}\"\n        to_id: \"{{ state.target_entity }}\"\n        edge_table: relations\n        vertex_table: entities\n        max_hops: 5\n      output: path_result",
          "line_number": 978
        },
        {
          "name": "actions.graph_shortest_path",
          "function": "graph_shortest_path",
          "parameters": [
            {
              "name": "graph",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "from_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "to_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "edge_table",
              "type": "Any",
              "required": false,
              "default": "'edges'"
            },
            {
              "name": "vertex_table",
              "type": "Any",
              "required": false,
              "default": "'vertices'"
            },
            {
              "name": "max_hops",
              "type": "Any",
              "required": false,
              "default": "10"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Find shortest path between two entities (DuckPGQ).\n\nUses SQL/PGQ ANY SHORTEST path query to find the shortest\npath between two entities in a property graph.\n\nArgs:\n    state: Current state dictionary\n    graph: Property graph name\n    from_id: Source entity ID\n    to_id: Target entity ID\n    edge_table: Edge table name/label (default: \"edges\")\n    vertex_table: Vertex table name/label (default: \"vertices\")\n    max_hops: Maximum path length (default: 10)\n\nReturns:\n    {\"success\": True, \"path\": list, \"hops\": int}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: find_path\n      uses: graph.shortest_path\n      with:\n        graph: knowledge_graph\n        from_id: \"{{ state.source_entity }}\"\n        to_id: \"{{ state.target_entity }}\"\n        edge_table: relations\n        vertex_table: entities\n        max_hops: 5\n      output: path_result",
          "line_number": 978
        },
        {
          "name": "graph.list_graphs",
          "function": "graph_list_graphs",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List all created property graphs (DuckPGQ).\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\"success\": True, \"graphs\": list}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 1058
        },
        {
          "name": "actions.graph_list_graphs",
          "function": "graph_list_graphs",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List all created property graphs (DuckPGQ).\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\"success\": True, \"graphs\": list}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 1058
        },
        {
          "name": "graph.vector_search",
          "function": "graph_vector_search",
          "parameters": [
            {
              "name": "embedding",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "10"
            },
            {
              "name": "index_name",
              "type": "Any",
              "required": false,
              "default": "'entity_embeddings'"
            },
            {
              "name": "threshold",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Perform vector similarity search using Neo4j Vector Index.\n\nThis action uses Neo4j's native vector search capabilities (Neo4j 5.11+)\nto find entities with similar embeddings.\n\nArgs:\n    state: Current state dictionary\n    embedding: Query embedding vector (list of floats)\n    limit: Maximum number of results (default: 10)\n    index_name: Name of the vector index to query (default: \"entity_embeddings\")\n    threshold: Minimum similarity score filter (optional)\n\nReturns:\n    {\n        \"success\": True,\n        \"results\": [\n            {\"entity_id\": str, \"entity_type\": str, \"properties\": dict, \"score\": float},\n            ...\n        ],\n        \"count\": int\n    }\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: find_similar\n      uses: graph.vector_search\n      with:\n        embedding: \"{{ state.query_embedding }}\"\n        limit: 10\n        index_name: entity_embeddings\n        threshold: 0.8\n      output: similar_entities",
          "line_number": 1098
        },
        {
          "name": "actions.graph_vector_search",
          "function": "graph_vector_search",
          "parameters": [
            {
              "name": "embedding",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "10"
            },
            {
              "name": "index_name",
              "type": "Any",
              "required": false,
              "default": "'entity_embeddings'"
            },
            {
              "name": "threshold",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Perform vector similarity search using Neo4j Vector Index.\n\nThis action uses Neo4j's native vector search capabilities (Neo4j 5.11+)\nto find entities with similar embeddings.\n\nArgs:\n    state: Current state dictionary\n    embedding: Query embedding vector (list of floats)\n    limit: Maximum number of results (default: 10)\n    index_name: Name of the vector index to query (default: \"entity_embeddings\")\n    threshold: Minimum similarity score filter (optional)\n\nReturns:\n    {\n        \"success\": True,\n        \"results\": [\n            {\"entity_id\": str, \"entity_type\": str, \"properties\": dict, \"score\": float},\n            ...\n        ],\n        \"count\": int\n    }\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: find_similar\n      uses: graph.vector_search\n      with:\n        embedding: \"{{ state.query_embedding }}\"\n        limit: 10\n        index_name: entity_embeddings\n        threshold: 0.8\n      output: similar_entities",
          "line_number": 1098
        },
        {
          "name": "graph.create_vector_index",
          "function": "graph_create_vector_index",
          "parameters": [
            {
              "name": "index_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "label",
              "type": "Any",
              "required": false,
              "default": "'Entity'"
            },
            {
              "name": "property_name",
              "type": "Any",
              "required": false,
              "default": "'_embedding'"
            },
            {
              "name": "dimensions",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "similarity",
              "type": "Any",
              "required": false,
              "default": "'cosine'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Create a vector index in Neo4j for similarity search.\n\nCreates a native vector index in Neo4j 5.11+ for efficient similarity\nsearch on embeddings stored in entity properties.\n\nArgs:\n    state: Current state dictionary\n    index_name: Name for the vector index\n    label: Node label to index (default: \"Entity\")\n    property_name: Property containing embeddings (default: \"_embedding\")\n    dimensions: Embedding dimensions (default: backend's embedding_dim)\n    similarity: Similarity function - \"cosine\", \"euclidean\", or \"dot_product\"\n\nReturns:\n    {\"success\": True, \"index_name\": str, \"created\": bool}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: setup_index\n      uses: graph.create_vector_index\n      with:\n        index_name: knowledge_embeddings\n        label: Entity\n        dimensions: 1536\n        similarity: cosine\n      output: index_result",
          "line_number": 1178
        },
        {
          "name": "actions.graph_create_vector_index",
          "function": "graph_create_vector_index",
          "parameters": [
            {
              "name": "index_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "label",
              "type": "Any",
              "required": false,
              "default": "'Entity'"
            },
            {
              "name": "property_name",
              "type": "Any",
              "required": false,
              "default": "'_embedding'"
            },
            {
              "name": "dimensions",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "similarity",
              "type": "Any",
              "required": false,
              "default": "'cosine'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Create a vector index in Neo4j for similarity search.\n\nCreates a native vector index in Neo4j 5.11+ for efficient similarity\nsearch on embeddings stored in entity properties.\n\nArgs:\n    state: Current state dictionary\n    index_name: Name for the vector index\n    label: Node label to index (default: \"Entity\")\n    property_name: Property containing embeddings (default: \"_embedding\")\n    dimensions: Embedding dimensions (default: backend's embedding_dim)\n    similarity: Similarity function - \"cosine\", \"euclidean\", or \"dot_product\"\n\nReturns:\n    {\"success\": True, \"index_name\": str, \"created\": bool}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: setup_index\n      uses: graph.create_vector_index\n      with:\n        index_name: knowledge_embeddings\n        label: Entity\n        dimensions: 1536\n        similarity: cosine\n      output: index_result",
          "line_number": 1178
        },
        {
          "name": "graph.drop_vector_index",
          "function": "graph_drop_vector_index",
          "parameters": [
            {
              "name": "index_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Drop a vector index from Neo4j.\n\nArgs:\n    state: Current state dictionary\n    index_name: Name of the index to drop\n\nReturns:\n    {\"success\": True, \"index_name\": str, \"dropped\": bool}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: cleanup_index\n      uses: graph.drop_vector_index\n      with:\n        index_name: knowledge_embeddings",
          "line_number": 1253
        },
        {
          "name": "actions.graph_drop_vector_index",
          "function": "graph_drop_vector_index",
          "parameters": [
            {
              "name": "index_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Drop a vector index from Neo4j.\n\nArgs:\n    state: Current state dictionary\n    index_name: Name of the index to drop\n\nReturns:\n    {\"success\": True, \"index_name\": str, \"dropped\": bool}\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: cleanup_index\n      uses: graph.drop_vector_index\n      with:\n        index_name: knowledge_embeddings",
          "line_number": 1253
        },
        {
          "name": "graph.list_vector_indexes",
          "function": "graph_list_vector_indexes",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List all vector indexes in Neo4j.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\n        \"success\": True,\n        \"indexes\": [\n            {\"name\": str, \"label\": str, \"property\": str, \"dimensions\": int, \"similarity\": str},\n            ...\n        ],\n        \"count\": int\n    }\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: check_indexes\n      uses: graph.list_vector_indexes\n      output: available_indexes",
          "line_number": 1303
        },
        {
          "name": "actions.graph_list_vector_indexes",
          "function": "graph_list_vector_indexes",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List all vector indexes in Neo4j.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\n        \"success\": True,\n        \"indexes\": [\n            {\"name\": str, \"label\": str, \"property\": str, \"dimensions\": int, \"similarity\": str},\n            ...\n        ],\n        \"count\": int\n    }\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: check_indexes\n      uses: graph.list_vector_indexes\n      output: available_indexes",
          "line_number": 1303
        },
        {
          "name": "graph.check_vector_support",
          "function": "graph_check_vector_support",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if the Neo4j instance supports vector indexes.\n\nVector indexes require Neo4j 5.11 or higher. This action checks\nthe Neo4j version and reports whether vector operations are available.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\n        \"success\": True,\n        \"supported\": bool,\n        \"version\": str,\n        \"message\": str\n    }\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: check_vector\n      uses: graph.check_vector_support\n      output: vector_support\n    - name: maybe_create_index\n      when: \"{{ state.vector_support.supported }}\"\n      uses: graph.create_vector_index\n      with:\n        index_name: entity_embeddings",
          "line_number": 1351
        },
        {
          "name": "actions.graph_check_vector_support",
          "function": "graph_check_vector_support",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if the Neo4j instance supports vector indexes.\n\nVector indexes require Neo4j 5.11 or higher. This action checks\nthe Neo4j version and reports whether vector operations are available.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\n        \"success\": True,\n        \"supported\": bool,\n        \"version\": str,\n        \"message\": str\n    }\n    or {\"success\": False, \"error\": str, \"error_type\": str} on failure\n\nExample YAML:\n    - name: check_vector\n      uses: graph.check_vector_support\n      output: vector_support\n    - name: maybe_create_index\n      when: \"{{ state.vector_support.supported }}\"\n      uses: graph.create_vector_index\n      with:\n        index_name: entity_embeddings",
          "line_number": 1351
        }
      ]
    },
    {
      "file": "http_response_actions.py",
      "namespace": "http_response",
      "actions": [
        {
          "name": "http.respond",
          "function": "http_respond_sync",
          "parameters": [
            {
              "name": "status",
              "type": "int",
              "required": false,
              "default": "200"
            },
            {
              "name": "body",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "headers",
              "type": "Optional[Dict[str, str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "content_type",
              "type": "str",
              "required": false,
              "default": "'application/json'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "None",
          "docstring": "Synchronous version of http.respond.\n\nArgs:\n    status: HTTP status code.\n    body: Response body.\n    headers: HTTP headers.\n    content_type: Content-Type header.\n\nRaises:\n    HTTPResponse: Always raised to signal termination.",
          "line_number": 79
        },
        {
          "name": "actions.http_respond",
          "function": "http_respond_sync",
          "parameters": [
            {
              "name": "status",
              "type": "int",
              "required": false,
              "default": "200"
            },
            {
              "name": "body",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "headers",
              "type": "Optional[Dict[str, str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "content_type",
              "type": "str",
              "required": false,
              "default": "'application/json'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "None",
          "docstring": "Synchronous version of http.respond.\n\nArgs:\n    status: HTTP status code.\n    body: Response body.\n    headers: HTTP headers.\n    content_type: Content-Type header.\n\nRaises:\n    HTTPResponse: Always raised to signal termination.",
          "line_number": 79
        }
      ]
    },
    {
      "file": "input_validation_actions.py",
      "namespace": "input_validation",
      "actions": [
        {
          "name": "validate.input",
          "function": "validate_input_action",
          "parameters": [
            {
              "name": "data",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "raise_on_error",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Validate input data against a schema (AC10).\n\nThis action allows explicit validation within an agent flow,\nuseful for validating user-provided data mid-execution or\nvalidating data before processing.\n\nArgs:\n    state: Current workflow state\n    data: Data to validate (defaults to entire state if not provided)\n    schema: Schema definition dict (inline or from data section)\n    raise_on_error: If True, raises ValidationError instead of returning result\n\nReturns:\n    Dict with:\n        - valid: bool - Whether validation passed\n        - data: validated/coerced data (if valid)\n        - errors: List of error dicts (if invalid)\n\nExample YAML:\n    - name: validate_user_input\n      uses: validate.input\n      with:\n        data: \"{{ state.user_data }}\"\n        schema:\n          name:\n            type: str\n            required: true\n          email:\n            type: str\n            pattern: \"^[\\w.-]+@[\\w.-]+\\.\\w+$\"\n      output: validation_result\n\n    # Check result\n    - name: check_validation\n      condition: \"{{ state.validation_result.valid }}\"",
          "line_number": 48
        },
        {
          "name": "actions.validate_input",
          "function": "validate_input_action",
          "parameters": [
            {
              "name": "data",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "raise_on_error",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Validate input data against a schema (AC10).\n\nThis action allows explicit validation within an agent flow,\nuseful for validating user-provided data mid-execution or\nvalidating data before processing.\n\nArgs:\n    state: Current workflow state\n    data: Data to validate (defaults to entire state if not provided)\n    schema: Schema definition dict (inline or from data section)\n    raise_on_error: If True, raises ValidationError instead of returning result\n\nReturns:\n    Dict with:\n        - valid: bool - Whether validation passed\n        - data: validated/coerced data (if valid)\n        - errors: List of error dicts (if invalid)\n\nExample YAML:\n    - name: validate_user_input\n      uses: validate.input\n      with:\n        data: \"{{ state.user_data }}\"\n        schema:\n          name:\n            type: str\n            required: true\n          email:\n            type: str\n            pattern: \"^[\\w.-]+@[\\w.-]+\\.\\w+$\"\n      output: validation_result\n\n    # Check result\n    - name: check_validation\n      condition: \"{{ state.validation_result.valid }}\"",
          "line_number": 48
        },
        {
          "name": "validate.schema",
          "function": "create_validator_action",
          "parameters": [
            {
              "name": "schema",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create a reusable schema validator.\n\nArgs:\n    state: Current workflow state\n    schema: Schema definition dict\n\nReturns:\n    Dict with:\n        - schema: Parsed InputSchema instance (for reuse)",
          "line_number": 142
        },
        {
          "name": "actions.validate_schema",
          "function": "create_validator_action",
          "parameters": [
            {
              "name": "schema",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create a reusable schema validator.\n\nArgs:\n    state: Current workflow state\n    schema: Schema definition dict\n\nReturns:\n    Dict with:\n        - schema: Parsed InputSchema instance (for reuse)",
          "line_number": 142
        }
      ]
    },
    {
      "file": "llamaextract_actions.py",
      "namespace": "llamaextract",
      "actions": [
        {
          "name": "llamaextract.extract",
          "function": "llamaextract_extract",
          "parameters": [
            {
              "name": "file",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_name",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mode",
              "type": "str",
              "required": false,
              "default": "'BALANCED'"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "300"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "use_rest",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "async_mode",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "polling_interval",
              "type": "Union[int, float]",
              "required": false,
              "default": "5"
            },
            {
              "name": "max_poll_attempts",
              "type": "int",
              "required": false,
              "default": "120"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Extract structured data from a document using LlamaExtract.\n\nUses SDK by default for client-side validation before sending to server.\nSet use_rest=True for direct REST API calls (fewer dependencies).\n\nTEA-BUILTIN-008.6: Async polling configuration parameters.\n\nArgs:\n    state: Current workflow state\n    file: Document source - URL, base64 content, or local file path\n    schema: JSON Schema defining the structure to extract.\n           Either schema or agent_id/agent_name must be provided.\n    agent_id: ID of existing extraction agent to use\n    agent_name: Name of existing extraction agent to use\n    mode: Extraction mode - BALANCED, MULTIMODAL, PREMIUM, or FAST\n    timeout: HTTP request timeout in seconds (default: 300)\n    max_retries: Maximum retry attempts for transient failures\n    use_rest: If True, use direct REST API instead of SDK (default: False)\n    async_mode: If True, use async /jobs endpoint explicitly (AC-1, AC-6)\n               Requires use_rest=True. When False (default), uses sync behavior.\n    polling_interval: Seconds between poll requests (default: 5) (AC-2, AC-7)\n                     Accepts floats for sub-second precision.\n    max_poll_attempts: Maximum number of poll attempts (default: 120) (AC-3, AC-8)\n                      Combined with polling_interval, default allows ~10 min.\n\nReturns:\n    Dict with 'success', 'data' (extracted data), 'status', etc.\n\nExample:\n    >>> result = llamaextract_extract(\n    ...     state={},\n    ...     file=\"https://example.com/invoice.pdf\",\n    ...     schema={\n    ...         \"type\": \"object\",\n    ...         \"properties\": {\n    ...             \"invoice_number\": {\"type\": \"string\"},\n    ...             \"total\": {\"type\": \"number\"}\n    ...         }\n    ...     },\n    ...     mode=\"PREMIUM\",\n    ...     timeout=300\n    ... )\n\nExample with async mode (for large documents):\n    >>> result = llamaextract_extract(\n    ...     state={},\n    ...     file=\"large-contract.pdf\",\n    ...     schema={...},\n    ...     use_rest=True,\n    ...     async_mode=True,\n    ...     polling_interval=10,\n    ...     max_poll_attempts=60,\n    ...     timeout=900\n    ... )",
          "line_number": 856
        },
        {
          "name": "llamaextract.upload_agent",
          "function": "llamaextract_upload_agent",
          "parameters": [
            {
              "name": "name",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "description",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mode",
              "type": "str",
              "required": false,
              "default": "'BALANCED'"
            },
            {
              "name": "force",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create or update an extraction agent.\n\nArgs:\n    state: Current workflow state\n    name: Agent name (must be unique)\n    schema: JSON Schema for extraction\n    description: Agent description\n    mode: Default extraction mode\n    force: If True, update existing agent with same name\n    max_retries: Maximum retry attempts\n\nReturns:\n    Dict with 'success', 'agent_id', 'name', 'status'\n\nExample:\n    >>> result = llamaextract_upload_agent(\n    ...     state={},\n    ...     name=\"invoice-extractor\",\n    ...     schema={\"type\": \"object\", \"properties\": {...}},\n    ...     mode=\"PREMIUM\"\n    ... )",
          "line_number": 1637
        },
        {
          "name": "llamaextract.list_agents",
          "function": "llamaextract_list_agents",
          "parameters": [
            {
              "name": "name_filter",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List available extraction agents.\n\nArgs:\n    state: Current workflow state\n    name_filter: Optional name filter (substring match)\n\nReturns:\n    Dict with 'success', 'agents' (list of agent info)\n\nExample:\n    >>> result = llamaextract_list_agents(state={})\n    >>> for agent in result['agents']:\n    ...     print(agent['name'], agent['id'])",
          "line_number": 1750
        },
        {
          "name": "llamaextract.get_agent",
          "function": "llamaextract_get_agent",
          "parameters": [
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_name",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get extraction agent details.\n\nArgs:\n    state: Current workflow state\n    agent_id: Agent ID (preferred)\n    agent_name: Agent name (alternative lookup)\n\nReturns:\n    Dict with 'success', 'agent' containing full configuration\n\nExample:\n    >>> result = llamaextract_get_agent(\n    ...     state={},\n    ...     agent_name=\"invoice-extractor\"\n    ... )\n    >>> print(result['agent']['schema'])",
          "line_number": 1799
        },
        {
          "name": "llamaextract.delete_agent",
          "function": "llamaextract_delete_agent",
          "parameters": [
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_name",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delete an extraction agent.\n\nArgs:\n    state: Current workflow state\n    agent_id: Agent ID (preferred)\n    agent_name: Agent name (alternative lookup)\n\nReturns:\n    Dict with 'success' and status info\n\nExample:\n    >>> result = llamaextract_delete_agent(\n    ...     state={},\n    ...     agent_name=\"old-extractor\"\n    ... )",
          "line_number": 1866
        },
        {
          "name": "llamaextract.submit_job",
          "function": "llamaextract_submit_job",
          "parameters": [
            {
              "name": "file",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mode",
              "type": "str",
              "required": false,
              "default": "'BALANCED'"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Submit async extraction job to LlamaExtract.\n\nTEA-BUILTIN-008.7: Workflow primitive for job submission.\n\nThis primitive submits a document extraction job and returns immediately\nwith the job_id. Use llamaextract.poll_status and llamaextract.get_result\nto monitor and retrieve results.\n\nArgs:\n    state: Current workflow state\n    file: Document source - URL, base64 content, or local file path\n    schema: JSON Schema defining the structure to extract\n    agent_id: ID of existing extraction agent to use\n    mode: Extraction mode - BALANCED, MULTIMODAL, PREMIUM, or FAST\n    max_retries: Maximum retry attempts for job submission\n\nReturns:\n    Dict with:\n    - success: True if job submitted successfully\n    - job_id: The extraction job ID for polling\n    - status: \"PENDING\" on successful submission\n    - error/error_type: On failure\n\nExample:\n    >>> result = llamaextract_submit_job(\n    ...     state={},\n    ...     file=\"invoice.pdf\",\n    ...     schema={\"type\": \"object\", \"properties\": {\"total\": {\"type\": \"number\"}}},\n    ...     mode=\"FAST\"\n    ... )\n    >>> if result['success']:\n    ...     job_id = result['job_id']  # Use for polling",
          "line_number": 1089
        },
        {
          "name": "llamaextract.poll_status",
          "function": "llamaextract_poll_status",
          "parameters": [
            {
              "name": "job_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Poll job status from LlamaExtract.\n\nTEA-BUILTIN-008.7: Workflow primitive for status polling.\n\nThis primitive checks the current status of an extraction job.\nUse in custom polling loops for advanced workflow control.\n\nArgs:\n    state: Current workflow state\n    job_id: The extraction job ID from submit_job\n    timeout: HTTP request timeout in seconds (default: 10)\n    max_retries: Maximum retry attempts for transient failures\n\nReturns:\n    Dict with:\n    - success: True if status retrieved successfully\n    - status: \"PENDING\", \"RUNNING\", \"SUCCESS\", \"ERROR\", \"PARTIAL_SUCCESS\"\n    - progress: Progress percentage (0-100) if available\n    - error: Error message when status is ERROR\n    - error_type: On API failure\n\nExample:\n    >>> result = llamaextract_poll_status(\n    ...     state={},\n    ...     job_id=\"job_abc123\"\n    ... )\n    >>> if result['success'] and result['status'] == 'SUCCESS':\n    ...     # Ready to get result\n    ...     pass",
          "line_number": 1322
        },
        {
          "name": "llamaextract.get_result",
          "function": "llamaextract_get_result",
          "parameters": [
            {
              "name": "job_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get extraction result for a completed job.\n\nTEA-BUILTIN-008.7: Workflow primitive for result retrieval.\n\nThis primitive retrieves the extracted data for a completed job.\nOnly call after poll_status returns SUCCESS or PARTIAL_SUCCESS.\n\nArgs:\n    state: Current workflow state\n    job_id: The extraction job ID\n    timeout: HTTP request timeout in seconds (default: 30)\n\nReturns:\n    Dict with:\n    - success: True if result retrieved successfully\n    - data: Extracted data when job is complete\n    - job_id: The job ID for reference\n    - error/error_type: If job not complete or failed\n\nExample:\n    >>> result = llamaextract_get_result(\n    ...     state={},\n    ...     job_id=\"job_abc123\"\n    ... )\n    >>> if result['success']:\n    ...     extracted_data = result['data']",
          "line_number": 1489
        },
        {
          "name": "actions.llamaextract_extract",
          "function": "llamaextract_extract",
          "parameters": [
            {
              "name": "file",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_name",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mode",
              "type": "str",
              "required": false,
              "default": "'BALANCED'"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "300"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "use_rest",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "async_mode",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "polling_interval",
              "type": "Union[int, float]",
              "required": false,
              "default": "5"
            },
            {
              "name": "max_poll_attempts",
              "type": "int",
              "required": false,
              "default": "120"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Extract structured data from a document using LlamaExtract.\n\nUses SDK by default for client-side validation before sending to server.\nSet use_rest=True for direct REST API calls (fewer dependencies).\n\nTEA-BUILTIN-008.6: Async polling configuration parameters.\n\nArgs:\n    state: Current workflow state\n    file: Document source - URL, base64 content, or local file path\n    schema: JSON Schema defining the structure to extract.\n           Either schema or agent_id/agent_name must be provided.\n    agent_id: ID of existing extraction agent to use\n    agent_name: Name of existing extraction agent to use\n    mode: Extraction mode - BALANCED, MULTIMODAL, PREMIUM, or FAST\n    timeout: HTTP request timeout in seconds (default: 300)\n    max_retries: Maximum retry attempts for transient failures\n    use_rest: If True, use direct REST API instead of SDK (default: False)\n    async_mode: If True, use async /jobs endpoint explicitly (AC-1, AC-6)\n               Requires use_rest=True. When False (default), uses sync behavior.\n    polling_interval: Seconds between poll requests (default: 5) (AC-2, AC-7)\n                     Accepts floats for sub-second precision.\n    max_poll_attempts: Maximum number of poll attempts (default: 120) (AC-3, AC-8)\n                      Combined with polling_interval, default allows ~10 min.\n\nReturns:\n    Dict with 'success', 'data' (extracted data), 'status', etc.\n\nExample:\n    >>> result = llamaextract_extract(\n    ...     state={},\n    ...     file=\"https://example.com/invoice.pdf\",\n    ...     schema={\n    ...         \"type\": \"object\",\n    ...         \"properties\": {\n    ...             \"invoice_number\": {\"type\": \"string\"},\n    ...             \"total\": {\"type\": \"number\"}\n    ...         }\n    ...     },\n    ...     mode=\"PREMIUM\",\n    ...     timeout=300\n    ... )\n\nExample with async mode (for large documents):\n    >>> result = llamaextract_extract(\n    ...     state={},\n    ...     file=\"large-contract.pdf\",\n    ...     schema={...},\n    ...     use_rest=True,\n    ...     async_mode=True,\n    ...     polling_interval=10,\n    ...     max_poll_attempts=60,\n    ...     timeout=900\n    ... )",
          "line_number": 856
        },
        {
          "name": "actions.llamaextract_upload_agent",
          "function": "llamaextract_upload_agent",
          "parameters": [
            {
              "name": "name",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "description",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mode",
              "type": "str",
              "required": false,
              "default": "'BALANCED'"
            },
            {
              "name": "force",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create or update an extraction agent.\n\nArgs:\n    state: Current workflow state\n    name: Agent name (must be unique)\n    schema: JSON Schema for extraction\n    description: Agent description\n    mode: Default extraction mode\n    force: If True, update existing agent with same name\n    max_retries: Maximum retry attempts\n\nReturns:\n    Dict with 'success', 'agent_id', 'name', 'status'\n\nExample:\n    >>> result = llamaextract_upload_agent(\n    ...     state={},\n    ...     name=\"invoice-extractor\",\n    ...     schema={\"type\": \"object\", \"properties\": {...}},\n    ...     mode=\"PREMIUM\"\n    ... )",
          "line_number": 1637
        },
        {
          "name": "actions.llamaextract_list_agents",
          "function": "llamaextract_list_agents",
          "parameters": [
            {
              "name": "name_filter",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List available extraction agents.\n\nArgs:\n    state: Current workflow state\n    name_filter: Optional name filter (substring match)\n\nReturns:\n    Dict with 'success', 'agents' (list of agent info)\n\nExample:\n    >>> result = llamaextract_list_agents(state={})\n    >>> for agent in result['agents']:\n    ...     print(agent['name'], agent['id'])",
          "line_number": 1750
        },
        {
          "name": "actions.llamaextract_get_agent",
          "function": "llamaextract_get_agent",
          "parameters": [
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_name",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get extraction agent details.\n\nArgs:\n    state: Current workflow state\n    agent_id: Agent ID (preferred)\n    agent_name: Agent name (alternative lookup)\n\nReturns:\n    Dict with 'success', 'agent' containing full configuration\n\nExample:\n    >>> result = llamaextract_get_agent(\n    ...     state={},\n    ...     agent_name=\"invoice-extractor\"\n    ... )\n    >>> print(result['agent']['schema'])",
          "line_number": 1799
        },
        {
          "name": "actions.llamaextract_delete_agent",
          "function": "llamaextract_delete_agent",
          "parameters": [
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_name",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delete an extraction agent.\n\nArgs:\n    state: Current workflow state\n    agent_id: Agent ID (preferred)\n    agent_name: Agent name (alternative lookup)\n\nReturns:\n    Dict with 'success' and status info\n\nExample:\n    >>> result = llamaextract_delete_agent(\n    ...     state={},\n    ...     agent_name=\"old-extractor\"\n    ... )",
          "line_number": 1866
        },
        {
          "name": "actions.llamaextract_submit_job",
          "function": "llamaextract_submit_job",
          "parameters": [
            {
              "name": "file",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mode",
              "type": "str",
              "required": false,
              "default": "'BALANCED'"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Submit async extraction job to LlamaExtract.\n\nTEA-BUILTIN-008.7: Workflow primitive for job submission.\n\nThis primitive submits a document extraction job and returns immediately\nwith the job_id. Use llamaextract.poll_status and llamaextract.get_result\nto monitor and retrieve results.\n\nArgs:\n    state: Current workflow state\n    file: Document source - URL, base64 content, or local file path\n    schema: JSON Schema defining the structure to extract\n    agent_id: ID of existing extraction agent to use\n    mode: Extraction mode - BALANCED, MULTIMODAL, PREMIUM, or FAST\n    max_retries: Maximum retry attempts for job submission\n\nReturns:\n    Dict with:\n    - success: True if job submitted successfully\n    - job_id: The extraction job ID for polling\n    - status: \"PENDING\" on successful submission\n    - error/error_type: On failure\n\nExample:\n    >>> result = llamaextract_submit_job(\n    ...     state={},\n    ...     file=\"invoice.pdf\",\n    ...     schema={\"type\": \"object\", \"properties\": {\"total\": {\"type\": \"number\"}}},\n    ...     mode=\"FAST\"\n    ... )\n    >>> if result['success']:\n    ...     job_id = result['job_id']  # Use for polling",
          "line_number": 1089
        },
        {
          "name": "actions.llamaextract_poll_status",
          "function": "llamaextract_poll_status",
          "parameters": [
            {
              "name": "job_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Poll job status from LlamaExtract.\n\nTEA-BUILTIN-008.7: Workflow primitive for status polling.\n\nThis primitive checks the current status of an extraction job.\nUse in custom polling loops for advanced workflow control.\n\nArgs:\n    state: Current workflow state\n    job_id: The extraction job ID from submit_job\n    timeout: HTTP request timeout in seconds (default: 10)\n    max_retries: Maximum retry attempts for transient failures\n\nReturns:\n    Dict with:\n    - success: True if status retrieved successfully\n    - status: \"PENDING\", \"RUNNING\", \"SUCCESS\", \"ERROR\", \"PARTIAL_SUCCESS\"\n    - progress: Progress percentage (0-100) if available\n    - error: Error message when status is ERROR\n    - error_type: On API failure\n\nExample:\n    >>> result = llamaextract_poll_status(\n    ...     state={},\n    ...     job_id=\"job_abc123\"\n    ... )\n    >>> if result['success'] and result['status'] == 'SUCCESS':\n    ...     # Ready to get result\n    ...     pass",
          "line_number": 1322
        },
        {
          "name": "actions.llamaextract_get_result",
          "function": "llamaextract_get_result",
          "parameters": [
            {
              "name": "job_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get extraction result for a completed job.\n\nTEA-BUILTIN-008.7: Workflow primitive for result retrieval.\n\nThis primitive retrieves the extracted data for a completed job.\nOnly call after poll_status returns SUCCESS or PARTIAL_SUCCESS.\n\nArgs:\n    state: Current workflow state\n    job_id: The extraction job ID\n    timeout: HTTP request timeout in seconds (default: 30)\n\nReturns:\n    Dict with:\n    - success: True if result retrieved successfully\n    - data: Extracted data when job is complete\n    - job_id: The job ID for reference\n    - error/error_type: If job not complete or failed\n\nExample:\n    >>> result = llamaextract_get_result(\n    ...     state={},\n    ...     job_id=\"job_abc123\"\n    ... )\n    >>> if result['success']:\n    ...     extracted_data = result['data']",
          "line_number": 1489
        }
      ]
    },
    {
      "file": "llamaindex_actions.py",
      "namespace": "llamaindex",
      "actions": [
        {
          "name": "rag.llamaindex.query",
          "function": "llamaindex_query",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "index_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "similarity_top_k",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "response_mode",
              "type": "str",
              "required": false,
              "default": "'compact'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute a simple vector query against a LlamaIndex index.\n\nArgs:\n    state: Current state dictionary\n    query: Query text\n    index_path: Path to index storage (uses settings default if not provided)\n    similarity_top_k: Number of nodes to retrieve (default: 5)\n    response_mode: Response synthesis mode (default: compact)\n    **kwargs: Additional configuration\n\nReturns:\n    {\n        \"response\": str,\n        \"nodes\": [{\"id\": str, \"text\": str, \"metadata\": dict}],\n        \"scores\": [float],\n        \"success\": True\n    }\n    Error: {\"error\": str, \"success\": False}",
          "line_number": 111
        },
        {
          "name": "actions.rag_llamaindex_query",
          "function": "llamaindex_query",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "index_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "similarity_top_k",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "response_mode",
              "type": "str",
              "required": false,
              "default": "'compact'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute a simple vector query against a LlamaIndex index.\n\nArgs:\n    state: Current state dictionary\n    query: Query text\n    index_path: Path to index storage (uses settings default if not provided)\n    similarity_top_k: Number of nodes to retrieve (default: 5)\n    response_mode: Response synthesis mode (default: compact)\n    **kwargs: Additional configuration\n\nReturns:\n    {\n        \"response\": str,\n        \"nodes\": [{\"id\": str, \"text\": str, \"metadata\": dict}],\n        \"scores\": [float],\n        \"success\": True\n    }\n    Error: {\"error\": str, \"success\": False}",
          "line_number": 111
        },
        {
          "name": "rag.llamaindex.router",
          "function": "llamaindex_router",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "engines",
              "type": "List[Dict[str, Any]]",
              "required": true,
              "default": null
            },
            {
              "name": "verbose",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute a router query that selects the best engine for the query.\n\nThe router uses an LLM to analyze the query and select the most\nappropriate engine based on the provided descriptions.\n\nArgs:\n    state: Current state dictionary\n    query: Query text\n    engines: List of engine configurations:\n        - type: \"vector\", \"keyword\", or \"sql\"\n        - index_path: Path to index (for vector/keyword)\n        - connection: DB connection string (for sql)\n        - description: Description for LLM selection\n        - name: Optional engine name\n    verbose: Enable verbose logging\n    **kwargs: Additional configuration\n\nReturns:\n    {\n        \"response\": str,\n        \"selected_engine\": dict,\n        \"nodes\": [{\"id\": str, \"text\": str, \"metadata\": dict}],\n        \"scores\": [float],\n        \"success\": True\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> result = llamaindex_router(\n    ...     state={},\n    ...     query=\"Find recent sales data\",\n    ...     engines=[\n    ...         {\n    ...             \"type\": \"vector\",\n    ...             \"index_path\": \"./docs_index\",\n    ...             \"description\": \"Semantic search over documentation\"\n    ...         },\n    ...         {\n    ...             \"type\": \"sql\",\n    ...             \"connection\": \"sqlite:///sales.db\",\n    ...             \"description\": \"Structured sales data queries\"\n    ...         }\n    ...     ]\n    ... )",
          "line_number": 174
        },
        {
          "name": "actions.rag_llamaindex_router",
          "function": "llamaindex_router",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "engines",
              "type": "List[Dict[str, Any]]",
              "required": true,
              "default": null
            },
            {
              "name": "verbose",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute a router query that selects the best engine for the query.\n\nThe router uses an LLM to analyze the query and select the most\nappropriate engine based on the provided descriptions.\n\nArgs:\n    state: Current state dictionary\n    query: Query text\n    engines: List of engine configurations:\n        - type: \"vector\", \"keyword\", or \"sql\"\n        - index_path: Path to index (for vector/keyword)\n        - connection: DB connection string (for sql)\n        - description: Description for LLM selection\n        - name: Optional engine name\n    verbose: Enable verbose logging\n    **kwargs: Additional configuration\n\nReturns:\n    {\n        \"response\": str,\n        \"selected_engine\": dict,\n        \"nodes\": [{\"id\": str, \"text\": str, \"metadata\": dict}],\n        \"scores\": [float],\n        \"success\": True\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> result = llamaindex_router(\n    ...     state={},\n    ...     query=\"Find recent sales data\",\n    ...     engines=[\n    ...         {\n    ...             \"type\": \"vector\",\n    ...             \"index_path\": \"./docs_index\",\n    ...             \"description\": \"Semantic search over documentation\"\n    ...         },\n    ...         {\n    ...             \"type\": \"sql\",\n    ...             \"connection\": \"sqlite:///sales.db\",\n    ...             \"description\": \"Structured sales data queries\"\n    ...         }\n    ...     ]\n    ... )",
          "line_number": 174
        },
        {
          "name": "rag.llamaindex.subquestion",
          "function": "llamaindex_subquestion",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "engines",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "index_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "parallel",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "synthesis_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "verbose",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute a sub-question query that decomposes complex queries.\n\nThe sub-question engine breaks down complex queries into simpler\nsub-questions, executes them, and synthesizes a final answer.\n\nArgs:\n    state: Current state dictionary\n    query: Complex query text\n    engines: List of engine configurations (same format as router)\n    index_path: Simple path for single-index queries\n    parallel: Enable parallel execution of sub-questions\n    synthesis_prompt: Custom prompt for answer synthesis\n    verbose: Enable verbose logging\n    **kwargs: Additional configuration\n\nReturns:\n    {\n        \"response\": str,\n        \"sub_questions\": [str],\n        \"sub_answers\": [str],\n        \"success\": True\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> result = llamaindex_subquestion(\n    ...     state={},\n    ...     query=\"Compare the revenue growth of Apple and Google from 2020 to 2024\",\n    ...     engines=[\n    ...         {\"type\": \"vector\", \"index_path\": \"./financials\", \"description\": \"Financial data\"}\n    ...     ],\n    ...     parallel=True\n    ... )\n    >>> print(result['sub_questions'])\n    ['What was Apple revenue in 2020?', 'What was Apple revenue in 2024?', ...]",
          "line_number": 276
        },
        {
          "name": "actions.rag_llamaindex_subquestion",
          "function": "llamaindex_subquestion",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "engines",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "index_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "parallel",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "synthesis_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "verbose",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute a sub-question query that decomposes complex queries.\n\nThe sub-question engine breaks down complex queries into simpler\nsub-questions, executes them, and synthesizes a final answer.\n\nArgs:\n    state: Current state dictionary\n    query: Complex query text\n    engines: List of engine configurations (same format as router)\n    index_path: Simple path for single-index queries\n    parallel: Enable parallel execution of sub-questions\n    synthesis_prompt: Custom prompt for answer synthesis\n    verbose: Enable verbose logging\n    **kwargs: Additional configuration\n\nReturns:\n    {\n        \"response\": str,\n        \"sub_questions\": [str],\n        \"sub_answers\": [str],\n        \"success\": True\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> result = llamaindex_subquestion(\n    ...     state={},\n    ...     query=\"Compare the revenue growth of Apple and Google from 2020 to 2024\",\n    ...     engines=[\n    ...         {\"type\": \"vector\", \"index_path\": \"./financials\", \"description\": \"Financial data\"}\n    ...     ],\n    ...     parallel=True\n    ... )\n    >>> print(result['sub_questions'])\n    ['What was Apple revenue in 2020?', 'What was Apple revenue in 2024?', ...]",
          "line_number": 276
        },
        {
          "name": "rag.llamaindex.create_index",
          "function": "llamaindex_create_index",
          "parameters": [
            {
              "name": "documents",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "directory",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "persist_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create a new LlamaIndex index from documents or a directory.\n\nArgs:\n    state: Current state dictionary\n    documents: List of document dicts with 'text' and optional 'metadata'\n    directory: Directory path to load documents from\n    persist_path: Path to persist the index\n\nReturns:\n    {\n        \"success\": True,\n        \"persist_path\": str,\n        \"document_count\": int\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> # From documents\n    >>> result = llamaindex_create_index(\n    ...     state={},\n    ...     documents=[\n    ...         {\"text\": \"Doc 1 content\", \"metadata\": {\"type\": \"article\"}},\n    ...         {\"text\": \"Doc 2 content\", \"metadata\": {\"type\": \"blog\"}}\n    ...     ],\n    ...     persist_path=\"./my_index\"\n    ... )\n\n    >>> # From directory\n    >>> result = llamaindex_create_index(\n    ...     state={},\n    ...     directory=\"./documents/\",\n    ...     persist_path=\"./my_index\"\n    ... )",
          "line_number": 382
        },
        {
          "name": "actions.rag_llamaindex_create_index",
          "function": "llamaindex_create_index",
          "parameters": [
            {
              "name": "documents",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "directory",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "persist_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create a new LlamaIndex index from documents or a directory.\n\nArgs:\n    state: Current state dictionary\n    documents: List of document dicts with 'text' and optional 'metadata'\n    directory: Directory path to load documents from\n    persist_path: Path to persist the index\n\nReturns:\n    {\n        \"success\": True,\n        \"persist_path\": str,\n        \"document_count\": int\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> # From documents\n    >>> result = llamaindex_create_index(\n    ...     state={},\n    ...     documents=[\n    ...         {\"text\": \"Doc 1 content\", \"metadata\": {\"type\": \"article\"}},\n    ...         {\"text\": \"Doc 2 content\", \"metadata\": {\"type\": \"blog\"}}\n    ...     ],\n    ...     persist_path=\"./my_index\"\n    ... )\n\n    >>> # From directory\n    >>> result = llamaindex_create_index(\n    ...     state={},\n    ...     directory=\"./documents/\",\n    ...     persist_path=\"./my_index\"\n    ... )",
          "line_number": 382
        },
        {
          "name": "rag.llamaindex.load_index",
          "function": "llamaindex_load_index",
          "parameters": [
            {
              "name": "index_path",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "force_reload",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Load a persisted LlamaIndex index.\n\nArgs:\n    state: Current state dictionary\n    index_path: Path to the index storage\n    force_reload: Force reload even if cached\n\nReturns:\n    {\n        \"success\": True,\n        \"index_path\": str,\n        \"cached\": bool\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> result = llamaindex_load_index(\n    ...     state={},\n    ...     index_path=\"./my_index\"\n    ... )\n    >>> if result['success']:\n    ...     print(f\"Loaded from: {result['index_path']}\")",
          "line_number": 470
        },
        {
          "name": "actions.rag_llamaindex_load_index",
          "function": "llamaindex_load_index",
          "parameters": [
            {
              "name": "index_path",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "force_reload",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Load a persisted LlamaIndex index.\n\nArgs:\n    state: Current state dictionary\n    index_path: Path to the index storage\n    force_reload: Force reload even if cached\n\nReturns:\n    {\n        \"success\": True,\n        \"index_path\": str,\n        \"cached\": bool\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> result = llamaindex_load_index(\n    ...     state={},\n    ...     index_path=\"./my_index\"\n    ... )\n    >>> if result['success']:\n    ...     print(f\"Loaded from: {result['index_path']}\")",
          "line_number": 470
        },
        {
          "name": "rag.llamaindex.add_documents",
          "function": "llamaindex_add_documents",
          "parameters": [
            {
              "name": "documents",
              "type": "List[Dict[str, Any]]",
              "required": true,
              "default": null
            },
            {
              "name": "index_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Add documents to an existing LlamaIndex index.\n\nArgs:\n    state: Current state dictionary\n    documents: List of document dicts with 'text' and optional 'metadata'\n    index_path: Path to index (uses settings default if not provided)\n\nReturns:\n    {\n        \"success\": True,\n        \"added\": int,\n        \"index_path\": str\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> result = llamaindex_add_documents(\n    ...     state={},\n    ...     documents=[\n    ...         {\"text\": \"New document content\", \"metadata\": {\"date\": \"2024-01-15\"}}\n    ...     ],\n    ...     index_path=\"./my_index\"\n    ... )",
          "line_number": 545
        },
        {
          "name": "actions.rag_llamaindex_add_documents",
          "function": "llamaindex_add_documents",
          "parameters": [
            {
              "name": "documents",
              "type": "List[Dict[str, Any]]",
              "required": true,
              "default": null
            },
            {
              "name": "index_path",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Add documents to an existing LlamaIndex index.\n\nArgs:\n    state: Current state dictionary\n    documents: List of document dicts with 'text' and optional 'metadata'\n    index_path: Path to index (uses settings default if not provided)\n\nReturns:\n    {\n        \"success\": True,\n        \"added\": int,\n        \"index_path\": str\n    }\n    Error: {\"error\": str, \"success\": False}\n\nExample:\n    >>> result = llamaindex_add_documents(\n    ...     state={},\n    ...     documents=[\n    ...         {\"text\": \"New document content\", \"metadata\": {\"date\": \"2024-01-15\"}}\n    ...     ],\n    ...     index_path=\"./my_index\"\n    ... )",
          "line_number": 545
        }
      ]
    },
    {
      "file": "llm_actions.py",
      "namespace": "llm",
      "actions": [
        {
          "name": "llm.call",
          "function": "llm_call",
          "parameters": [
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "system",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "Any",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "max_retries",
              "type": "Any",
              "required": false,
              "default": "0"
            },
            {
              "name": "base_delay",
              "type": "Any",
              "required": false,
              "default": "1.0"
            },
            {
              "name": "max_delay",
              "type": "Any",
              "required": false,
              "default": "60.0"
            },
            {
              "name": "opik_trace",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "opik_project_name",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "Any",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "api_base",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "300"
            },
            {
              "name": "shell_provider",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Call a language model (supports OpenAI, Azure OpenAI, Ollama, LiteLLM, and Shell CLI) with optional retry logic.\n\nProvider Detection Priority:\n1. Explicit `provider` parameter (highest priority)\n2. Environment variable detection:\n   - OLLAMA_API_BASE  Ollama\n   - AZURE_OPENAI_API_KEY + AZURE_OPENAI_ENDPOINT  Azure OpenAI\n3. Default  OpenAI\n\nAutomatically detects Azure OpenAI configuration via environment variables:\n- AZURE_OPENAI_API_KEY: Azure OpenAI API key\n- AZURE_OPENAI_ENDPOINT: Azure OpenAI endpoint URL\n- AZURE_OPENAI_DEPLOYMENT: Deployment name (defaults to model param)\n- OPENAI_API_VERSION: API version (defaults to 2024-02-15-preview)\n\nFor Ollama:\n- OLLAMA_API_BASE: Ollama API endpoint (default: http://localhost:11434/v1)\n- No API key required\n\nFor LiteLLM (TEA-LLM-003):\n- Supports 100+ LLM providers via unified interface\n- Model format: \"provider/model-name\" (e.g., \"anthropic/claude-3-opus\")\n- Requires: pip install litellm\n- Environment variables per provider (e.g., ANTHROPIC_API_KEY, etc.)\n\nFor Shell CLI (TEA-LLM-004):\n- Execute local CLI commands (claude, gemini, qwen, etc.)\n- Use provider: \"shell\" with shell_provider: \"claude\" (or other)\n- Configure in settings.llm.shell_providers or use built-in defaults\n- No API key required for local CLI tools\n\nFor Local LLM (TEA-RELEASE-004.5):\n- Uses llama-cpp-python for local GGUF model inference\n- Use provider: \"local\" with model pointing to a .gguf file\n- Model path resolution: model param > TEA_MODEL_PATH env > APPDIR > ~/.cache/tea/models\n- Supports Phi-4-mini (128K context) and Gemma (32K context) auto-detection\n- Requires: pip install the_edge_agent[llm-local]\n\nArgs:\n    state: Current state dictionary\n    model: Model name (e.g., \"gpt-4\", \"llama3.2\", \"anthropic/claude-3-opus\", or path to .gguf file for local)\n    messages: List of message dicts with 'role' and 'content'. Takes precedence over prompt.\n    prompt: Simple prompt string (TEA-LLM-005 - Rust parity). Alternative to messages array.\n           When provided without messages, converts to [{\"role\": \"user\", \"content\": prompt}].\n    system: System message (TEA-LLM-005 - Rust parity). Only used with prompt parameter.\n           When provided, prepends [{\"role\": \"system\", \"content\": system}] to messages.\n    temperature: Sampling temperature (default: 0.7)\n    max_retries: Maximum retry attempts (default: 0, no retry)\n    base_delay: Initial delay in seconds for exponential backoff (default: 1.0)\n    max_delay: Maximum delay between retries (default: 60.0)\n    opik_trace: If True, wrap client with Opik's track_openai for rich LLM\n               telemetry (model, tokens, latency). Requires opik SDK installed.\n               Default: False (opt-in feature).\n    provider: LLM provider - \"auto\" (detect), \"openai\", \"azure\", \"ollama\", \"litellm\", \"shell\", or \"local\"\n    api_base: Custom API base URL (overrides defaults)\n    timeout: Request timeout in seconds (default: 300 for slow local models like Ollama)\n    shell_provider: Shell provider name when provider=\"shell\" (e.g., \"claude\", \"gemini\", \"qwen\")\n    **kwargs: Additional parameters passed to OpenAI/LiteLLM (for local: n_ctx, n_threads, n_gpu_layers, max_tokens, stop)\n\nExamples:\n    # Using prompt (simple single-turn)\n    result = llm_call(state, model=\"gpt-4\", prompt=\"What is 2+2?\")\n\n    # Using prompt with system message\n    result = llm_call(state, model=\"gpt-4\", prompt=\"Hello\", system=\"You are a helpful assistant\")\n\n    # Using messages (full control)\n    result = llm_call(state, model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n\nReturns:\n    When max_retries=0:\n        {\"content\": str, \"usage\": dict}\n    When max_retries>0:\n        {\"content\": str, \"usage\": dict, \"attempts\": int, \"total_delay\": float}\n    When opik_trace=True:\n        Result dict also includes \"cost_usd\": float (estimated cost)\n    When provider=litellm:\n        Result dict also includes \"cost_usd\": float (from LiteLLM cost tracking)\n    When provider=shell:\n        {\"content\": str, \"usage\": {}, \"provider\": \"shell\", \"shell_provider\": str}\n    When provider=local:\n        {\"content\": str, \"usage\": {\"total_tokens\": int}, \"provider\": \"local\", \"model\": str, \"finish_reason\": str}\n    Or {\"error\": str, \"success\": False, \"attempts\": int} on failure\n\nRetry behavior:\n    - max_retries=0: Respects Retry-After header once, then fails\n    - max_retries>0: Full exponential backoff with Retry-After support\n    - Retries: HTTP 429, 5xx errors, timeouts, connection errors\n    - Fails fast: HTTP 4xx (except 429)",
          "line_number": 668
        },
        {
          "name": "actions.llm_call",
          "function": "llm_call",
          "parameters": [
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "system",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "Any",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "max_retries",
              "type": "Any",
              "required": false,
              "default": "0"
            },
            {
              "name": "base_delay",
              "type": "Any",
              "required": false,
              "default": "1.0"
            },
            {
              "name": "max_delay",
              "type": "Any",
              "required": false,
              "default": "60.0"
            },
            {
              "name": "opik_trace",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "opik_project_name",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "Any",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "api_base",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "300"
            },
            {
              "name": "shell_provider",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Call a language model (supports OpenAI, Azure OpenAI, Ollama, LiteLLM, and Shell CLI) with optional retry logic.\n\nProvider Detection Priority:\n1. Explicit `provider` parameter (highest priority)\n2. Environment variable detection:\n   - OLLAMA_API_BASE  Ollama\n   - AZURE_OPENAI_API_KEY + AZURE_OPENAI_ENDPOINT  Azure OpenAI\n3. Default  OpenAI\n\nAutomatically detects Azure OpenAI configuration via environment variables:\n- AZURE_OPENAI_API_KEY: Azure OpenAI API key\n- AZURE_OPENAI_ENDPOINT: Azure OpenAI endpoint URL\n- AZURE_OPENAI_DEPLOYMENT: Deployment name (defaults to model param)\n- OPENAI_API_VERSION: API version (defaults to 2024-02-15-preview)\n\nFor Ollama:\n- OLLAMA_API_BASE: Ollama API endpoint (default: http://localhost:11434/v1)\n- No API key required\n\nFor LiteLLM (TEA-LLM-003):\n- Supports 100+ LLM providers via unified interface\n- Model format: \"provider/model-name\" (e.g., \"anthropic/claude-3-opus\")\n- Requires: pip install litellm\n- Environment variables per provider (e.g., ANTHROPIC_API_KEY, etc.)\n\nFor Shell CLI (TEA-LLM-004):\n- Execute local CLI commands (claude, gemini, qwen, etc.)\n- Use provider: \"shell\" with shell_provider: \"claude\" (or other)\n- Configure in settings.llm.shell_providers or use built-in defaults\n- No API key required for local CLI tools\n\nFor Local LLM (TEA-RELEASE-004.5):\n- Uses llama-cpp-python for local GGUF model inference\n- Use provider: \"local\" with model pointing to a .gguf file\n- Model path resolution: model param > TEA_MODEL_PATH env > APPDIR > ~/.cache/tea/models\n- Supports Phi-4-mini (128K context) and Gemma (32K context) auto-detection\n- Requires: pip install the_edge_agent[llm-local]\n\nArgs:\n    state: Current state dictionary\n    model: Model name (e.g., \"gpt-4\", \"llama3.2\", \"anthropic/claude-3-opus\", or path to .gguf file for local)\n    messages: List of message dicts with 'role' and 'content'. Takes precedence over prompt.\n    prompt: Simple prompt string (TEA-LLM-005 - Rust parity). Alternative to messages array.\n           When provided without messages, converts to [{\"role\": \"user\", \"content\": prompt}].\n    system: System message (TEA-LLM-005 - Rust parity). Only used with prompt parameter.\n           When provided, prepends [{\"role\": \"system\", \"content\": system}] to messages.\n    temperature: Sampling temperature (default: 0.7)\n    max_retries: Maximum retry attempts (default: 0, no retry)\n    base_delay: Initial delay in seconds for exponential backoff (default: 1.0)\n    max_delay: Maximum delay between retries (default: 60.0)\n    opik_trace: If True, wrap client with Opik's track_openai for rich LLM\n               telemetry (model, tokens, latency). Requires opik SDK installed.\n               Default: False (opt-in feature).\n    provider: LLM provider - \"auto\" (detect), \"openai\", \"azure\", \"ollama\", \"litellm\", \"shell\", or \"local\"\n    api_base: Custom API base URL (overrides defaults)\n    timeout: Request timeout in seconds (default: 300 for slow local models like Ollama)\n    shell_provider: Shell provider name when provider=\"shell\" (e.g., \"claude\", \"gemini\", \"qwen\")\n    **kwargs: Additional parameters passed to OpenAI/LiteLLM (for local: n_ctx, n_threads, n_gpu_layers, max_tokens, stop)\n\nExamples:\n    # Using prompt (simple single-turn)\n    result = llm_call(state, model=\"gpt-4\", prompt=\"What is 2+2?\")\n\n    # Using prompt with system message\n    result = llm_call(state, model=\"gpt-4\", prompt=\"Hello\", system=\"You are a helpful assistant\")\n\n    # Using messages (full control)\n    result = llm_call(state, model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n\nReturns:\n    When max_retries=0:\n        {\"content\": str, \"usage\": dict}\n    When max_retries>0:\n        {\"content\": str, \"usage\": dict, \"attempts\": int, \"total_delay\": float}\n    When opik_trace=True:\n        Result dict also includes \"cost_usd\": float (estimated cost)\n    When provider=litellm:\n        Result dict also includes \"cost_usd\": float (from LiteLLM cost tracking)\n    When provider=shell:\n        {\"content\": str, \"usage\": {}, \"provider\": \"shell\", \"shell_provider\": str}\n    When provider=local:\n        {\"content\": str, \"usage\": {\"total_tokens\": int}, \"provider\": \"local\", \"model\": str, \"finish_reason\": str}\n    Or {\"error\": str, \"success\": False, \"attempts\": int} on failure\n\nRetry behavior:\n    - max_retries=0: Respects Retry-After header once, then fails\n    - max_retries>0: Full exponential backoff with Retry-After support\n    - Retries: HTTP 429, 5xx errors, timeouts, connection errors\n    - Fails fast: HTTP 4xx (except 429)",
          "line_number": 668
        },
        {
          "name": "llm.stream",
          "function": "llm_stream",
          "parameters": [
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "system",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "Any",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "opik_trace",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "opik_project_name",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "Any",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "api_base",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "300"
            },
            {
              "name": "shell_provider",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Stream LLM responses token-by-token.\n\nUses OpenAI streaming API to yield partial content chunks as they arrive.\nThis action aggregates all chunks and returns the final result.\n\nProvider detection follows same priority as llm.call:\n1. Explicit `provider` parameter\n2. Environment variable detection (OLLAMA_API_BASE, AZURE_OPENAI_*)\n3. Default  OpenAI\n\nFor LiteLLM (TEA-LLM-003):\n- Supports 100+ LLM providers via unified streaming interface\n- Model format: \"provider/model-name\" (e.g., \"anthropic/claude-3-opus\")\n\nFor Shell CLI (TEA-LLM-004):\n- Execute local CLI commands with line-by-line streaming\n- Use provider: \"shell\" with shell_provider: \"claude\" (or other)\n\nArgs:\n    state: Current state dictionary\n    model: Model name (e.g., \"gpt-4\", \"llama3.2\", \"anthropic/claude-3-opus\")\n    messages: List of message dicts with 'role' and 'content'. Takes precedence over prompt.\n    prompt: Simple prompt string (TEA-LLM-005 - Rust parity). Alternative to messages array.\n           When provided without messages, converts to [{\"role\": \"user\", \"content\": prompt}].\n    system: System message (TEA-LLM-005 - Rust parity). Only used with prompt parameter.\n           When provided, prepends [{\"role\": \"system\", \"content\": system}] to messages.\n    temperature: Sampling temperature (default: 0.7)\n    opik_trace: If True, wrap client with Opik's track_openai for rich LLM\n               telemetry. Opik's wrapper handles streaming chunk aggregation.\n               For LiteLLM, uses OpikLogger callback.\n               Default: False (opt-in feature).\n    provider: LLM provider - \"auto\" (detect), \"openai\", \"azure\", \"ollama\", \"litellm\", or \"shell\"\n    api_base: Custom API base URL (overrides defaults)\n    timeout: Request timeout in seconds (default: 300)\n    shell_provider: Shell provider name when provider=\"shell\" (e.g., \"claude\", \"gemini\", \"qwen\")\n    **kwargs: Additional parameters passed to OpenAI/LiteLLM\n\nReturns:\n    {\"content\": str, \"usage\": dict, \"streamed\": True, \"chunk_count\": int}\n    When opik_trace=True (and not Ollama):\n        Result dict also includes \"cost_usd\": float (estimated cost)\n    When provider=litellm:\n        Result dict also includes \"cost_usd\": float (from LiteLLM cost tracking)\n    When provider=shell:\n        {\"content\": str, \"usage\": {}, \"streamed\": True, \"chunk_count\": int, ...}\n    Or {\"error\": str, \"success\": False} on failure\n\nExamples:\n    # Using prompt (simple)\n    result = llm_stream(state, \"gpt-4\", prompt=\"Tell me a story\")\n\n    # Using prompt with system\n    result = llm_stream(state, \"gpt-4\", prompt=\"Tell me a story\", system=\"Be brief\")\n\n    # Using messages (traditional)\n    result = llm_stream(state, \"gpt-4\", messages=[{\"role\": \"user\", \"content\": \"Hello\"}])",
          "line_number": 1328
        },
        {
          "name": "actions.llm_stream",
          "function": "llm_stream",
          "parameters": [
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "system",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "Any",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "opik_trace",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "opik_project_name",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "Any",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "api_base",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "300"
            },
            {
              "name": "shell_provider",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Stream LLM responses token-by-token.\n\nUses OpenAI streaming API to yield partial content chunks as they arrive.\nThis action aggregates all chunks and returns the final result.\n\nProvider detection follows same priority as llm.call:\n1. Explicit `provider` parameter\n2. Environment variable detection (OLLAMA_API_BASE, AZURE_OPENAI_*)\n3. Default  OpenAI\n\nFor LiteLLM (TEA-LLM-003):\n- Supports 100+ LLM providers via unified streaming interface\n- Model format: \"provider/model-name\" (e.g., \"anthropic/claude-3-opus\")\n\nFor Shell CLI (TEA-LLM-004):\n- Execute local CLI commands with line-by-line streaming\n- Use provider: \"shell\" with shell_provider: \"claude\" (or other)\n\nArgs:\n    state: Current state dictionary\n    model: Model name (e.g., \"gpt-4\", \"llama3.2\", \"anthropic/claude-3-opus\")\n    messages: List of message dicts with 'role' and 'content'. Takes precedence over prompt.\n    prompt: Simple prompt string (TEA-LLM-005 - Rust parity). Alternative to messages array.\n           When provided without messages, converts to [{\"role\": \"user\", \"content\": prompt}].\n    system: System message (TEA-LLM-005 - Rust parity). Only used with prompt parameter.\n           When provided, prepends [{\"role\": \"system\", \"content\": system}] to messages.\n    temperature: Sampling temperature (default: 0.7)\n    opik_trace: If True, wrap client with Opik's track_openai for rich LLM\n               telemetry. Opik's wrapper handles streaming chunk aggregation.\n               For LiteLLM, uses OpikLogger callback.\n               Default: False (opt-in feature).\n    provider: LLM provider - \"auto\" (detect), \"openai\", \"azure\", \"ollama\", \"litellm\", or \"shell\"\n    api_base: Custom API base URL (overrides defaults)\n    timeout: Request timeout in seconds (default: 300)\n    shell_provider: Shell provider name when provider=\"shell\" (e.g., \"claude\", \"gemini\", \"qwen\")\n    **kwargs: Additional parameters passed to OpenAI/LiteLLM\n\nReturns:\n    {\"content\": str, \"usage\": dict, \"streamed\": True, \"chunk_count\": int}\n    When opik_trace=True (and not Ollama):\n        Result dict also includes \"cost_usd\": float (estimated cost)\n    When provider=litellm:\n        Result dict also includes \"cost_usd\": float (from LiteLLM cost tracking)\n    When provider=shell:\n        {\"content\": str, \"usage\": {}, \"streamed\": True, \"chunk_count\": int, ...}\n    Or {\"error\": str, \"success\": False} on failure\n\nExamples:\n    # Using prompt (simple)\n    result = llm_stream(state, \"gpt-4\", prompt=\"Tell me a story\")\n\n    # Using prompt with system\n    result = llm_stream(state, \"gpt-4\", prompt=\"Tell me a story\", system=\"Be brief\")\n\n    # Using messages (traditional)\n    result = llm_stream(state, \"gpt-4\", messages=[{\"role\": \"user\", \"content\": \"Hello\"}])",
          "line_number": 1328
        },
        {
          "name": "llm.retry",
          "function": "llm_retry",
          "parameters": [
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_retries",
              "type": "Any",
              "required": false,
              "default": "3"
            },
            {
              "name": "base_delay",
              "type": "Any",
              "required": false,
              "default": "1.0"
            },
            {
              "name": "max_delay",
              "type": "Any",
              "required": false,
              "default": "60.0"
            },
            {
              "name": "temperature",
              "type": "Any",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "DEPRECATED: Use llm.call with max_retries parameter instead.\n\nThis action is deprecated and will be removed in v0.9.0.\nIt now delegates to llm.call with the same parameters.\n\nMigration:\n    # Before:\n    uses: llm.retry\n    with:\n      max_retries: 3\n\n    # After:\n    uses: llm.call\n    with:\n      max_retries: 3\n\nArgs:\n    state: Current state dictionary\n    model: Model name (e.g., \"gpt-4\", \"gpt-3.5-turbo\")\n    messages: List of message dicts with 'role' and 'content'\n    max_retries: Maximum number of retry attempts (default: 3)\n    base_delay: Initial delay in seconds (default: 1.0)\n    max_delay: Maximum delay between retries (default: 60.0)\n    temperature: Sampling temperature (default: 0.7)\n    **kwargs: Additional parameters passed to OpenAI\n\nReturns:\n    {\"content\": str, \"usage\": dict, \"attempts\": int, \"total_delay\": float}\n    Or {\"error\": str, \"success\": False, \"attempts\": int} on failure",
          "line_number": 1662
        },
        {
          "name": "actions.llm_retry",
          "function": "llm_retry",
          "parameters": [
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_retries",
              "type": "Any",
              "required": false,
              "default": "3"
            },
            {
              "name": "base_delay",
              "type": "Any",
              "required": false,
              "default": "1.0"
            },
            {
              "name": "max_delay",
              "type": "Any",
              "required": false,
              "default": "60.0"
            },
            {
              "name": "temperature",
              "type": "Any",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "DEPRECATED: Use llm.call with max_retries parameter instead.\n\nThis action is deprecated and will be removed in v0.9.0.\nIt now delegates to llm.call with the same parameters.\n\nMigration:\n    # Before:\n    uses: llm.retry\n    with:\n      max_retries: 3\n\n    # After:\n    uses: llm.call\n    with:\n      max_retries: 3\n\nArgs:\n    state: Current state dictionary\n    model: Model name (e.g., \"gpt-4\", \"gpt-3.5-turbo\")\n    messages: List of message dicts with 'role' and 'content'\n    max_retries: Maximum number of retry attempts (default: 3)\n    base_delay: Initial delay in seconds (default: 1.0)\n    max_delay: Maximum delay between retries (default: 60.0)\n    temperature: Sampling temperature (default: 0.7)\n    **kwargs: Additional parameters passed to OpenAI\n\nReturns:\n    {\"content\": str, \"usage\": dict, \"attempts\": int, \"total_delay\": float}\n    Or {\"error\": str, \"success\": False, \"attempts\": int} on failure",
          "line_number": 1662
        },
        {
          "name": "llm.tools",
          "function": "llm_tools",
          "parameters": [
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "system",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "tools",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "tool_choice",
              "type": "Any",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "max_tool_rounds",
              "type": "Any",
              "required": false,
              "default": "10"
            },
            {
              "name": "temperature",
              "type": "Any",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "opik_trace",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "opik_project_name",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "Any",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "api_base",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "LLM call with tool/function calling support.\n\nSupports OpenAI function calling with automatic dispatch to registered\nactions. Handles multi-turn tool use (call -> result -> continue).\n\nProvider detection follows same priority as llm.call:\n1. Explicit `provider` parameter\n2. Environment variable detection (OLLAMA_API_BASE, AZURE_OPENAI_*)\n3. Default  OpenAI\n\nFor LiteLLM (TEA-LLM-003):\n- Supports 100+ LLM providers with tool calling via unified interface\n- Model format: \"provider/model-name\" (e.g., \"anthropic/claude-3-opus\")\n- Tool calling support varies by provider/model\n\nNote: Tool calling with Ollama requires models that support it\n(e.g., llama3.1+, mistral-nemo, qwen2.5).\n\nArgs:\n    state: Current state dictionary\n    model: Model name (e.g., \"gpt-4\", \"llama3.1\", \"anthropic/claude-3-opus\")\n    messages: List of message dicts with 'role' and 'content'. Takes precedence over prompt.\n    prompt: Simple prompt string (TEA-LLM-005 - Rust parity). Alternative to messages array.\n           When provided without messages, converts to [{\"role\": \"user\", \"content\": prompt}].\n    system: System message (TEA-LLM-005 - Rust parity). Only used with prompt parameter.\n           When provided, prepends [{\"role\": \"system\", \"content\": system}] to messages.\n    tools: List of tool definitions (YAML-style or OpenAI-style)\n    tool_choice: Tool selection mode - \"auto\", \"none\", or specific tool\n    max_tool_rounds: Maximum tool call rounds (default: 10)\n    temperature: Sampling temperature (default: 0.7)\n    opik_trace: If True, enable Opik tracing for LLM calls.\n               For LiteLLM, uses OpikLogger callback.\n               Default: False (opt-in feature).\n    provider: LLM provider - \"auto\" (detect), \"openai\", \"azure\", \"ollama\", or \"litellm\"\n    api_base: Custom API base URL (overrides defaults)\n    **kwargs: Additional parameters passed to OpenAI/LiteLLM\n\nExamples:\n    # Using prompt with tools\n    result = llm_tools(state, model=\"gpt-4\", prompt=\"What's the weather?\", tools=[...])\n\n    # Using prompt with system and tools\n    result = llm_tools(state, model=\"gpt-4\", prompt=\"Check weather\",\n                     system=\"Use tools when helpful\", tools=[...])\n\nTool definition YAML schema:\n    tools:\n      - name: search_web\n        description: Search the web for information\n        parameters:\n          query:\n            type: string\n            description: Search query\n            required: true\n        action: web.search  # Maps to registered action\n\nReturns:\n    {\n        \"content\": str,\n        \"tool_calls\": List[dict],  # All tool calls made\n        \"tool_results\": List[dict],  # Results from each tool call\n        \"rounds\": int  # Number of tool call rounds\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 1725
        },
        {
          "name": "actions.llm_tools",
          "function": "llm_tools",
          "parameters": [
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "system",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "tools",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "tool_choice",
              "type": "Any",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "max_tool_rounds",
              "type": "Any",
              "required": false,
              "default": "10"
            },
            {
              "name": "temperature",
              "type": "Any",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "opik_trace",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "opik_project_name",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "Any",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "api_base",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "LLM call with tool/function calling support.\n\nSupports OpenAI function calling with automatic dispatch to registered\nactions. Handles multi-turn tool use (call -> result -> continue).\n\nProvider detection follows same priority as llm.call:\n1. Explicit `provider` parameter\n2. Environment variable detection (OLLAMA_API_BASE, AZURE_OPENAI_*)\n3. Default  OpenAI\n\nFor LiteLLM (TEA-LLM-003):\n- Supports 100+ LLM providers with tool calling via unified interface\n- Model format: \"provider/model-name\" (e.g., \"anthropic/claude-3-opus\")\n- Tool calling support varies by provider/model\n\nNote: Tool calling with Ollama requires models that support it\n(e.g., llama3.1+, mistral-nemo, qwen2.5).\n\nArgs:\n    state: Current state dictionary\n    model: Model name (e.g., \"gpt-4\", \"llama3.1\", \"anthropic/claude-3-opus\")\n    messages: List of message dicts with 'role' and 'content'. Takes precedence over prompt.\n    prompt: Simple prompt string (TEA-LLM-005 - Rust parity). Alternative to messages array.\n           When provided without messages, converts to [{\"role\": \"user\", \"content\": prompt}].\n    system: System message (TEA-LLM-005 - Rust parity). Only used with prompt parameter.\n           When provided, prepends [{\"role\": \"system\", \"content\": system}] to messages.\n    tools: List of tool definitions (YAML-style or OpenAI-style)\n    tool_choice: Tool selection mode - \"auto\", \"none\", or specific tool\n    max_tool_rounds: Maximum tool call rounds (default: 10)\n    temperature: Sampling temperature (default: 0.7)\n    opik_trace: If True, enable Opik tracing for LLM calls.\n               For LiteLLM, uses OpikLogger callback.\n               Default: False (opt-in feature).\n    provider: LLM provider - \"auto\" (detect), \"openai\", \"azure\", \"ollama\", or \"litellm\"\n    api_base: Custom API base URL (overrides defaults)\n    **kwargs: Additional parameters passed to OpenAI/LiteLLM\n\nExamples:\n    # Using prompt with tools\n    result = llm_tools(state, model=\"gpt-4\", prompt=\"What's the weather?\", tools=[...])\n\n    # Using prompt with system and tools\n    result = llm_tools(state, model=\"gpt-4\", prompt=\"Check weather\",\n                     system=\"Use tools when helpful\", tools=[...])\n\nTool definition YAML schema:\n    tools:\n      - name: search_web\n        description: Search the web for information\n        parameters:\n          query:\n            type: string\n            description: Search query\n            required: true\n        action: web.search  # Maps to registered action\n\nReturns:\n    {\n        \"content\": str,\n        \"tool_calls\": List[dict],  # All tool calls made\n        \"tool_results\": List[dict],  # Results from each tool call\n        \"rounds\": int  # Number of tool call rounds\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 1725
        }
      ]
    },
    {
      "file": "llm_local_actions.py",
      "namespace": "llm_local",
      "actions": [
        {
          "name": "llm.local.call",
          "function": "llm_call_local",
          "parameters": [
            {
              "name": "prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Optional[List[Dict[str, str]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_tokens",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "stop",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "LLM completion using local or API backend.\n\nUses the configured backend (local or API) based on settings.llm.backend.\nIf local backend is configured but unavailable, falls back to API.\n\nArgs:\n    state: Current state dictionary\n    prompt: Raw prompt text (for completion mode)\n    messages: Chat messages (for chat mode, takes precedence)\n    max_tokens: Maximum tokens to generate\n    temperature: Sampling temperature\n    stop: Optional stop sequences\n    **kwargs: Additional parameters passed to backend\n\nReturns:\n    {\"content\": str, \"model\": str, \"tokens_used\": int|None}\n\nExample YAML:\n    - name: generate\n      action: llm.call\n      params:\n        prompt: \"Complete this: Hello\"\n        max_tokens: 50",
          "line_number": 85
        },
        {
          "name": "llm.local.chat",
          "function": "llm_chat_local",
          "parameters": [
            {
              "name": "messages",
              "type": "List[Dict[str, str]]",
              "required": true,
              "default": null
            },
            {
              "name": "max_tokens",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "stop",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chat completion using OpenAI-compatible format.\n\nThis is the recommended method for instruction-following models.\nThe backend handles proper chat template formatting.\n\nArgs:\n    state: Current state dictionary\n    messages: List of message dicts with 'role' and 'content'\n    max_tokens: Maximum tokens to generate\n    temperature: Sampling temperature\n    stop: Optional stop sequences\n    **kwargs: Additional backend parameters\n\nReturns:\n    {\"content\": str, \"model\": str, \"tokens_used\": int|None}\n\nExample YAML:\n    - name: chat\n      action: llm.chat\n      params:\n        messages:\n          - role: system\n            content: \"You are a helpful assistant.\"\n          - role: user\n            content: \"{{ state.question }}\"\n        max_tokens: 500",
          "line_number": 175
        },
        {
          "name": "llm.local.stream",
          "function": "llm_stream_local",
          "parameters": [
            {
              "name": "prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "messages",
              "type": "Optional[List[Dict[str, str]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_tokens",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "stop",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Streaming LLM generation with token-by-token output.\n\nStreams output to stdout and returns the complete response.\nUses local backend if configured, otherwise falls back to API.\n\nArgs:\n    state: Current state dictionary\n    prompt: Raw prompt text (for completion mode)\n    messages: Chat messages (for chat mode, takes precedence)\n    max_tokens: Maximum tokens to generate\n    temperature: Sampling temperature\n    stop: Optional stop sequences\n    **kwargs: Additional parameters\n\nReturns:\n    {\"content\": str, \"model\": str, \"streamed\": True, \"chunk_count\": int}\n\nExample YAML:\n    - name: stream_response\n      action: llm.stream\n      params:\n        prompt: \"Write a haiku about coding\"\n        max_tokens: 100",
          "line_number": 249
        },
        {
          "name": "llm.local.embed",
          "function": "llm_embed_local",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Generate text embeddings using local or API backend.\n\nNote: For local backend, the model must be loaded with embedding=True.\n\nArgs:\n    state: Current state dictionary\n    text: Input text to embed\n\nReturns:\n    {\"embedding\": List[float], \"dimensions\": int}\n\nExample YAML:\n    - name: embed_document\n      action: llm.embed\n      params:\n        text: \"{{ state.document }}\"",
          "line_number": 349
        },
        {
          "name": "llm.chat",
          "function": "llm_chat_local",
          "parameters": [
            {
              "name": "messages",
              "type": "List[Dict[str, str]]",
              "required": true,
              "default": null
            },
            {
              "name": "max_tokens",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "stop",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chat completion using OpenAI-compatible format.\n\nThis is the recommended method for instruction-following models.\nThe backend handles proper chat template formatting.\n\nArgs:\n    state: Current state dictionary\n    messages: List of message dicts with 'role' and 'content'\n    max_tokens: Maximum tokens to generate\n    temperature: Sampling temperature\n    stop: Optional stop sequences\n    **kwargs: Additional backend parameters\n\nReturns:\n    {\"content\": str, \"model\": str, \"tokens_used\": int|None}\n\nExample YAML:\n    - name: chat\n      action: llm.chat\n      params:\n        messages:\n          - role: system\n            content: \"You are a helpful assistant.\"\n          - role: user\n            content: \"{{ state.question }}\"\n        max_tokens: 500",
          "line_number": 175
        },
        {
          "name": "llm.embed",
          "function": "llm_embed_local",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Generate text embeddings using local or API backend.\n\nNote: For local backend, the model must be loaded with embedding=True.\n\nArgs:\n    state: Current state dictionary\n    text: Input text to embed\n\nReturns:\n    {\"embedding\": List[float], \"dimensions\": int}\n\nExample YAML:\n    - name: embed_document\n      action: llm.embed\n      params:\n        text: \"{{ state.document }}\"",
          "line_number": 349
        }
      ]
    },
    {
      "file": "ltm_actions.py",
      "namespace": "ltm",
      "actions": [
        {
          "name": "ltm.store",
          "function": "ltm_store",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "value",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "metadata",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Store a key-value pair persistently with optional metadata.\n\nArgs:\n    state: Current state dictionary\n    key: The key to store the value under (required)\n    value: The value to store (will be JSON serialized)\n    metadata: Optional metadata dict for filtering/tagging\n\nReturns:\n    {\"success\": True, \"stored\": True, \"key\": str, \"created\": bool} on success\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 61
        },
        {
          "name": "actions.ltm_store",
          "function": "ltm_store",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "value",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "metadata",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Store a key-value pair persistently with optional metadata.\n\nArgs:\n    state: Current state dictionary\n    key: The key to store the value under (required)\n    value: The value to store (will be JSON serialized)\n    metadata: Optional metadata dict for filtering/tagging\n\nReturns:\n    {\"success\": True, \"stored\": True, \"key\": str, \"created\": bool} on success\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 61
        },
        {
          "name": "ltm.retrieve",
          "function": "ltm_retrieve",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Retrieve a value from long-term memory by key.\n\nArgs:\n    state: Current state dictionary\n    key: The key to retrieve (required)\n    default: Default value to return if key not found\n\nReturns:\n    {\"success\": True, \"value\": any, \"found\": bool, \"metadata\": dict|None} on success\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 94
        },
        {
          "name": "actions.ltm_retrieve",
          "function": "ltm_retrieve",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Retrieve a value from long-term memory by key.\n\nArgs:\n    state: Current state dictionary\n    key: The key to retrieve (required)\n    default: Default value to return if key not found\n\nReturns:\n    {\"success\": True, \"value\": any, \"found\": bool, \"metadata\": dict|None} on success\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 94
        },
        {
          "name": "ltm.delete",
          "function": "ltm_delete",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete a value from long-term memory by key.\n\nArgs:\n    state: Current state dictionary\n    key: The key to delete (required)\n\nReturns:\n    {\"success\": True, \"deleted\": bool, \"key\": str} on success\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 126
        },
        {
          "name": "actions.ltm_delete",
          "function": "ltm_delete",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete a value from long-term memory by key.\n\nArgs:\n    state: Current state dictionary\n    key: The key to delete (required)\n\nReturns:\n    {\"success\": True, \"deleted\": bool, \"key\": str} on success\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 126
        },
        {
          "name": "ltm.search",
          "function": "ltm_search",
          "parameters": [
            {
              "name": "query",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata_filter",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "10"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Search across long-term memory using FTS5 and/or metadata filtering.\n\nArgs:\n    state: Current state dictionary\n    query: Full-text search query (FTS5 syntax)\n    metadata_filter: Dict of metadata key-value pairs to match\n    limit: Maximum number of results (default: 10)\n\nReturns:\n    {\n        \"success\": True,\n        \"results\": [{\"key\": str, \"value\": any, \"metadata\": dict, \"score\": float}, ...],\n        \"count\": int\n    } on success\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 157
        },
        {
          "name": "actions.ltm_search",
          "function": "ltm_search",
          "parameters": [
            {
              "name": "query",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata_filter",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "Any",
              "required": false,
              "default": "10"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Search across long-term memory using FTS5 and/or metadata filtering.\n\nArgs:\n    state: Current state dictionary\n    query: Full-text search query (FTS5 syntax)\n    metadata_filter: Dict of metadata key-value pairs to match\n    limit: Maximum number of results (default: 10)\n\nReturns:\n    {\n        \"success\": True,\n        \"results\": [{\"key\": str, \"value\": any, \"metadata\": dict, \"score\": float}, ...],\n        \"count\": int\n    } on success\n    {\"success\": False, \"error\": str, \"error_type\": str} on failure",
          "line_number": 157
        }
      ]
    },
    {
      "file": "markdown_actions.py",
      "namespace": "markdown",
      "actions": [
        {
          "name": "markdown.parse",
          "function": "markdown_parse",
          "parameters": [
            {
              "name": "content",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "extract",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Parse Markdown content into a structured document.\n\nUses the md-parser crate with PyO3 bindings for fast, accurate parsing\nwith guaranteed schema parity with the Rust implementation.\n\nArgs:\n    state: Current state dictionary\n    content: Raw Markdown string to parse\n    extract: Optional list of components to extract. If None, extracts all.\n             Supported: [\"tasks\", \"sections\", \"variables\", \"frontmatter\"]\n\nReturns:\n    {\n        \"title\": Optional[str],        # Document title from first H1\n        \"sections\": List[dict],        # Structured sections\n        \"variables\": List[str],        # Template variables found\n        \"frontmatter\": Optional[dict], # YAML frontmatter if present\n        \"tasks\": List[dict],           # Checklist items\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False, \"error_type\": \"parse\"|\"import\"} on failure\n\nSection dict schema:\n    {\n        \"id\": str,                     # Unique section ID\n        \"section_type\": str,           # heading, paragraph, list, code, etc.\n        \"content\": str,                # Section content\n        \"order_idx\": int,              # Order in document\n        \"variables\": List[str],        # Variables in this section\n        \"level\": Optional[int],        # Heading level (1-6)\n        \"language\": Optional[str]      # Code block language\n    }\n\nTask/ChecklistItem dict schema:\n    {\n        \"text\": str,                   # Task text\n        \"checked\": bool,               # Completion status\n        \"indent\": int,                 # Nesting level (0 = top level)\n        \"ac_refs\": List[int],          # AC references from \"(AC: 1, 2)\"\n        \"line_number\": Optional[int]   # Line number in source\n    }\n\nExample:\n    >>> result = markdown_parse(\n    ...     state={},\n    ...     content=\"# Title\\n\\n- [ ] Task 1 (AC: 1)\\n- [x] Task 2\"\n    ... )\n    >>> assert result['title'] == 'Title'\n    >>> assert len(result['tasks']) == 2\n    >>> assert result['tasks'][0]['checked'] == False\n    >>> assert result['tasks'][0]['ac_refs'] == [1]",
          "line_number": 47
        },
        {
          "name": "markdown_parse",
          "function": "markdown_parse",
          "parameters": [
            {
              "name": "content",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "extract",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Parse Markdown content into a structured document.\n\nUses the md-parser crate with PyO3 bindings for fast, accurate parsing\nwith guaranteed schema parity with the Rust implementation.\n\nArgs:\n    state: Current state dictionary\n    content: Raw Markdown string to parse\n    extract: Optional list of components to extract. If None, extracts all.\n             Supported: [\"tasks\", \"sections\", \"variables\", \"frontmatter\"]\n\nReturns:\n    {\n        \"title\": Optional[str],        # Document title from first H1\n        \"sections\": List[dict],        # Structured sections\n        \"variables\": List[str],        # Template variables found\n        \"frontmatter\": Optional[dict], # YAML frontmatter if present\n        \"tasks\": List[dict],           # Checklist items\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False, \"error_type\": \"parse\"|\"import\"} on failure\n\nSection dict schema:\n    {\n        \"id\": str,                     # Unique section ID\n        \"section_type\": str,           # heading, paragraph, list, code, etc.\n        \"content\": str,                # Section content\n        \"order_idx\": int,              # Order in document\n        \"variables\": List[str],        # Variables in this section\n        \"level\": Optional[int],        # Heading level (1-6)\n        \"language\": Optional[str]      # Code block language\n    }\n\nTask/ChecklistItem dict schema:\n    {\n        \"text\": str,                   # Task text\n        \"checked\": bool,               # Completion status\n        \"indent\": int,                 # Nesting level (0 = top level)\n        \"ac_refs\": List[int],          # AC references from \"(AC: 1, 2)\"\n        \"line_number\": Optional[int]   # Line number in source\n    }\n\nExample:\n    >>> result = markdown_parse(\n    ...     state={},\n    ...     content=\"# Title\\n\\n- [ ] Task 1 (AC: 1)\\n- [x] Task 2\"\n    ... )\n    >>> assert result['title'] == 'Title'\n    >>> assert len(result['tasks']) == 2\n    >>> assert result['tasks'][0]['checked'] == False\n    >>> assert result['tasks'][0]['ac_refs'] == [1]",
          "line_number": 47
        }
      ]
    },
    {
      "file": "mem0_actions.py",
      "namespace": "mem0",
      "actions": [
        {
          "name": "memory.mem0.add",
          "function": "memory_mem0_add",
          "parameters": [
            {
              "name": "messages",
              "type": "Union[str, List[Dict[str, str]], Dict[str, str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "user_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Store messages with automatic fact extraction using Mem0.\n\nThis action stores conversation messages and automatically extracts\nfacts and relationships for later retrieval.\n\nArgs:\n    state: Current state dictionary\n    messages: Conversation messages in one of these formats:\n        - String: Single user message\n        - Dict: {\"role\": \"user\", \"content\": \"...\"}\n        - List[Dict]: Full conversation history\n    user_id: User scope for the memory\n    session_id: Session scope for the memory\n    agent_id: Agent scope for the memory\n    metadata: Additional metadata to store with memories\n\nReturns:\n    {\n        \"success\": True,\n        \"memory_id\": \"...\",  # If single memory created\n        \"memories\": [...],   # List of extracted memories\n    }\n    Or {\"success\": False, \"error\": \"...\"} on failure\n\nExample YAML:\n    - name: store_conversation\n      action: memory.mem0.add\n      with:\n        messages:\n          - role: user\n            content: \"{{ state.user_input }}\"\n          - role: assistant\n            content: \"{{ state.response }}\"\n        user_id: \"{{ state.user_id }}\"",
          "line_number": 180
        },
        {
          "name": "actions.memory_mem0_add",
          "function": "memory_mem0_add",
          "parameters": [
            {
              "name": "messages",
              "type": "Union[str, List[Dict[str, str]], Dict[str, str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "user_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Store messages with automatic fact extraction using Mem0.\n\nThis action stores conversation messages and automatically extracts\nfacts and relationships for later retrieval.\n\nArgs:\n    state: Current state dictionary\n    messages: Conversation messages in one of these formats:\n        - String: Single user message\n        - Dict: {\"role\": \"user\", \"content\": \"...\"}\n        - List[Dict]: Full conversation history\n    user_id: User scope for the memory\n    session_id: Session scope for the memory\n    agent_id: Agent scope for the memory\n    metadata: Additional metadata to store with memories\n\nReturns:\n    {\n        \"success\": True,\n        \"memory_id\": \"...\",  # If single memory created\n        \"memories\": [...],   # List of extracted memories\n    }\n    Or {\"success\": False, \"error\": \"...\"} on failure\n\nExample YAML:\n    - name: store_conversation\n      action: memory.mem0.add\n      with:\n        messages:\n          - role: user\n            content: \"{{ state.user_input }}\"\n          - role: assistant\n            content: \"{{ state.response }}\"\n        user_id: \"{{ state.user_id }}\"",
          "line_number": 180
        },
        {
          "name": "memory.mem0.search",
          "function": "memory_mem0_search",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "user_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "include_relations",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search memories by semantic similarity using Mem0.\n\nRetrieves relevant memories based on the query string using\nvector similarity search.\n\nArgs:\n    state: Current state dictionary\n    query: Search query string\n    user_id: Filter by user scope\n    session_id: Filter by session scope\n    agent_id: Filter by agent scope\n    limit: Maximum number of results (default: 5)\n    include_relations: Include graph relations (requires graph: true)\n\nReturns:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"id\": \"...\",\n                \"memory\": \"...\",\n                \"score\": 0.95,\n                \"metadata\": {...}\n            },\n            ...\n        ],\n        \"relations\": [...]  # If include_relations=True\n    }\n    Or {\"success\": False, \"error\": \"...\", \"results\": []} on failure\n\nExample YAML:\n    - name: recall_context\n      action: memory.mem0.search\n      with:\n        query: \"{{ state.user_question }}\"\n        user_id: \"{{ state.user_id }}\"\n        limit: 5",
          "line_number": 254
        },
        {
          "name": "actions.memory_mem0_search",
          "function": "memory_mem0_search",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "user_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "include_relations",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search memories by semantic similarity using Mem0.\n\nRetrieves relevant memories based on the query string using\nvector similarity search.\n\nArgs:\n    state: Current state dictionary\n    query: Search query string\n    user_id: Filter by user scope\n    session_id: Filter by session scope\n    agent_id: Filter by agent scope\n    limit: Maximum number of results (default: 5)\n    include_relations: Include graph relations (requires graph: true)\n\nReturns:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"id\": \"...\",\n                \"memory\": \"...\",\n                \"score\": 0.95,\n                \"metadata\": {...}\n            },\n            ...\n        ],\n        \"relations\": [...]  # If include_relations=True\n    }\n    Or {\"success\": False, \"error\": \"...\", \"results\": []} on failure\n\nExample YAML:\n    - name: recall_context\n      action: memory.mem0.search\n      with:\n        query: \"{{ state.user_question }}\"\n        user_id: \"{{ state.user_id }}\"\n        limit: 5",
          "line_number": 254
        },
        {
          "name": "memory.mem0.get_all",
          "function": "memory_mem0_get_all",
          "parameters": [
            {
              "name": "user_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "offset",
              "type": "int",
              "required": false,
              "default": "0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get all memories for a specified scope.\n\nRetrieves all stored memories for the given user, session, or agent\nwith optional pagination.\n\nArgs:\n    state: Current state dictionary\n    user_id: Filter by user scope\n    session_id: Filter by session scope\n    agent_id: Filter by agent scope\n    limit: Maximum number of results (optional)\n    offset: Skip first N results for pagination\n\nReturns:\n    {\n        \"success\": True,\n        \"memories\": [...],\n        \"total\": 42\n    }\n    Or {\"success\": False, \"error\": \"...\", \"memories\": []} on failure\n\nExample YAML:\n    - name: get_user_memories\n      action: memory.mem0.get_all\n      with:\n        user_id: \"{{ state.user_id }}\"\n        limit: 20\n        offset: 0",
          "line_number": 333
        },
        {
          "name": "actions.memory_mem0_get_all",
          "function": "memory_mem0_get_all",
          "parameters": [
            {
              "name": "user_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "offset",
              "type": "int",
              "required": false,
              "default": "0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get all memories for a specified scope.\n\nRetrieves all stored memories for the given user, session, or agent\nwith optional pagination.\n\nArgs:\n    state: Current state dictionary\n    user_id: Filter by user scope\n    session_id: Filter by session scope\n    agent_id: Filter by agent scope\n    limit: Maximum number of results (optional)\n    offset: Skip first N results for pagination\n\nReturns:\n    {\n        \"success\": True,\n        \"memories\": [...],\n        \"total\": 42\n    }\n    Or {\"success\": False, \"error\": \"...\", \"memories\": []} on failure\n\nExample YAML:\n    - name: get_user_memories\n      action: memory.mem0.get_all\n      with:\n        user_id: \"{{ state.user_id }}\"\n        limit: 20\n        offset: 0",
          "line_number": 333
        },
        {
          "name": "memory.mem0.get",
          "function": "memory_mem0_get",
          "parameters": [
            {
              "name": "memory_id",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get a specific memory by its ID.\n\nArgs:\n    state: Current state dictionary\n    memory_id: The memory identifier\n\nReturns:\n    {\n        \"success\": True,\n        \"memory\": {...}\n    }\n    Or {\"success\": False, \"error\": \"...\"} on failure\n\nExample YAML:\n    - name: get_memory\n      action: memory.mem0.get\n      with:\n        memory_id: \"{{ state.memory_id }}\"",
          "line_number": 393
        },
        {
          "name": "actions.memory_mem0_get",
          "function": "memory_mem0_get",
          "parameters": [
            {
              "name": "memory_id",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get a specific memory by its ID.\n\nArgs:\n    state: Current state dictionary\n    memory_id: The memory identifier\n\nReturns:\n    {\n        \"success\": True,\n        \"memory\": {...}\n    }\n    Or {\"success\": False, \"error\": \"...\"} on failure\n\nExample YAML:\n    - name: get_memory\n      action: memory.mem0.get\n      with:\n        memory_id: \"{{ state.memory_id }}\"",
          "line_number": 393
        },
        {
          "name": "memory.mem0.update",
          "function": "memory_mem0_update",
          "parameters": [
            {
              "name": "memory_id",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "text",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Update an existing memory by ID.\n\nSupports partial updates - only specified fields are modified.\nMetadata is merged with existing metadata.\n\nArgs:\n    state: Current state dictionary\n    memory_id: The memory identifier to update\n    text: New text content (optional)\n    metadata: Metadata to update/merge (optional)\n\nReturns:\n    {\n        \"success\": True,\n        \"memory\": {...}  # Updated memory object\n    }\n    Or {\"success\": False, \"error\": \"...\"} on failure\n\nExample YAML:\n    - name: update_memory\n      action: memory.mem0.update\n      with:\n        memory_id: \"{{ state.memory_id }}\"\n        metadata:\n          verified: true\n          updated_at: \"{{ state.timestamp }}\"",
          "line_number": 436
        },
        {
          "name": "actions.memory_mem0_update",
          "function": "memory_mem0_update",
          "parameters": [
            {
              "name": "memory_id",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "text",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Update an existing memory by ID.\n\nSupports partial updates - only specified fields are modified.\nMetadata is merged with existing metadata.\n\nArgs:\n    state: Current state dictionary\n    memory_id: The memory identifier to update\n    text: New text content (optional)\n    metadata: Metadata to update/merge (optional)\n\nReturns:\n    {\n        \"success\": True,\n        \"memory\": {...}  # Updated memory object\n    }\n    Or {\"success\": False, \"error\": \"...\"} on failure\n\nExample YAML:\n    - name: update_memory\n      action: memory.mem0.update\n      with:\n        memory_id: \"{{ state.memory_id }}\"\n        metadata:\n          verified: true\n          updated_at: \"{{ state.timestamp }}\"",
          "line_number": 436
        },
        {
          "name": "memory.mem0.delete",
          "function": "memory_mem0_delete",
          "parameters": [
            {
              "name": "memory_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "user_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "delete_all",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delete memories by ID or scope.\n\nCan delete a single memory by ID or bulk delete all memories\nfor a user, session, or agent.\n\nArgs:\n    state: Current state dictionary\n    memory_id: Delete specific memory by ID\n    user_id: Delete all memories for user (requires delete_all=True)\n    session_id: Delete all memories for session (requires delete_all=True)\n    agent_id: Delete all memories for agent (requires delete_all=True)\n    delete_all: Must be True for bulk delete operations (safety flag)\n\nReturns:\n    {\n        \"success\": True,\n        \"deleted_count\": 1\n    }\n    Or {\"success\": False, \"error\": \"...\"} on failure\n\nExample YAML (single delete):\n    - name: delete_memory\n      action: memory.mem0.delete\n      with:\n        memory_id: \"{{ state.memory_id }}\"\n\nExample YAML (bulk delete):\n    - name: delete_user_memories\n      action: memory.mem0.delete\n      with:\n        user_id: \"{{ state.user_id }}\"\n        delete_all: true",
          "line_number": 499
        },
        {
          "name": "actions.memory_mem0_delete",
          "function": "memory_mem0_delete",
          "parameters": [
            {
              "name": "memory_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "user_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "delete_all",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delete memories by ID or scope.\n\nCan delete a single memory by ID or bulk delete all memories\nfor a user, session, or agent.\n\nArgs:\n    state: Current state dictionary\n    memory_id: Delete specific memory by ID\n    user_id: Delete all memories for user (requires delete_all=True)\n    session_id: Delete all memories for session (requires delete_all=True)\n    agent_id: Delete all memories for agent (requires delete_all=True)\n    delete_all: Must be True for bulk delete operations (safety flag)\n\nReturns:\n    {\n        \"success\": True,\n        \"deleted_count\": 1\n    }\n    Or {\"success\": False, \"error\": \"...\"} on failure\n\nExample YAML (single delete):\n    - name: delete_memory\n      action: memory.mem0.delete\n      with:\n        memory_id: \"{{ state.memory_id }}\"\n\nExample YAML (bulk delete):\n    - name: delete_user_memories\n      action: memory.mem0.delete\n      with:\n        user_id: \"{{ state.user_id }}\"\n        delete_all: true",
          "line_number": 499
        },
        {
          "name": "memory.mem0.test",
          "function": "memory_mem0_test",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Test Mem0 connection and configuration.\n\nUseful for verifying setup before using other Mem0 actions.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\n        \"success\": True,\n        \"message\": \"Mem0 connection successful\"\n    }\n    Or {\"success\": False, \"error\": \"...\", \"message\": \"...\"} on failure\n\nExample YAML:\n    - name: check_mem0\n      action: memory.mem0.test",
          "line_number": 566
        },
        {
          "name": "actions.memory_mem0_test",
          "function": "memory_mem0_test",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Test Mem0 connection and configuration.\n\nUseful for verifying setup before using other Mem0 actions.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\n        \"success\": True,\n        \"message\": \"Mem0 connection successful\"\n    }\n    Or {\"success\": False, \"error\": \"...\", \"message\": \"...\"} on failure\n\nExample YAML:\n    - name: check_mem0\n      action: memory.mem0.test",
          "line_number": 566
        }
      ]
    },
    {
      "file": "memory_actions.py",
      "namespace": "memory",
      "actions": [
        {
          "name": "memory.store",
          "function": "memory_store",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "value",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "ttl",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "namespace",
              "type": "Any",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Store a key-value pair in memory with optional TTL.\n\nArgs:\n    state: Current state dictionary\n    key: The key to store the value under\n    value: The value to store (must be pickle-serializable)\n    ttl: Time-to-live in seconds (None for no expiration)\n    namespace: Namespace for key isolation (default: \"default\")\n\nReturns:\n    {\"stored\": True, \"key\": str, \"namespace\": str} on success\n    {\"stored\": False, \"key\": str, \"error\": str} on failure",
          "line_number": 52
        },
        {
          "name": "actions.memory_store",
          "function": "memory_store",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "value",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "ttl",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "namespace",
              "type": "Any",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Store a key-value pair in memory with optional TTL.\n\nArgs:\n    state: Current state dictionary\n    key: The key to store the value under\n    value: The value to store (must be pickle-serializable)\n    ttl: Time-to-live in seconds (None for no expiration)\n    namespace: Namespace for key isolation (default: \"default\")\n\nReturns:\n    {\"stored\": True, \"key\": str, \"namespace\": str} on success\n    {\"stored\": False, \"key\": str, \"error\": str} on failure",
          "line_number": 52
        },
        {
          "name": "memory.retrieve",
          "function": "memory_retrieve",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "namespace",
              "type": "Any",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Retrieve a value from memory by key.\n\nArgs:\n    state: Current state dictionary\n    key: The key to retrieve\n    default: Default value to return if key not found or expired\n    namespace: Namespace to look in (default: \"default\")\n\nReturns:\n    {\"value\": any, \"found\": True, \"key\": str} if found\n    {\"value\": default, \"found\": False, \"key\": str} if not found/expired",
          "line_number": 97
        },
        {
          "name": "actions.memory_retrieve",
          "function": "memory_retrieve",
          "parameters": [
            {
              "name": "key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "namespace",
              "type": "Any",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Retrieve a value from memory by key.\n\nArgs:\n    state: Current state dictionary\n    key: The key to retrieve\n    default: Default value to return if key not found or expired\n    namespace: Namespace to look in (default: \"default\")\n\nReturns:\n    {\"value\": any, \"found\": True, \"key\": str} if found\n    {\"value\": default, \"found\": False, \"key\": str} if not found/expired",
          "line_number": 97
        },
        {
          "name": "memory.summarize",
          "function": "memory_summarize",
          "parameters": [
            {
              "name": "messages_key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "max_tokens",
              "type": "Any",
              "required": false,
              "default": "1000"
            },
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Summarize conversation history using LLM to fit token windows.\n\nUses the internal llm.call action for summarization. Implements\na sliding window + summarization strategy for long conversations.\n\nArgs:\n    state: Current state dictionary\n    messages_key: Key in state containing messages list\n    max_tokens: Maximum tokens for the summary (default: 1000)\n    model: LLM model to use (default: gpt-3.5-turbo)\n\nReturns:\n    {\n        \"summary\": str,\n        \"original_count\": int,\n        \"token_estimate\": int,\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 148
        },
        {
          "name": "actions.memory_summarize",
          "function": "memory_summarize",
          "parameters": [
            {
              "name": "messages_key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "max_tokens",
              "type": "Any",
              "required": false,
              "default": "1000"
            },
            {
              "name": "model",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Summarize conversation history using LLM to fit token windows.\n\nUses the internal llm.call action for summarization. Implements\na sliding window + summarization strategy for long conversations.\n\nArgs:\n    state: Current state dictionary\n    messages_key: Key in state containing messages list\n    max_tokens: Maximum tokens for the summary (default: 1000)\n    model: LLM model to use (default: gpt-3.5-turbo)\n\nReturns:\n    {\n        \"summary\": str,\n        \"original_count\": int,\n        \"token_estimate\": int,\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 148
        }
      ]
    },
    {
      "file": "neo4j_gds_actions.py",
      "namespace": "neo4j_gds",
      "actions": [
        {
          "name": "neo4j.gds_check_available",
          "function": "neo4j_gds_check_available",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if Neo4j GDS library is available.\n\nReturns:\n    {\"success\": True, \"gds_available\": bool}",
          "line_number": 104
        },
        {
          "name": "actions.neo4j_gds_check_available",
          "function": "neo4j_gds_check_available",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if Neo4j GDS library is available.\n\nReturns:\n    {\"success\": True, \"gds_available\": bool}",
          "line_number": 104
        },
        {
          "name": "neo4j.gds_version",
          "function": "neo4j_gds_version",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Get the installed Neo4j GDS library version.\n\nReturns:\n    {\"success\": True, \"version\": str} if GDS is available\n    {\"success\": False, \"error\": str, \"error_type\": str} if not",
          "line_number": 124
        },
        {
          "name": "actions.neo4j_gds_version",
          "function": "neo4j_gds_version",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Get the installed Neo4j GDS library version.\n\nReturns:\n    {\"success\": True, \"version\": str} if GDS is available\n    {\"success\": False, \"error\": str, \"error_type\": str} if not",
          "line_number": 124
        },
        {
          "name": "neo4j.gds_project_graph",
          "function": "neo4j_gds_project_graph",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "node_projection",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relationship_projection",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Create an in-memory graph projection for GDS algorithms.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name for the projected graph\n    node_projection: Node labels to include (string, list, or dict)\n    relationship_projection: Relationship types (string, list, or dict)\n    config: Additional configuration options\n\nReturns:\n    {\"success\": True, \"graph_name\": str, \"node_count\": int,\n     \"relationship_count\": int}",
          "line_number": 145
        },
        {
          "name": "actions.neo4j_gds_project_graph",
          "function": "neo4j_gds_project_graph",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "node_projection",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "relationship_projection",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Create an in-memory graph projection for GDS algorithms.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name for the projected graph\n    node_projection: Node labels to include (string, list, or dict)\n    relationship_projection: Relationship types (string, list, or dict)\n    config: Additional configuration options\n\nReturns:\n    {\"success\": True, \"graph_name\": str, \"node_count\": int,\n     \"relationship_count\": int}",
          "line_number": 145
        },
        {
          "name": "neo4j.gds_drop_graph",
          "function": "neo4j_gds_drop_graph",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Drop (remove) an in-memory graph projection.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph to drop\n\nReturns:\n    {\"success\": True, \"graph_name\": str, \"dropped\": True}",
          "line_number": 181
        },
        {
          "name": "actions.neo4j_gds_drop_graph",
          "function": "neo4j_gds_drop_graph",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Drop (remove) an in-memory graph projection.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph to drop\n\nReturns:\n    {\"success\": True, \"graph_name\": str, \"dropped\": True}",
          "line_number": 181
        },
        {
          "name": "neo4j.gds_list_graphs",
          "function": "neo4j_gds_list_graphs",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List all active in-memory graph projections.\n\nReturns:\n    {\"success\": True, \"graphs\": [...], \"count\": int}",
          "line_number": 201
        },
        {
          "name": "actions.neo4j_gds_list_graphs",
          "function": "neo4j_gds_list_graphs",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List all active in-memory graph projections.\n\nReturns:\n    {\"success\": True, \"graphs\": [...], \"count\": int}",
          "line_number": 201
        },
        {
          "name": "neo4j.gds_estimate_memory",
          "function": "neo4j_gds_estimate_memory",
          "parameters": [
            {
              "name": "algorithm",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Estimate memory requirements for a GDS algorithm.\n\nArgs:\n    state: Current state dictionary\n    algorithm: Algorithm name (e.g., \"pageRank\", \"louvain\")\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"required_memory\": str, ...}",
          "line_number": 217
        },
        {
          "name": "actions.neo4j_gds_estimate_memory",
          "function": "neo4j_gds_estimate_memory",
          "parameters": [
            {
              "name": "algorithm",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Estimate memory requirements for a GDS algorithm.\n\nArgs:\n    state: Current state dictionary\n    algorithm: Algorithm name (e.g., \"pageRank\", \"louvain\")\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"required_memory\": str, ...}",
          "line_number": 217
        },
        {
          "name": "neo4j.gds_page_rank",
          "function": "neo4j_gds_page_rank",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run PageRank algorithm on a projected graph.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n        - maxIterations: Max iterations (default: 20)\n        - dampingFactor: Damping factor (default: 0.85)\n        - mode: \"stream\", \"write\", \"mutate\", or \"stats\"\n\nReturns:\n    {\"success\": True, \"results\": [...]} (stream mode)",
          "line_number": 245
        },
        {
          "name": "actions.neo4j_gds_page_rank",
          "function": "neo4j_gds_page_rank",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run PageRank algorithm on a projected graph.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n        - maxIterations: Max iterations (default: 20)\n        - dampingFactor: Damping factor (default: 0.85)\n        - mode: \"stream\", \"write\", \"mutate\", or \"stats\"\n\nReturns:\n    {\"success\": True, \"results\": [...]} (stream mode)",
          "line_number": 245
        },
        {
          "name": "neo4j.gds_betweenness",
          "function": "neo4j_gds_betweenness",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Betweenness Centrality algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [...]}",
          "line_number": 269
        },
        {
          "name": "actions.neo4j_gds_betweenness",
          "function": "neo4j_gds_betweenness",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Betweenness Centrality algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [...]}",
          "line_number": 269
        },
        {
          "name": "neo4j.gds_degree",
          "function": "neo4j_gds_degree",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Degree Centrality algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [...]}",
          "line_number": 290
        },
        {
          "name": "actions.neo4j_gds_degree",
          "function": "neo4j_gds_degree",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Degree Centrality algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [...]}",
          "line_number": 290
        },
        {
          "name": "neo4j.gds_closeness",
          "function": "neo4j_gds_closeness",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Closeness Centrality algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [...]}",
          "line_number": 311
        },
        {
          "name": "actions.neo4j_gds_closeness",
          "function": "neo4j_gds_closeness",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Closeness Centrality algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [...]}",
          "line_number": 311
        },
        {
          "name": "neo4j.gds_louvain",
          "function": "neo4j_gds_louvain",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Louvain community detection algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity_id\": str, \"community_id\": int}, ...]}",
          "line_number": 336
        },
        {
          "name": "actions.neo4j_gds_louvain",
          "function": "neo4j_gds_louvain",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Louvain community detection algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity_id\": str, \"community_id\": int}, ...]}",
          "line_number": 336
        },
        {
          "name": "neo4j.gds_label_propagation",
          "function": "neo4j_gds_label_propagation",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Label Propagation community detection algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity_id\": str, \"community_id\": int}, ...]}",
          "line_number": 357
        },
        {
          "name": "actions.neo4j_gds_label_propagation",
          "function": "neo4j_gds_label_propagation",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Label Propagation community detection algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity_id\": str, \"community_id\": int}, ...]}",
          "line_number": 357
        },
        {
          "name": "neo4j.gds_wcc",
          "function": "neo4j_gds_wcc",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Weakly Connected Components algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity_id\": str, \"community_id\": int}, ...]}",
          "line_number": 378
        },
        {
          "name": "actions.neo4j_gds_wcc",
          "function": "neo4j_gds_wcc",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run Weakly Connected Components algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity_id\": str, \"community_id\": int}, ...]}",
          "line_number": 378
        },
        {
          "name": "neo4j.gds_dijkstra",
          "function": "neo4j_gds_dijkstra",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "source_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "target_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Find shortest weighted path using Dijkstra's algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    source_id: Source entity ID\n    target_id: Target entity ID\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"path\": [...], \"total_cost\": float}",
          "line_number": 403
        },
        {
          "name": "actions.neo4j_gds_dijkstra",
          "function": "neo4j_gds_dijkstra",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "source_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "target_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Find shortest weighted path using Dijkstra's algorithm.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    source_id: Source entity ID\n    target_id: Target entity ID\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"path\": [...], \"total_cost\": float}",
          "line_number": 403
        },
        {
          "name": "neo4j.gds_astar",
          "function": "neo4j_gds_astar",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "source_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "target_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Find shortest path using A* algorithm with heuristic.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    source_id: Source entity ID\n    target_id: Target entity ID\n    config: Algorithm configuration (requires latitudeProperty, longitudeProperty)\n\nReturns:\n    {\"success\": True, \"path\": [...], \"total_cost\": float}",
          "line_number": 433
        },
        {
          "name": "actions.neo4j_gds_astar",
          "function": "neo4j_gds_astar",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "source_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "target_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Find shortest path using A* algorithm with heuristic.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    source_id: Source entity ID\n    target_id: Target entity ID\n    config: Algorithm configuration (requires latitudeProperty, longitudeProperty)\n\nReturns:\n    {\"success\": True, \"path\": [...], \"total_cost\": float}",
          "line_number": 433
        },
        {
          "name": "neo4j.gds_all_shortest_paths",
          "function": "neo4j_gds_all_shortest_paths",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "source_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Find shortest paths from source to all other nodes.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    source_id: Source entity ID\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"paths\": [...], \"count\": int}",
          "line_number": 461
        },
        {
          "name": "actions.neo4j_gds_all_shortest_paths",
          "function": "neo4j_gds_all_shortest_paths",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "source_id",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Find shortest paths from source to all other nodes.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    source_id: Source entity ID\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"paths\": [...], \"count\": int}",
          "line_number": 461
        },
        {
          "name": "neo4j.gds_node_similarity",
          "function": "neo4j_gds_node_similarity",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Compute Jaccard similarity between nodes based on shared neighbors.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity1_id\": str, \"entity2_id\": str,\n     \"similarity\": float}, ...]}",
          "line_number": 491
        },
        {
          "name": "actions.neo4j_gds_node_similarity",
          "function": "neo4j_gds_node_similarity",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Compute Jaccard similarity between nodes based on shared neighbors.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity1_id\": str, \"entity2_id\": str,\n     \"similarity\": float}, ...]}",
          "line_number": 491
        },
        {
          "name": "neo4j.gds_knn",
          "function": "neo4j_gds_knn",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run K-Nearest Neighbors algorithm on node properties.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration (requires nodeProperties)\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity1_id\": str, \"entity2_id\": str,\n     \"similarity\": float}, ...]}",
          "line_number": 513
        },
        {
          "name": "actions.neo4j_gds_knn",
          "function": "neo4j_gds_knn",
          "parameters": [
            {
              "name": "graph_name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Run K-Nearest Neighbors algorithm on node properties.\n\nArgs:\n    state: Current state dictionary\n    graph_name: Name of the projected graph\n    config: Algorithm configuration (requires nodeProperties)\n\nReturns:\n    {\"success\": True, \"results\": [{\"entity1_id\": str, \"entity2_id\": str,\n     \"similarity\": float}, ...]}",
          "line_number": 513
        }
      ]
    },
    {
      "file": "neo4j_trigger_actions.py",
      "namespace": "neo4j_trigger",
      "actions": [
        {
          "name": "neo4j.check_apoc",
          "function": "neo4j_check_apoc",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if APOC library is installed and available.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\"success\": True, \"available\": bool, \"version\": str or None}",
          "line_number": 85
        },
        {
          "name": "actions.neo4j_check_apoc",
          "function": "neo4j_check_apoc",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if APOC library is installed and available.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\"success\": True, \"available\": bool, \"version\": str or None}",
          "line_number": 85
        },
        {
          "name": "neo4j.get_apoc_version",
          "function": "neo4j_get_apoc_version",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Get the installed APOC library version.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\"success\": True, \"version\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 104
        },
        {
          "name": "actions.neo4j_get_apoc_version",
          "function": "neo4j_get_apoc_version",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Get the installed APOC library version.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\"success\": True, \"version\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 104
        },
        {
          "name": "neo4j.check_triggers",
          "function": "neo4j_check_triggers",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if APOC triggers are enabled in Neo4j configuration.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\"success\": True, \"enabled\": bool, \"refresh_interval\": int or None}",
          "line_number": 124
        },
        {
          "name": "actions.neo4j_check_triggers",
          "function": "neo4j_check_triggers",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if APOC triggers are enabled in Neo4j configuration.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\"success\": True, \"enabled\": bool, \"refresh_interval\": int or None}",
          "line_number": 124
        },
        {
          "name": "neo4j.register_trigger",
          "function": "neo4j_register_trigger",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "query",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "selector",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Register a database trigger using APOC.\n\nArgs:\n    state: Current state dictionary\n    name: Unique trigger identifier\n    query: Cypher query to execute when trigger fires\n    selector: What changes to watch (optional, see docs for options)\n    config: Trigger configuration:\n        - phase: \"before\" or \"after\" (default: \"after\")\n        - params: Additional parameters passed to trigger query\n\nSupported selectors:\n    - createdNodes: New nodes created\n    - createdRelationships: New relationships created\n    - deletedNodes: Nodes deleted\n    - deletedRelationships: Relationships deleted\n    - assignedLabels: Labels added to nodes\n    - removedLabels: Labels removed from nodes\n    - assignedNodeProperties: Node properties set\n    - assignedRelationshipProperties: Relationship properties set\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"registered\": True}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 147
        },
        {
          "name": "actions.neo4j_register_trigger",
          "function": "neo4j_register_trigger",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "query",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "selector",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Register a database trigger using APOC.\n\nArgs:\n    state: Current state dictionary\n    name: Unique trigger identifier\n    query: Cypher query to execute when trigger fires\n    selector: What changes to watch (optional, see docs for options)\n    config: Trigger configuration:\n        - phase: \"before\" or \"after\" (default: \"after\")\n        - params: Additional parameters passed to trigger query\n\nSupported selectors:\n    - createdNodes: New nodes created\n    - createdRelationships: New relationships created\n    - deletedNodes: Nodes deleted\n    - deletedRelationships: Relationships deleted\n    - assignedLabels: Labels added to nodes\n    - removedLabels: Labels removed from nodes\n    - assignedNodeProperties: Node properties set\n    - assignedRelationshipProperties: Relationship properties set\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"registered\": True}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 147
        },
        {
          "name": "neo4j.unregister_trigger",
          "function": "neo4j_unregister_trigger",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Remove a registered trigger.\n\nArgs:\n    state: Current state dictionary\n    name: Trigger name to remove\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"removed\": True}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 201
        },
        {
          "name": "actions.neo4j_unregister_trigger",
          "function": "neo4j_unregister_trigger",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Remove a registered trigger.\n\nArgs:\n    state: Current state dictionary\n    name: Trigger name to remove\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"removed\": True}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 201
        },
        {
          "name": "neo4j.list_triggers",
          "function": "neo4j_list_triggers",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List all registered triggers.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\n        \"success\": True,\n        \"triggers\": [\n            {\n                \"name\": str,\n                \"query\": str,\n                \"selector\": dict,\n                \"params\": dict,\n                \"installed\": bool,\n                \"paused\": bool\n            },\n            ...\n        ],\n        \"count\": int\n    }",
          "line_number": 229
        },
        {
          "name": "actions.neo4j_list_triggers",
          "function": "neo4j_list_triggers",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List all registered triggers.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    {\n        \"success\": True,\n        \"triggers\": [\n            {\n                \"name\": str,\n                \"query\": str,\n                \"selector\": dict,\n                \"params\": dict,\n                \"installed\": bool,\n                \"paused\": bool\n            },\n            ...\n        ],\n        \"count\": int\n    }",
          "line_number": 229
        },
        {
          "name": "neo4j.pause_trigger",
          "function": "neo4j_pause_trigger",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Temporarily disable a trigger without removing it.\n\nArgs:\n    state: Current state dictionary\n    name: Trigger name to pause\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"paused\": True}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 262
        },
        {
          "name": "actions.neo4j_pause_trigger",
          "function": "neo4j_pause_trigger",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Temporarily disable a trigger without removing it.\n\nArgs:\n    state: Current state dictionary\n    name: Trigger name to pause\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"paused\": True}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 262
        },
        {
          "name": "neo4j.resume_trigger",
          "function": "neo4j_resume_trigger",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Re-enable a paused trigger.\n\nArgs:\n    state: Current state dictionary\n    name: Trigger name to resume\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"paused\": False}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 290
        },
        {
          "name": "actions.neo4j_resume_trigger",
          "function": "neo4j_resume_trigger",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Re-enable a paused trigger.\n\nArgs:\n    state: Current state dictionary\n    name: Trigger name to resume\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"paused\": False}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 290
        },
        {
          "name": "neo4j.register_callback",
          "function": "neo4j_register_callback",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "callback_url",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Register a trigger that fires an HTTP webhook on graph changes.\n\nThe trigger will POST to the callback_url when changes matching\nthe selector occur.\n\nArgs:\n    state: Current state dictionary\n    name: Unique trigger identifier\n    callback_url: HTTP endpoint to POST to when trigger fires\n    config: Trigger configuration:\n        - selector: What changes to watch (default: createdNodes)\n        - phase: \"before\" or \"after\" (default: \"after\")\n        - headers: Optional HTTP headers\n        - label_filter: Optional label to filter nodes\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"registered\": True, \"callback_url\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 322
        },
        {
          "name": "actions.neo4j_register_callback",
          "function": "neo4j_register_callback",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "callback_url",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Register a trigger that fires an HTTP webhook on graph changes.\n\nThe trigger will POST to the callback_url when changes matching\nthe selector occur.\n\nArgs:\n    state: Current state dictionary\n    name: Unique trigger identifier\n    callback_url: HTTP endpoint to POST to when trigger fires\n    config: Trigger configuration:\n        - selector: What changes to watch (default: createdNodes)\n        - phase: \"before\" or \"after\" (default: \"after\")\n        - headers: Optional HTTP headers\n        - label_filter: Optional label to filter nodes\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"registered\": True, \"callback_url\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 322
        },
        {
          "name": "neo4j.register_state_update",
          "function": "neo4j_register_state_update",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "state_key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "transform",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Register a trigger that writes to a state node for agent consumption.\n\nCreates/updates a TriggerStateLog node that agents can query for\ntriggered events. This provides an in-database event queue pattern.\n\nArgs:\n    state: Current state dictionary\n    name: Unique trigger identifier\n    state_key: Key to use in the TriggerStateLog node\n    transform: Optional Cypher expression to transform the data\n    config: Trigger configuration:\n        - selector: What changes to watch (default: createdNodes)\n        - phase: \"before\" or \"after\" (default: \"after\")\n        - label_filter: Optional label to filter nodes\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"registered\": True, \"state_key\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 368
        },
        {
          "name": "actions.neo4j_register_state_update",
          "function": "neo4j_register_state_update",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "state_key",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "transform",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "config",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Register a trigger that writes to a state node for agent consumption.\n\nCreates/updates a TriggerStateLog node that agents can query for\ntriggered events. This provides an in-database event queue pattern.\n\nArgs:\n    state: Current state dictionary\n    name: Unique trigger identifier\n    state_key: Key to use in the TriggerStateLog node\n    transform: Optional Cypher expression to transform the data\n    config: Trigger configuration:\n        - selector: What changes to watch (default: createdNodes)\n        - phase: \"before\" or \"after\" (default: \"after\")\n        - label_filter: Optional label to filter nodes\n\nReturns:\n    {\"success\": True, \"trigger_name\": str, \"registered\": True, \"state_key\": str}\n    or {\"success\": False, \"error\": str, \"error_type\": str}",
          "line_number": 368
        },
        {
          "name": "neo4j.cleanup_triggers",
          "function": "neo4j_cleanup_triggers",
          "parameters": [
            {
              "name": "prefix",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Remove triggers by prefix, used for session/agent cleanup.\n\nArgs:\n    state: Current state dictionary\n    prefix: If provided, only remove triggers with names starting\n           with this prefix. If None, removes all triggers (use with caution).\n\nReturns:\n    {\n        \"success\": True,\n        \"removed\": list of trigger names removed,\n        \"count\": int\n    }",
          "line_number": 420
        },
        {
          "name": "actions.neo4j_cleanup_triggers",
          "function": "neo4j_cleanup_triggers",
          "parameters": [
            {
              "name": "prefix",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Remove triggers by prefix, used for session/agent cleanup.\n\nArgs:\n    state: Current state dictionary\n    prefix: If provided, only remove triggers with names starting\n           with this prefix. If None, removes all triggers (use with caution).\n\nReturns:\n    {\n        \"success\": True,\n        \"removed\": list of trigger names removed,\n        \"count\": int\n    }",
          "line_number": 420
        }
      ]
    },
    {
      "file": "observability_actions.py",
      "namespace": "observability",
      "actions": [
        {
          "name": "trace.start",
          "function": "trace_start",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "metadata",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "parent_id",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Start a new trace span.\n\nArgs:\n    state: Current state dictionary\n    name: Name of the span (operation being traced)\n    metadata: Optional metadata to attach to the span\n    parent_id: Optional explicit parent span ID. If not provided,\n              auto-parents to the current span.\n\nReturns:\n    {\"span_id\": str, \"name\": str, \"parent_id\": Optional[str], \"success\": True}\n    Or {\"error\": str, \"success\": False} if tracing is disabled",
          "line_number": 45
        },
        {
          "name": "actions.trace_start",
          "function": "trace_start",
          "parameters": [
            {
              "name": "name",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "metadata",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "parent_id",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Start a new trace span.\n\nArgs:\n    state: Current state dictionary\n    name: Name of the span (operation being traced)\n    metadata: Optional metadata to attach to the span\n    parent_id: Optional explicit parent span ID. If not provided,\n              auto-parents to the current span.\n\nReturns:\n    {\"span_id\": str, \"name\": str, \"parent_id\": Optional[str], \"success\": True}\n    Or {\"error\": str, \"success\": False} if tracing is disabled",
          "line_number": 45
        },
        {
          "name": "trace.log",
          "function": "trace_log",
          "parameters": [
            {
              "name": "message",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "event",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "metrics",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "snapshot_state",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "sanitize_keys",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Log an event, metrics, or state snapshot to the current span.\n\nArgs:\n    state: Current state dictionary\n    message: Optional message to log\n    event: Optional event dictionary with additional data\n    metrics: Optional metrics dictionary to merge into span\n    snapshot_state: If True, capture current state in the event\n    sanitize_keys: List of state keys to redact in snapshot\n\nReturns:\n    {\"logged\": True, \"span_id\": str, \"event_count\": int, \"success\": True}\n    Or {\"success\": False, \"error\": str} if no active span or tracing disabled",
          "line_number": 77
        },
        {
          "name": "actions.trace_log",
          "function": "trace_log",
          "parameters": [
            {
              "name": "message",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "event",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "metrics",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "snapshot_state",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "sanitize_keys",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Log an event, metrics, or state snapshot to the current span.\n\nArgs:\n    state: Current state dictionary\n    message: Optional message to log\n    event: Optional event dictionary with additional data\n    metrics: Optional metrics dictionary to merge into span\n    snapshot_state: If True, capture current state in the event\n    sanitize_keys: List of state keys to redact in snapshot\n\nReturns:\n    {\"logged\": True, \"span_id\": str, \"event_count\": int, \"success\": True}\n    Or {\"success\": False, \"error\": str} if no active span or tracing disabled",
          "line_number": 77
        },
        {
          "name": "trace.end",
          "function": "trace_end",
          "parameters": [
            {
              "name": "status",
              "type": "Any",
              "required": false,
              "default": "'ok'"
            },
            {
              "name": "error",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "End the current trace span.\n\nArgs:\n    state: Current state dictionary\n    status: Span status - \"ok\" or \"error\" (default: \"ok\")\n    error: Optional error message if status is \"error\"\n\nReturns:\n    {\"span_id\": str, \"duration_ms\": float, \"status\": str, \"success\": True}\n    Or {\"success\": False, \"error\": str} if no active span or tracing disabled",
          "line_number": 130
        },
        {
          "name": "actions.trace_end",
          "function": "trace_end",
          "parameters": [
            {
              "name": "status",
              "type": "Any",
              "required": false,
              "default": "'ok'"
            },
            {
              "name": "error",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "End the current trace span.\n\nArgs:\n    state: Current state dictionary\n    status: Span status - \"ok\" or \"error\" (default: \"ok\")\n    error: Optional error message if status is \"error\"\n\nReturns:\n    {\"span_id\": str, \"duration_ms\": float, \"status\": str, \"success\": True}\n    Or {\"success\": False, \"error\": str} if no active span or tracing disabled",
          "line_number": 130
        },
        {
          "name": "opik.healthcheck",
          "function": "opik_healthcheck",
          "parameters": [
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "5.0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Validate Opik connectivity and authentication (TEA-BUILTIN-005.3).\n\nTests connectivity to the configured Opik instance and validates\nthat the API key (if required) is valid.\n\nArgs:\n    state: Current state dictionary\n    timeout: Connection timeout in seconds (default: 5.0)\n\nReturns:\n    On success:\n        {\n            \"success\": True,\n            \"latency_ms\": float,\n            \"workspace\": str,\n            \"project\": str,\n            \"message\": \"Connected to Opik successfully\"\n        }\n    On failure:\n        {\n            \"success\": False,\n            \"error\": str,\n            \"message\": str  # User-friendly guidance\n        }\n\nExample:\n    >>> result = registry['opik.healthcheck'](state={})\n    >>> if result['success']:\n    ...     print(f\"Connected in {result['latency_ms']:.1f}ms\")\n    ... else:\n    ...     print(result['message'])",
          "line_number": 162
        },
        {
          "name": "actions.opik_healthcheck",
          "function": "opik_healthcheck",
          "parameters": [
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "5.0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Validate Opik connectivity and authentication (TEA-BUILTIN-005.3).\n\nTests connectivity to the configured Opik instance and validates\nthat the API key (if required) is valid.\n\nArgs:\n    state: Current state dictionary\n    timeout: Connection timeout in seconds (default: 5.0)\n\nReturns:\n    On success:\n        {\n            \"success\": True,\n            \"latency_ms\": float,\n            \"workspace\": str,\n            \"project\": str,\n            \"message\": \"Connected to Opik successfully\"\n        }\n    On failure:\n        {\n            \"success\": False,\n            \"error\": str,\n            \"message\": str  # User-friendly guidance\n        }\n\nExample:\n    >>> result = registry['opik.healthcheck'](state={})\n    >>> if result['success']:\n    ...     print(f\"Connected in {result['latency_ms']:.1f}ms\")\n    ... else:\n    ...     print(result['message'])",
          "line_number": 162
        },
        {
          "name": "obs.get_flow_log",
          "function": "obs_get_flow_log",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Get the complete flow log from ObservabilityContext (TEA-OBS-001.1).\n\nReturns a structured trace containing all events, spans, and metrics\nfor the current flow execution. Requires observability to be enabled\nvia the 'observability' configuration in YAML.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    On success:\n        {\n            \"success\": True,\n            \"flow_log\": {\n                \"flow_id\": str,\n                \"events\": List[dict],\n                \"spans\": List[dict],\n                \"metrics\": {\n                    \"total_duration_ms\": float,\n                    \"node_count\": int,\n                    \"error_count\": int,\n                    \"event_count\": int\n                },\n                \"timeline\": List[dict]\n            }\n        }\n    On failure:\n        {\"success\": False, \"error\": str}\n\nExample:\n    >>> result = registry['obs.get_flow_log'](state={})\n    >>> if result['success']:\n    ...     print(f\"Flow: {result['flow_log']['flow_id']}\")\n    ...     print(f\"Events: {result['flow_log']['metrics']['event_count']}\")",
          "line_number": 282
        },
        {
          "name": "actions.obs_get_flow_log",
          "function": "obs_get_flow_log",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Get the complete flow log from ObservabilityContext (TEA-OBS-001.1).\n\nReturns a structured trace containing all events, spans, and metrics\nfor the current flow execution. Requires observability to be enabled\nvia the 'observability' configuration in YAML.\n\nArgs:\n    state: Current state dictionary\n\nReturns:\n    On success:\n        {\n            \"success\": True,\n            \"flow_log\": {\n                \"flow_id\": str,\n                \"events\": List[dict],\n                \"spans\": List[dict],\n                \"metrics\": {\n                    \"total_duration_ms\": float,\n                    \"node_count\": int,\n                    \"error_count\": int,\n                    \"event_count\": int\n                },\n                \"timeline\": List[dict]\n            }\n        }\n    On failure:\n        {\"success\": False, \"error\": str}\n\nExample:\n    >>> result = registry['obs.get_flow_log'](state={})\n    >>> if result['success']:\n    ...     print(f\"Flow: {result['flow_log']['flow_id']}\")\n    ...     print(f\"Events: {result['flow_log']['metrics']['event_count']}\")",
          "line_number": 282
        },
        {
          "name": "obs.log_event",
          "function": "obs_log_event",
          "parameters": [
            {
              "name": "node",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "level",
              "type": "Any",
              "required": false,
              "default": "'info'"
            },
            {
              "name": "event_type",
              "type": "Any",
              "required": false,
              "default": "'metric'"
            },
            {
              "name": "message",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "data",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "metrics",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Log a custom event to the observability stream (TEA-OBS-001.1).\n\nAllows workflows to emit custom log events that appear in the flow log.\nRequires observability to be enabled.\n\nArgs:\n    state: Current state dictionary\n    node: Node name (optional, defaults to 'custom')\n    level: Log level - 'debug', 'info', 'warn', 'error' (default: 'info')\n    event_type: Event type - 'entry', 'exit', 'error', 'metric' (default: 'metric')\n    message: Optional message text\n    data: Optional data dictionary\n    metrics: Optional metrics dictionary\n\nReturns:\n    {\"success\": True, \"flow_id\": str}\n    or {\"success\": False, \"error\": str}\n\nExample:\n    >>> registry['obs.log_event'](\n    ...     state={},\n    ...     node=\"my_step\",\n    ...     level=\"info\",\n    ...     message=\"Processing complete\",\n    ...     metrics={\"items_processed\": 42}\n    ... )",
          "line_number": 334
        },
        {
          "name": "actions.obs_log_event",
          "function": "obs_log_event",
          "parameters": [
            {
              "name": "node",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "level",
              "type": "Any",
              "required": false,
              "default": "'info'"
            },
            {
              "name": "event_type",
              "type": "Any",
              "required": false,
              "default": "'metric'"
            },
            {
              "name": "message",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "data",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "metrics",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Log a custom event to the observability stream (TEA-OBS-001.1).\n\nAllows workflows to emit custom log events that appear in the flow log.\nRequires observability to be enabled.\n\nArgs:\n    state: Current state dictionary\n    node: Node name (optional, defaults to 'custom')\n    level: Log level - 'debug', 'info', 'warn', 'error' (default: 'info')\n    event_type: Event type - 'entry', 'exit', 'error', 'metric' (default: 'metric')\n    message: Optional message text\n    data: Optional data dictionary\n    metrics: Optional metrics dictionary\n\nReturns:\n    {\"success\": True, \"flow_id\": str}\n    or {\"success\": False, \"error\": str}\n\nExample:\n    >>> registry['obs.log_event'](\n    ...     state={},\n    ...     node=\"my_step\",\n    ...     level=\"info\",\n    ...     message=\"Processing complete\",\n    ...     metrics={\"items_processed\": 42}\n    ... )",
          "line_number": 334
        },
        {
          "name": "obs.query_events",
          "function": "obs_query_events",
          "parameters": [
            {
              "name": "filters",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Query events from the observability stream (TEA-OBS-001.1).\n\nFilter events by node pattern, level, event type, or time range.\nRequires observability to be enabled.\n\nArgs:\n    state: Current state dictionary\n    filters: Dictionary of filters:\n        - node: Glob pattern to match node name (e.g., \"llm.*\")\n        - level: Exact level match (debug, info, warn, error)\n        - event_type: Exact type match (entry, exit, error, metric)\n        - start_time: Minimum timestamp (Unix float)\n        - end_time: Maximum timestamp (Unix float)\n\nReturns:\n    {\"success\": True, \"events\": List[dict], \"count\": int}\n    or {\"success\": False, \"error\": str}\n\nExample:\n    >>> result = registry['obs.query_events'](\n    ...     state={},\n    ...     filters={\"node\": \"llm.*\", \"level\": \"error\"}\n    ... )\n    >>> print(f\"Found {result['count']} error events\")",
          "line_number": 396
        },
        {
          "name": "actions.obs_query_events",
          "function": "obs_query_events",
          "parameters": [
            {
              "name": "filters",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Query events from the observability stream (TEA-OBS-001.1).\n\nFilter events by node pattern, level, event type, or time range.\nRequires observability to be enabled.\n\nArgs:\n    state: Current state dictionary\n    filters: Dictionary of filters:\n        - node: Glob pattern to match node name (e.g., \"llm.*\")\n        - level: Exact level match (debug, info, warn, error)\n        - event_type: Exact type match (entry, exit, error, metric)\n        - start_time: Minimum timestamp (Unix float)\n        - end_time: Maximum timestamp (Unix float)\n\nReturns:\n    {\"success\": True, \"events\": List[dict], \"count\": int}\n    or {\"success\": False, \"error\": str}\n\nExample:\n    >>> result = registry['obs.query_events'](\n    ...     state={},\n    ...     filters={\"node\": \"llm.*\", \"level\": \"error\"}\n    ... )\n    >>> print(f\"Found {result['count']} error events\")",
          "line_number": 396
        }
      ]
    },
    {
      "file": "planning_actions.py",
      "namespace": "planning",
      "actions": [
        {
          "name": "plan.decompose",
          "function": "plan_decompose",
          "parameters": [
            {
              "name": "goal",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "strategy",
              "type": "str",
              "required": false,
              "default": "'flat'"
            },
            {
              "name": "max_depth",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "max_subtasks",
              "type": "int",
              "required": false,
              "default": "15"
            },
            {
              "name": "prompt_template",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Decompose a goal into subtasks using LLM.\n\nUses LLM to decompose goal into subtasks with dependencies,\nvalidates the plan structure, and returns a Plan object.\n\nArgs:\n    state: Current state dictionary\n    goal: The goal to decompose\n    model: LLM model to use (default: gpt-4)\n    strategy: Decomposition strategy - flat, hierarchical, iterative\n    max_depth: Maximum depth for hierarchical plans (default: 3)\n    max_subtasks: Maximum number of subtasks (default: 15)\n    prompt_template: Optional custom prompt template\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"plan\": dict,              # Plan structure with subtasks\n        \"planning_trace\": list,    # Trace for observability\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 447
        },
        {
          "name": "actions.plan_decompose",
          "function": "plan_decompose",
          "parameters": [
            {
              "name": "goal",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "strategy",
              "type": "str",
              "required": false,
              "default": "'flat'"
            },
            {
              "name": "max_depth",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "max_subtasks",
              "type": "int",
              "required": false,
              "default": "15"
            },
            {
              "name": "prompt_template",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Decompose a goal into subtasks using LLM.\n\nUses LLM to decompose goal into subtasks with dependencies,\nvalidates the plan structure, and returns a Plan object.\n\nArgs:\n    state: Current state dictionary\n    goal: The goal to decompose\n    model: LLM model to use (default: gpt-4)\n    strategy: Decomposition strategy - flat, hierarchical, iterative\n    max_depth: Maximum depth for hierarchical plans (default: 3)\n    max_subtasks: Maximum number of subtasks (default: 15)\n    prompt_template: Optional custom prompt template\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"plan\": dict,              # Plan structure with subtasks\n        \"planning_trace\": list,    # Trace for observability\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 447
        },
        {
          "name": "plan.execute",
          "function": "plan_execute",
          "parameters": [
            {
              "name": "plan",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "parallel",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "max_concurrent",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "subtask_executor",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "on_subtask_failure",
              "type": "str",
              "required": false,
              "default": "'abort'"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "retry_delay",
              "type": "float",
              "required": false,
              "default": "1.0"
            },
            {
              "name": "max_replans",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute plan subtasks respecting dependency order.\n\nArgs:\n    state: Current state dictionary (should contain 'plan' if not provided)\n    plan: Plan dict to execute (optional, uses state['plan'] if not provided)\n    parallel: Execute independent subtasks in parallel (default: False)\n    max_concurrent: Max concurrent subtasks when parallel=True (default: 3)\n    subtask_executor: Action config for executing subtasks\n    on_subtask_failure: Failure strategy - replan, retry, skip, abort\n    max_retries: Max retries per subtask (for retry strategy)\n    retry_delay: Delay between retries in seconds\n    max_replans: Max replan attempts (for replan strategy)\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"plan\": dict,              # Updated plan with results\n        \"subtask_results\": dict,   # Map of subtask_id -> result\n        \"plan_status\": dict,       # Status counts\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 704
        },
        {
          "name": "actions.plan_execute",
          "function": "plan_execute",
          "parameters": [
            {
              "name": "plan",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "parallel",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "max_concurrent",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "subtask_executor",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "on_subtask_failure",
              "type": "str",
              "required": false,
              "default": "'abort'"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "retry_delay",
              "type": "float",
              "required": false,
              "default": "1.0"
            },
            {
              "name": "max_replans",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute plan subtasks respecting dependency order.\n\nArgs:\n    state: Current state dictionary (should contain 'plan' if not provided)\n    plan: Plan dict to execute (optional, uses state['plan'] if not provided)\n    parallel: Execute independent subtasks in parallel (default: False)\n    max_concurrent: Max concurrent subtasks when parallel=True (default: 3)\n    subtask_executor: Action config for executing subtasks\n    on_subtask_failure: Failure strategy - replan, retry, skip, abort\n    max_retries: Max retries per subtask (for retry strategy)\n    retry_delay: Delay between retries in seconds\n    max_replans: Max replan attempts (for replan strategy)\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"plan\": dict,              # Updated plan with results\n        \"subtask_results\": dict,   # Map of subtask_id -> result\n        \"plan_status\": dict,       # Status counts\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 704
        },
        {
          "name": "plan.replan",
          "function": "plan_replan",
          "parameters": [
            {
              "name": "plan",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Re-plan from current state, preserving completed subtasks.\n\nTriggers re-planning while keeping completed work. Adjusts the\nremaining plan based on current context and any failures.\n\nArgs:\n    state: Current state (should contain plan, subtask_results)\n    plan: Plan dict to replan (optional, uses state['plan'])\n    model: LLM model for planning (default: gpt-4)\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"plan\": dict,              # New plan with adjusted subtasks\n        \"preserved_subtasks\": int, # Count of preserved completed subtasks\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 1165
        },
        {
          "name": "actions.plan_replan",
          "function": "plan_replan",
          "parameters": [
            {
              "name": "plan",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Re-plan from current state, preserving completed subtasks.\n\nTriggers re-planning while keeping completed work. Adjusts the\nremaining plan based on current context and any failures.\n\nArgs:\n    state: Current state (should contain plan, subtask_results)\n    plan: Plan dict to replan (optional, uses state['plan'])\n    model: LLM model for planning (default: gpt-4)\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"plan\": dict,              # New plan with adjusted subtasks\n        \"preserved_subtasks\": int, # Count of preserved completed subtasks\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 1165
        },
        {
          "name": "plan.status",
          "function": "plan_status",
          "parameters": [
            {
              "name": "plan",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "include_completed",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "include_details",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get current plan execution status.\n\nReturns aggregated status counts and optionally detailed subtask info.\n\nArgs:\n    state: Current state (should contain 'plan')\n    plan: Plan dict (optional, uses state['plan'])\n    include_completed: Include completed subtasks in response\n    include_details: Include full subtask details\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"status\": dict,           # Status counts\n        \"progress\": float,        # 0.0 to 1.0\n        \"subtasks\": list,         # Optional: subtask details\n        \"success\": True\n    }",
          "line_number": 1360
        },
        {
          "name": "actions.plan_status",
          "function": "plan_status",
          "parameters": [
            {
              "name": "plan",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "include_completed",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "include_details",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get current plan execution status.\n\nReturns aggregated status counts and optionally detailed subtask info.\n\nArgs:\n    state: Current state (should contain 'plan')\n    plan: Plan dict (optional, uses state['plan'])\n    include_completed: Include completed subtasks in response\n    include_details: Include full subtask details\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"status\": dict,           # Status counts\n        \"progress\": float,        # 0.0 to 1.0\n        \"subtasks\": list,         # Optional: subtask details\n        \"success\": True\n    }",
          "line_number": 1360
        }
      ]
    },
    {
      "file": "rag_actions.py",
      "namespace": "rag",
      "actions": [
        {
          "name": "embedding.create",
          "function": "embedding_create",
          "parameters": [
            {
              "name": "text",
              "type": "Union[str, List[str]]",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "batch",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create embeddings from text.\n\nArgs:\n    state: Current state dictionary\n    text: Text or list of texts to embed\n    model: Model name (optional, uses config default)\n    batch: If True, always return batch format\n    **kwargs: Additional provider configuration\n\nReturns:\n    Single text: {\"embedding\": List[float], \"model\": str, \"dimensions\": int}\n    Batch: {\"embeddings\": List[List[float]], \"model\": str, \"count\": int, \"dimensions\": int}\n    Error: {\"error\": str, \"success\": False}",
          "line_number": 1180
        },
        {
          "name": "actions.embedding_create",
          "function": "embedding_create",
          "parameters": [
            {
              "name": "text",
              "type": "Union[str, List[str]]",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "batch",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create embeddings from text.\n\nArgs:\n    state: Current state dictionary\n    text: Text or list of texts to embed\n    model: Model name (optional, uses config default)\n    batch: If True, always return batch format\n    **kwargs: Additional provider configuration\n\nReturns:\n    Single text: {\"embedding\": List[float], \"model\": str, \"dimensions\": int}\n    Batch: {\"embeddings\": List[List[float]], \"model\": str, \"count\": int, \"dimensions\": int}\n    Error: {\"error\": str, \"success\": False}",
          "line_number": 1180
        },
        {
          "name": "vector.store",
          "function": "vector_store",
          "parameters": [
            {
              "name": "texts",
              "type": "Union[str, List[str]]",
              "required": true,
              "default": null
            },
            {
              "name": "embeddings",
              "type": "Optional[List[List[float]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ids",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "collection",
              "type": "str",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Store documents with embeddings in vector store.\n\nArgs:\n    state: Current state dictionary\n    texts: Text or list of texts to store\n    embeddings: Pre-computed embeddings (auto-generated if not provided)\n    ids: Document IDs (auto-generated UUIDs if not provided)\n    metadata: Metadata dict or list of dicts\n    collection: Collection name (default: \"default\")\n    **kwargs: Additional configuration\n\nReturns:\n    {\"stored\": int, \"collection\": str, \"ids\": List[str]}\n    Error: {\"error\": str, \"success\": False}",
          "line_number": 1238
        },
        {
          "name": "actions.vector_store",
          "function": "vector_store",
          "parameters": [
            {
              "name": "texts",
              "type": "Union[str, List[str]]",
              "required": true,
              "default": null
            },
            {
              "name": "embeddings",
              "type": "Optional[List[List[float]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ids",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "metadata",
              "type": "Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "collection",
              "type": "str",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Store documents with embeddings in vector store.\n\nArgs:\n    state: Current state dictionary\n    texts: Text or list of texts to store\n    embeddings: Pre-computed embeddings (auto-generated if not provided)\n    ids: Document IDs (auto-generated UUIDs if not provided)\n    metadata: Metadata dict or list of dicts\n    collection: Collection name (default: \"default\")\n    **kwargs: Additional configuration\n\nReturns:\n    {\"stored\": int, \"collection\": str, \"ids\": List[str]}\n    Error: {\"error\": str, \"success\": False}",
          "line_number": 1238
        },
        {
          "name": "vector.query",
          "function": "vector_query",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "k",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "collection",
              "type": "str",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "filter",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "include_embeddings",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Query vector store for similar documents.\n\nArgs:\n    state: Current state dictionary\n    query: Query text\n    k: Number of results to return (default: 5)\n    collection: Collection to query (default: \"default\")\n    filter: Metadata filter conditions\n    include_embeddings: Include embeddings in results\n    **kwargs: Additional configuration\n\nReturns:\n    {\n        \"results\": [{\"id\": str, \"text\": str, \"score\": float, \"metadata\": dict}],\n        \"query\": str,\n        \"collection\": str,\n        \"k\": int\n    }\n    Error: {\"error\": str, \"success\": False}",
          "line_number": 1318
        },
        {
          "name": "actions.vector_query",
          "function": "vector_query",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "k",
              "type": "int",
              "required": false,
              "default": "5"
            },
            {
              "name": "collection",
              "type": "str",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "filter",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "include_embeddings",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Query vector store for similar documents.\n\nArgs:\n    state: Current state dictionary\n    query: Query text\n    k: Number of results to return (default: 5)\n    collection: Collection to query (default: \"default\")\n    filter: Metadata filter conditions\n    include_embeddings: Include embeddings in results\n    **kwargs: Additional configuration\n\nReturns:\n    {\n        \"results\": [{\"id\": str, \"text\": str, \"score\": float, \"metadata\": dict}],\n        \"query\": str,\n        \"collection\": str,\n        \"k\": int\n    }\n    Error: {\"error\": str, \"success\": False}",
          "line_number": 1318
        },
        {
          "name": "vector.index_files",
          "function": "vector_index_files",
          "parameters": [
            {
              "name": "paths",
              "type": "Union[str, List[str]]",
              "required": true,
              "default": null
            },
            {
              "name": "pattern",
              "type": "str",
              "required": false,
              "default": "'**/*'"
            },
            {
              "name": "chunk_by",
              "type": "str",
              "required": false,
              "default": "'line'"
            },
            {
              "name": "collection",
              "type": "str",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "recursive",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "extensions",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "incremental",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Index files/directories into vector store (AC: 1-13).\n\nReads files from local or cloud storage, chunks them based on\nstrategy, generates embeddings, and stores in vector store.\n\nArgs:\n    state: Current state dictionary\n    paths: File/directory paths (local or fsspec URIs like s3://, gs://)\n    pattern: Glob pattern for file filtering (default: \"**/*\")\n    chunk_by: Chunking strategy - \"line\", \"paragraph\", or \"document\"\n    collection: Vector store collection name\n    recursive: Whether to traverse directories recursively\n    extensions: File extensions to filter (e.g., [\".py\", \".md\"])\n    incremental: Skip unchanged files (default: True)\n    **kwargs: Additional embedding/store configuration\n\nReturns:\n    {\n        \"success\": True,\n        \"indexed\": int,\n        \"skipped\": int,\n        \"errors\": List[str],\n        \"collection\": str,\n        \"files\": int\n    }\n    Error: {\"success\": False, \"error\": str}\n\nExample YAML:\n    - name: index_codebase\n      uses: vector.index_files\n      with:\n        paths:\n          - src/\n          - docs/\n        pattern: \"**/*.py\"\n        chunk_by: line\n        collection: codebase\n        extensions: [\".py\", \".md\"]\n      output: index_result",
          "line_number": 1659
        },
        {
          "name": "actions.vector_index_files",
          "function": "vector_index_files",
          "parameters": [
            {
              "name": "paths",
              "type": "Union[str, List[str]]",
              "required": true,
              "default": null
            },
            {
              "name": "pattern",
              "type": "str",
              "required": false,
              "default": "'**/*'"
            },
            {
              "name": "chunk_by",
              "type": "str",
              "required": false,
              "default": "'line'"
            },
            {
              "name": "collection",
              "type": "str",
              "required": false,
              "default": "'default'"
            },
            {
              "name": "recursive",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "extensions",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "incremental",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Index files/directories into vector store (AC: 1-13).\n\nReads files from local or cloud storage, chunks them based on\nstrategy, generates embeddings, and stores in vector store.\n\nArgs:\n    state: Current state dictionary\n    paths: File/directory paths (local or fsspec URIs like s3://, gs://)\n    pattern: Glob pattern for file filtering (default: \"**/*\")\n    chunk_by: Chunking strategy - \"line\", \"paragraph\", or \"document\"\n    collection: Vector store collection name\n    recursive: Whether to traverse directories recursively\n    extensions: File extensions to filter (e.g., [\".py\", \".md\"])\n    incremental: Skip unchanged files (default: True)\n    **kwargs: Additional embedding/store configuration\n\nReturns:\n    {\n        \"success\": True,\n        \"indexed\": int,\n        \"skipped\": int,\n        \"errors\": List[str],\n        \"collection\": str,\n        \"files\": int\n    }\n    Error: {\"success\": False, \"error\": str}\n\nExample YAML:\n    - name: index_codebase\n      uses: vector.index_files\n      with:\n        paths:\n          - src/\n          - docs/\n        pattern: \"**/*.py\"\n        chunk_by: line\n        collection: codebase\n        extensions: [\".py\", \".md\"]\n      output: index_result",
          "line_number": 1659
        }
      ]
    },
    {
      "file": "ratelimit_actions.py",
      "namespace": "ratelimit",
      "actions": [
        {
          "name": "ratelimit.wrap",
          "function": "ratelimit_wrap",
          "parameters": [
            {
              "name": "action",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "args",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "limiter",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "rpm",
              "type": "Optional[float]",
              "required": false,
              "default": "None"
            },
            {
              "name": "rps",
              "type": "Optional[float]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Optional[float]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Wrap any action with rate limiting.\n\nWaits if necessary before executing the wrapped action to ensure\nthe rate limit is respected. Rate limiters are shared across all\nnodes using the same limiter name.\n\nArgs:\n    state: Current state dictionary\n    action: The action to wrap (e.g., 'llm.call', 'http.get')\n    args: Arguments to pass to the wrapped action\n    limiter: Name of the rate limiter (shared across parallel nodes)\n    rpm: Requests per minute limit\n    rps: Requests per second limit (takes precedence over rpm)\n    timeout: Maximum time to wait in seconds. If exceeded, returns error.\n\nReturns:\n    Action result with rate limit metadata:\n    - success: bool\n    - result: The wrapped action's result (if successful)\n    - _ratelimit_waited_ms: float - Time spent waiting in milliseconds\n    - _ratelimit_limiter: str - Name of the limiter used\n    - error: str - Error message if failed\n    - error_type: str - \"ratelimit_timeout\" or \"action_error\"",
          "line_number": 242
        },
        {
          "name": "actions.ratelimit_wrap",
          "function": "ratelimit_wrap",
          "parameters": [
            {
              "name": "action",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "args",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "limiter",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "rpm",
              "type": "Optional[float]",
              "required": false,
              "default": "None"
            },
            {
              "name": "rps",
              "type": "Optional[float]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "Optional[float]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Wrap any action with rate limiting.\n\nWaits if necessary before executing the wrapped action to ensure\nthe rate limit is respected. Rate limiters are shared across all\nnodes using the same limiter name.\n\nArgs:\n    state: Current state dictionary\n    action: The action to wrap (e.g., 'llm.call', 'http.get')\n    args: Arguments to pass to the wrapped action\n    limiter: Name of the rate limiter (shared across parallel nodes)\n    rpm: Requests per minute limit\n    rps: Requests per second limit (takes precedence over rpm)\n    timeout: Maximum time to wait in seconds. If exceeded, returns error.\n\nReturns:\n    Action result with rate limit metadata:\n    - success: bool\n    - result: The wrapped action's result (if successful)\n    - _ratelimit_waited_ms: float - Time spent waiting in milliseconds\n    - _ratelimit_limiter: str - Name of the limiter used\n    - error: str - Error message if failed\n    - error_type: str - \"ratelimit_timeout\" or \"action_error\"",
          "line_number": 242
        }
      ]
    },
    {
      "file": "reasoning_actions.py",
      "namespace": "reasoning",
      "actions": [
        {
          "name": "reason.cot",
          "function": "reason_cot",
          "parameters": [
            {
              "name": "problem",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "thinking_format",
              "type": "str",
              "required": false,
              "default": "'step_by_step'"
            },
            {
              "name": "few_shot_examples",
              "type": "Optional[List[Dict]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chain-of-Thought reasoning action.\n\nWraps an LLM call with CoT prompting to produce structured output\nwith explicit thinking steps and a final answer.\n\nArgs:\n    state: Current state dictionary\n    problem: The problem or question to reason about\n    model: LLM model to use (default: gpt-4)\n    thinking_format: Reasoning format - step_by_step, pros_cons, tree, first_principles\n    few_shot_examples: Optional list of example dicts with problem/thinking/answer\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"thinking\": str,           # Chain-of-thought reasoning\n        \"answer\": any,             # Final answer\n        \"reasoning_trace\": list,   # Full trace for observability\n        \"model\": str,\n        \"thinking_format\": str\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 259
        },
        {
          "name": "actions.reason_cot",
          "function": "reason_cot",
          "parameters": [
            {
              "name": "problem",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "thinking_format",
              "type": "str",
              "required": false,
              "default": "'step_by_step'"
            },
            {
              "name": "few_shot_examples",
              "type": "Optional[List[Dict]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chain-of-Thought reasoning action.\n\nWraps an LLM call with CoT prompting to produce structured output\nwith explicit thinking steps and a final answer.\n\nArgs:\n    state: Current state dictionary\n    problem: The problem or question to reason about\n    model: LLM model to use (default: gpt-4)\n    thinking_format: Reasoning format - step_by_step, pros_cons, tree, first_principles\n    few_shot_examples: Optional list of example dicts with problem/thinking/answer\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"thinking\": str,           # Chain-of-thought reasoning\n        \"answer\": any,             # Final answer\n        \"reasoning_trace\": list,   # Full trace for observability\n        \"model\": str,\n        \"thinking_format\": str\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 259
        },
        {
          "name": "reason.react",
          "function": "reason_react",
          "parameters": [
            {
              "name": "goal",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "tools",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_steps",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "ReAct (Reason-Act) reasoning action.\n\nImplements the Thought -> Action -> Observation loop for\ngoal-directed reasoning with tool use.\n\nArgs:\n    state: Current state dictionary\n    goal: The goal or question to achieve/answer\n    model: LLM model to use (default: gpt-4)\n    tools: List of tool/action names to make available\n    max_steps: Maximum reasoning steps (default: 10)\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"steps\": list,             # Thought-action-observation steps\n        \"final_answer\": any,       # Final answer when goal achieved\n        \"reasoning_trace\": list,   # Full trace for observability\n        \"react_steps\": list,       # Alias for steps (state variable)\n        \"model\": str,\n        \"total_steps\": int\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 380
        },
        {
          "name": "actions.reason_react",
          "function": "reason_react",
          "parameters": [
            {
              "name": "goal",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "tools",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_steps",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "ReAct (Reason-Act) reasoning action.\n\nImplements the Thought -> Action -> Observation loop for\ngoal-directed reasoning with tool use.\n\nArgs:\n    state: Current state dictionary\n    goal: The goal or question to achieve/answer\n    model: LLM model to use (default: gpt-4)\n    tools: List of tool/action names to make available\n    max_steps: Maximum reasoning steps (default: 10)\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"steps\": list,             # Thought-action-observation steps\n        \"final_answer\": any,       # Final answer when goal achieved\n        \"reasoning_trace\": list,   # Full trace for observability\n        \"react_steps\": list,       # Alias for steps (state variable)\n        \"model\": str,\n        \"total_steps\": int\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 380
        },
        {
          "name": "reason.self_correct",
          "function": "reason_self_correct",
          "parameters": [
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "generator_model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "critic_model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "improvement_rounds",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "critic_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Self-correction reasoning action.\n\nImplements generate -> critique -> improve cycle for iterative\nrefinement of outputs.\n\nArgs:\n    state: Current state dictionary\n    task: The task description\n    model: Default LLM model to use\n    generator_model: Model for generation (default: same as model)\n    critic_model: Model for critique (default: same as model)\n    improvement_rounds: Number of improvement iterations (default: 2)\n    critic_prompt: Custom prompt for the critic\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"output\": any,                 # Final improved output\n        \"improvement_history\": list,   # History of improvements\n        \"reasoning_trace\": list,       # Full trace for observability\n        \"model\": str,\n        \"rounds_completed\": int\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 637
        },
        {
          "name": "actions.reason_self_correct",
          "function": "reason_self_correct",
          "parameters": [
            {
              "name": "task",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "generator_model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "critic_model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "improvement_rounds",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "critic_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Self-correction reasoning action.\n\nImplements generate -> critique -> improve cycle for iterative\nrefinement of outputs.\n\nArgs:\n    state: Current state dictionary\n    task: The task description\n    model: Default LLM model to use\n    generator_model: Model for generation (default: same as model)\n    critic_model: Model for critique (default: same as model)\n    improvement_rounds: Number of improvement iterations (default: 2)\n    critic_prompt: Custom prompt for the critic\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"output\": any,                 # Final improved output\n        \"improvement_history\": list,   # History of improvements\n        \"reasoning_trace\": list,       # Full trace for observability\n        \"model\": str,\n        \"rounds_completed\": int\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 637
        },
        {
          "name": "reason.decompose",
          "function": "reason_decompose",
          "parameters": [
            {
              "name": "problem",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "max_depth",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "synthesis_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Problem decomposition reasoning action.\n\nBreaks complex problems into sub-problems, solves each independently,\nand synthesizes a final answer.\n\nArgs:\n    state: Current state dictionary\n    problem: The complex problem to decompose\n    model: LLM model to use (default: gpt-4)\n    max_depth: Maximum recursion depth for sub-decomposition (default: 2)\n    synthesis_prompt: Custom prompt for answer synthesis\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"sub_problems\": list,      # Decomposed sub-problems\n        \"sub_answers\": list,       # Answers to sub-problems\n        \"final_answer\": any,       # Synthesized final answer\n        \"reasoning_trace\": list,   # Full trace for observability\n        \"model\": str,\n        \"depth_used\": int\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 857
        },
        {
          "name": "actions.reason_decompose",
          "function": "reason_decompose",
          "parameters": [
            {
              "name": "problem",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "max_depth",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "synthesis_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Problem decomposition reasoning action.\n\nBreaks complex problems into sub-problems, solves each independently,\nand synthesizes a final answer.\n\nArgs:\n    state: Current state dictionary\n    problem: The complex problem to decompose\n    model: LLM model to use (default: gpt-4)\n    max_depth: Maximum recursion depth for sub-decomposition (default: 2)\n    synthesis_prompt: Custom prompt for answer synthesis\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional LLM parameters\n\nReturns:\n    {\n        \"sub_problems\": list,      # Decomposed sub-problems\n        \"sub_answers\": list,       # Answers to sub-problems\n        \"final_answer\": any,       # Synthesized final answer\n        \"reasoning_trace\": list,   # Full trace for observability\n        \"model\": str,\n        \"depth_used\": int\n    }\n    Or {\"error\": str, \"success\": False} on failure",
          "line_number": 857
        },
        {
          "name": "reason.dspy.cot",
          "function": "reason_dspy_cot",
          "parameters": [
            {
              "name": "problem",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "signature",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chain-of-Thought using DSPy ChainOfThought module.\n\nWraps DSPy's ChainOfThought for model-agnostic CoT prompting with\ncompiled/optimized prompts. Falls back to native reason.cot when\nDSPy is unavailable.\n\nArgs:\n    state: Current state dictionary\n    problem: The problem to solve\n    model: LLM model to use (default: gpt-4)\n    signature: DSPy signature string (default: \"question -> answer\")\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"thinking\": str,           # Chain-of-thought reasoning\n        \"answer\": any,             # Final answer\n        \"reasoning_trace\": list,   # Trace for observability\n        \"dspy_module\": str,        # \"ChainOfThought\" or \"native\"\n    }\n    Or falls back to native reason.cot if DSPy unavailable",
          "line_number": 1130
        },
        {
          "name": "actions.reason_dspy_cot",
          "function": "reason_dspy_cot",
          "parameters": [
            {
              "name": "problem",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "signature",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Chain-of-Thought using DSPy ChainOfThought module.\n\nWraps DSPy's ChainOfThought for model-agnostic CoT prompting with\ncompiled/optimized prompts. Falls back to native reason.cot when\nDSPy is unavailable.\n\nArgs:\n    state: Current state dictionary\n    problem: The problem to solve\n    model: LLM model to use (default: gpt-4)\n    signature: DSPy signature string (default: \"question -> answer\")\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"thinking\": str,           # Chain-of-thought reasoning\n        \"answer\": any,             # Final answer\n        \"reasoning_trace\": list,   # Trace for observability\n        \"dspy_module\": str,        # \"ChainOfThought\" or \"native\"\n    }\n    Or falls back to native reason.cot if DSPy unavailable",
          "line_number": 1130
        },
        {
          "name": "reason.dspy.react",
          "function": "reason_dspy_react",
          "parameters": [
            {
              "name": "goal",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "tools",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_steps",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "signature",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "ReAct using DSPy ReAct module with tool bridge.\n\nWraps DSPy's ReAct for tool-using agents with compiled prompts.\nFalls back to native reason.react when DSPy is unavailable.\n\nArgs:\n    state: Current state dictionary\n    goal: The goal to achieve\n    model: LLM model to use (default: gpt-4)\n    tools: List of tool/action names to make available\n    max_steps: Maximum reasoning steps (default: 10)\n    signature: DSPy signature string\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional parameters\n\nReturns:\n    ReAct result with steps and final_answer, or falls back to native",
          "line_number": 1249
        },
        {
          "name": "actions.reason_dspy_react",
          "function": "reason_dspy_react",
          "parameters": [
            {
              "name": "goal",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "tools",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_steps",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "signature",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "temperature",
              "type": "float",
              "required": false,
              "default": "0.7"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "ReAct using DSPy ReAct module with tool bridge.\n\nWraps DSPy's ReAct for tool-using agents with compiled prompts.\nFalls back to native reason.react when DSPy is unavailable.\n\nArgs:\n    state: Current state dictionary\n    goal: The goal to achieve\n    model: LLM model to use (default: gpt-4)\n    tools: List of tool/action names to make available\n    max_steps: Maximum reasoning steps (default: 10)\n    signature: DSPy signature string\n    temperature: LLM temperature (default: 0.7)\n    **kwargs: Additional parameters\n\nReturns:\n    ReAct result with steps and final_answer, or falls back to native",
          "line_number": 1249
        },
        {
          "name": "reason.dspy.compile",
          "function": "reason_dspy_compile",
          "parameters": [
            {
              "name": "module_type",
              "type": "str",
              "required": false,
              "default": "'cot'"
            },
            {
              "name": "signature",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "training_data",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "teleprompter",
              "type": "str",
              "required": false,
              "default": "'BootstrapFewShot'"
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "metric",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compile DSPy module with teleprompter for optimized prompts.\n\nUses DSPy's compilation capabilities to optimize prompts based on\ntraining examples. Compiled prompts can be persisted and reused.\n\nArgs:\n    state: Current state dictionary\n    module_type: Type of module to compile (\"cot\", \"react\")\n    signature: DSPy signature string\n    training_data: List of example dicts for optimization\n    teleprompter: Teleprompter to use (BootstrapFewShot, BootstrapFewShotWithRandomSearch)\n    model: Model for compilation (default: gpt-4)\n    metric: Metric function name or \"exact_match\"\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"compiled\": True,\n        \"module_type\": str,\n        \"teleprompter\": str,\n        \"training_examples\": int,\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} if DSPy unavailable",
          "line_number": 1402
        },
        {
          "name": "actions.reason_dspy_compile",
          "function": "reason_dspy_compile",
          "parameters": [
            {
              "name": "module_type",
              "type": "str",
              "required": false,
              "default": "'cot'"
            },
            {
              "name": "signature",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "training_data",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "teleprompter",
              "type": "str",
              "required": false,
              "default": "'BootstrapFewShot'"
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'gpt-4'"
            },
            {
              "name": "metric",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compile DSPy module with teleprompter for optimized prompts.\n\nUses DSPy's compilation capabilities to optimize prompts based on\ntraining examples. Compiled prompts can be persisted and reused.\n\nArgs:\n    state: Current state dictionary\n    module_type: Type of module to compile (\"cot\", \"react\")\n    signature: DSPy signature string\n    training_data: List of example dicts for optimization\n    teleprompter: Teleprompter to use (BootstrapFewShot, BootstrapFewShotWithRandomSearch)\n    model: Model for compilation (default: gpt-4)\n    metric: Metric function name or \"exact_match\"\n    **kwargs: Additional parameters\n\nReturns:\n    {\n        \"compiled\": True,\n        \"module_type\": str,\n        \"teleprompter\": str,\n        \"training_examples\": int,\n        \"success\": True\n    }\n    Or {\"error\": str, \"success\": False} if DSPy unavailable",
          "line_number": 1402
        }
      ]
    },
    {
      "file": "reflection_actions.py",
      "namespace": "reflection",
      "actions": [
        {
          "name": "reflection.loop",
          "function": "reflection_loop_action",
          "parameters": [
            {
              "name": "generator",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "evaluator",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "corrector",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_iterations",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "on_failure",
              "type": "str",
              "required": false,
              "default": "'return_best'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute a generateevaluatecorrect loop (AC: 1, 5, 6).\n\nThis action implements the reflection loop pattern:\n1. Generate output using generator config\n2. Evaluate output using evaluator config\n3. If valid, return success\n4. If invalid and iterations remaining, correct and repeat\n5. On exhaustion, apply on_failure strategy\n\nArgs:\n    state: Current workflow state\n    generator: Generator configuration\n        - action: Action name to call (e.g., \"llm.call\")\n        - run: Inline Python code (alternative to action)\n        - Additional params passed to the action\n    evaluator: Evaluator configuration\n        - type: \"schema\" | \"llm\" | \"custom\"\n        - For schema: schema dict or $ref path\n        - For llm: prompt, model, examples\n        - For custom: run code, language\n    corrector: Corrector configuration (optional, same format as generator)\n    max_iterations: Maximum iterations before giving up (default: 3)\n    on_failure: Strategy when max_iterations reached\n        - \"return_best\": Return highest-scoring attempt\n        - \"return_last\": Return final attempt\n        - \"raise\": Raise ReflectionFailedError\n\nReturns:\n    Dict with:\n        - reflection_iteration: Final iteration count\n        - reflection_output: Final output\n        - reflection_errors: Errors from final evaluation (empty if valid)\n        - reflection_history: All attempts\n        - reflection_best: Best output seen\n        - reflection_best_score: Score of best output\n        - success: True if valid output produced\n\nRaises:\n    ReflectionFailedError: If on_failure=\"raise\" and max_iterations exhausted\n    ValueError: If configuration is invalid",
          "line_number": 645
        },
        {
          "name": "actions.reflection_loop",
          "function": "reflection_loop_action",
          "parameters": [
            {
              "name": "generator",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "evaluator",
              "type": "Dict[str, Any]",
              "required": true,
              "default": null
            },
            {
              "name": "corrector",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "max_iterations",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "on_failure",
              "type": "str",
              "required": false,
              "default": "'return_best'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute a generateevaluatecorrect loop (AC: 1, 5, 6).\n\nThis action implements the reflection loop pattern:\n1. Generate output using generator config\n2. Evaluate output using evaluator config\n3. If valid, return success\n4. If invalid and iterations remaining, correct and repeat\n5. On exhaustion, apply on_failure strategy\n\nArgs:\n    state: Current workflow state\n    generator: Generator configuration\n        - action: Action name to call (e.g., \"llm.call\")\n        - run: Inline Python code (alternative to action)\n        - Additional params passed to the action\n    evaluator: Evaluator configuration\n        - type: \"schema\" | \"llm\" | \"custom\"\n        - For schema: schema dict or $ref path\n        - For llm: prompt, model, examples\n        - For custom: run code, language\n    corrector: Corrector configuration (optional, same format as generator)\n    max_iterations: Maximum iterations before giving up (default: 3)\n    on_failure: Strategy when max_iterations reached\n        - \"return_best\": Return highest-scoring attempt\n        - \"return_last\": Return final attempt\n        - \"raise\": Raise ReflectionFailedError\n\nReturns:\n    Dict with:\n        - reflection_iteration: Final iteration count\n        - reflection_output: Final output\n        - reflection_errors: Errors from final evaluation (empty if valid)\n        - reflection_history: All attempts\n        - reflection_best: Best output seen\n        - reflection_best_score: Score of best output\n        - success: True if valid output produced\n\nRaises:\n    ReflectionFailedError: If on_failure=\"raise\" and max_iterations exhausted\n    ValueError: If configuration is invalid",
          "line_number": 645
        },
        {
          "name": "reflection.evaluate",
          "function": "reflection_evaluate_action",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "evaluator_type",
              "type": "str",
              "required": false,
              "default": "'schema'"
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "examples",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "run",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "language",
              "type": "str",
              "required": false,
              "default": "'python'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Standalone evaluation action (AC: 7).\n\nEvaluates data using the specified evaluator type without running a full loop.\n\nArgs:\n    state: Current workflow state\n    data: Data to evaluate (or from state['reflection_output'])\n    evaluator_type: \"schema\" | \"llm\" | \"custom\"\n    schema: JSON Schema for schema evaluator\n    prompt: Evaluation prompt for LLM evaluator\n    model: LLM model for LLM evaluator\n    examples: Few-shot examples for LLM evaluator\n    run: Code for custom evaluator\n    language: Language for custom evaluator (\"python\", \"lua\", \"prolog\")\n\nReturns:\n    {\n        \"valid\": bool,\n        \"score\": float,\n        \"errors\": List[Dict],\n        \"suggestions\": List[str],\n        \"success\": True\n    }",
          "line_number": 844
        },
        {
          "name": "actions.reflection_evaluate",
          "function": "reflection_evaluate_action",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "evaluator_type",
              "type": "str",
              "required": false,
              "default": "'schema'"
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "model",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "examples",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "run",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "language",
              "type": "str",
              "required": false,
              "default": "'python'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Standalone evaluation action (AC: 7).\n\nEvaluates data using the specified evaluator type without running a full loop.\n\nArgs:\n    state: Current workflow state\n    data: Data to evaluate (or from state['reflection_output'])\n    evaluator_type: \"schema\" | \"llm\" | \"custom\"\n    schema: JSON Schema for schema evaluator\n    prompt: Evaluation prompt for LLM evaluator\n    model: LLM model for LLM evaluator\n    examples: Few-shot examples for LLM evaluator\n    run: Code for custom evaluator\n    language: Language for custom evaluator (\"python\", \"lua\", \"prolog\")\n\nReturns:\n    {\n        \"valid\": bool,\n        \"score\": float,\n        \"errors\": List[Dict],\n        \"suggestions\": List[str],\n        \"success\": True\n    }",
          "line_number": 844
        },
        {
          "name": "reflection.correct",
          "function": "reflection_correct_action",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "errors",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "corrector_action",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "run",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Standalone correction action (AC: 8).\n\nCorrects data based on evaluation errors.\n\nArgs:\n    state: Current workflow state\n    data: Data to correct (or from state['reflection_output'])\n    errors: Evaluation errors (or from state['reflection_errors'])\n    corrector_action: Action to use for correction\n    run: Inline Python code for correction\n    prompt: LLM prompt for correction (uses llm.call)\n\nReturns:\n    {\n        \"corrected_output\": any,\n        \"success\": True\n    }",
          "line_number": 928
        },
        {
          "name": "actions.reflection_correct",
          "function": "reflection_correct_action",
          "parameters": [
            {
              "name": "data",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "errors",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "corrector_action",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "run",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Standalone correction action (AC: 8).\n\nCorrects data based on evaluation errors.\n\nArgs:\n    state: Current workflow state\n    data: Data to correct (or from state['reflection_output'])\n    errors: Evaluation errors (or from state['reflection_errors'])\n    corrector_action: Action to use for correction\n    run: Inline Python code for correction\n    prompt: LLM prompt for correction (uses llm.call)\n\nReturns:\n    {\n        \"corrected_output\": any,\n        \"success\": True\n    }",
          "line_number": 928
        }
      ]
    },
    {
      "file": "retry_actions.py",
      "namespace": "retry",
      "actions": [
        {
          "name": "retry.loop",
          "function": "retry_loop_action",
          "parameters": [
            {
              "name": "validate",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "validate_args",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "correct",
              "type": "str",
              "required": false,
              "default": "''"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "1"
            },
            {
              "name": "retry_delay",
              "type": "float",
              "required": false,
              "default": "0.0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute validation with retry loop (TEA-YAML-005).\n\nThis action wraps a validation action and automatically retries\nwith a correction node on failure. It implements the pattern:\n1. Call validation action\n2. If valid, return success\n3. If invalid and retries remaining:\n   a. Set _retry_errors in state for correction context\n   b. Execute correction node\n   c. Increment _retry_count\n   d. Go to step 1\n4. If invalid and no retries left, return failure\n\nArgs:\n    state: Current workflow state\n    validate: Name of validation action to call (e.g., \"validate.extraction\")\n    validate_args: Arguments to pass to validation action (template-processed)\n    correct: Name of correction node to execute on validation failure\n    max_retries: Maximum number of correction attempts (default: 1)\n    retry_delay: Delay in seconds between retry attempts (default: 0)\n\nReturns:\n    Dict with:\n        - _retry_count: int - Final retry attempt number (0-indexed)\n        - _retry_errors: list - Errors from last validation attempt\n        - _retry_result: dict - Final validation result\n        - _retry_exhausted: bool - True if max retries exceeded\n        - Plus all fields from the final validation result\n\nRaises:\n    ValueError: If validation action not found, correction node not found,\n               or max_retries is invalid\n\nExample:\n    >>> # In YAML:\n    >>> # uses: retry.loop\n    >>> # with:\n    >>> #   validate: validate.extraction\n    >>> #   validate_args:\n    >>> #     entities: \"{{ state.entities }}\"\n    >>> #   correct: fix_extraction\n    >>> #   max_retries: 2",
          "line_number": 45
        },
        {
          "name": "actions.retry_loop",
          "function": "retry_loop_action",
          "parameters": [
            {
              "name": "validate",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "validate_args",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "correct",
              "type": "str",
              "required": false,
              "default": "''"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "1"
            },
            {
              "name": "retry_delay",
              "type": "float",
              "required": false,
              "default": "0.0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute validation with retry loop (TEA-YAML-005).\n\nThis action wraps a validation action and automatically retries\nwith a correction node on failure. It implements the pattern:\n1. Call validation action\n2. If valid, return success\n3. If invalid and retries remaining:\n   a. Set _retry_errors in state for correction context\n   b. Execute correction node\n   c. Increment _retry_count\n   d. Go to step 1\n4. If invalid and no retries left, return failure\n\nArgs:\n    state: Current workflow state\n    validate: Name of validation action to call (e.g., \"validate.extraction\")\n    validate_args: Arguments to pass to validation action (template-processed)\n    correct: Name of correction node to execute on validation failure\n    max_retries: Maximum number of correction attempts (default: 1)\n    retry_delay: Delay in seconds between retry attempts (default: 0)\n\nReturns:\n    Dict with:\n        - _retry_count: int - Final retry attempt number (0-indexed)\n        - _retry_errors: list - Errors from last validation attempt\n        - _retry_result: dict - Final validation result\n        - _retry_exhausted: bool - True if max retries exceeded\n        - Plus all fields from the final validation result\n\nRaises:\n    ValueError: If validation action not found, correction node not found,\n               or max_retries is invalid\n\nExample:\n    >>> # In YAML:\n    >>> # uses: retry.loop\n    >>> # with:\n    >>> #   validate: validate.extraction\n    >>> #   validate_args:\n    >>> #     entities: \"{{ state.entities }}\"\n    >>> #   correct: fix_extraction\n    >>> #   max_retries: 2",
          "line_number": 45
        }
      ]
    },
    {
      "file": "schema_actions.py",
      "namespace": "schema",
      "actions": [
        {
          "name": "schema.merge",
          "function": "schema_merge",
          "parameters": [
            {
              "name": "schemas",
              "type": "List[Dict[str, Any]]",
              "required": true,
              "default": null
            },
            {
              "name": "validate",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "output_key",
              "type": "str",
              "required": false,
              "default": "'merged_schema'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Deep merge multiple JSON Schemas with kubectl-style semantics.\n\nMerge rules:\n- Objects are recursively merged\n- Arrays are replaced (not concatenated)\n- Scalars use last-wins\n- Null can override non-null\n\nArgs:\n    state: Current workflow state\n    schemas: List of schema sources. Each item can be:\n        - {\"path\": \"./local/schema.json\"} - Local file\n        - {\"uses\": \"owner/repo@ref#path\"} - Git reference (requires schema_loader)\n        - {\"uses\": \"s3://bucket/path.json\"} - fsspec URI\n        - {\"inline\": {...}} - Inline schema object\n    validate: Validate merged result against JSON Schema Draft 2020-12\n    output_key: State key to store merged schema\n\nReturns:\n    Dict with 'success', the output_key containing merged schema,\n    and optionally 'validation' results.\n\nExample:\n    >>> result = schema_merge(\n    ...     state={},\n    ...     schemas=[\n    ...         {\"path\": \"./base.json\"},\n    ...         {\"inline\": {\"properties\": {\"extra\": {\"type\": \"string\"}}}}\n    ...     ],\n    ...     validate=True\n    ... )\n    >>> print(result['success'])\n    True",
          "line_number": 42
        },
        {
          "name": "actions.schema_merge",
          "function": "schema_merge",
          "parameters": [
            {
              "name": "schemas",
              "type": "List[Dict[str, Any]]",
              "required": true,
              "default": null
            },
            {
              "name": "validate",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "output_key",
              "type": "str",
              "required": false,
              "default": "'merged_schema'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Deep merge multiple JSON Schemas with kubectl-style semantics.\n\nMerge rules:\n- Objects are recursively merged\n- Arrays are replaced (not concatenated)\n- Scalars use last-wins\n- Null can override non-null\n\nArgs:\n    state: Current workflow state\n    schemas: List of schema sources. Each item can be:\n        - {\"path\": \"./local/schema.json\"} - Local file\n        - {\"uses\": \"owner/repo@ref#path\"} - Git reference (requires schema_loader)\n        - {\"uses\": \"s3://bucket/path.json\"} - fsspec URI\n        - {\"inline\": {...}} - Inline schema object\n    validate: Validate merged result against JSON Schema Draft 2020-12\n    output_key: State key to store merged schema\n\nReturns:\n    Dict with 'success', the output_key containing merged schema,\n    and optionally 'validation' results.\n\nExample:\n    >>> result = schema_merge(\n    ...     state={},\n    ...     schemas=[\n    ...         {\"path\": \"./base.json\"},\n    ...         {\"inline\": {\"properties\": {\"extra\": {\"type\": \"string\"}}}}\n    ...     ],\n    ...     validate=True\n    ... )\n    >>> print(result['success'])\n    True",
          "line_number": 42
        }
      ]
    },
    {
      "file": "search_actions.py",
      "namespace": "search",
      "actions": [
        {
          "name": "memory.grep",
          "function": "memory_grep",
          "parameters": [
            {
              "name": "pattern",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "mode",
              "type": "str",
              "required": false,
              "default": "'like'"
            },
            {
              "name": "file_types",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "paths",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "case_sensitive",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "context_lines",
              "type": "int",
              "required": false,
              "default": "0"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute grep-like search across agent memory.\n\nTEA Custom Action: memory.grep\n\nArgs:\n    state: Current agent state (must contain project_id)\n    pattern: Search pattern\n    mode: 'like' (SQL LIKE), 'regex' (regexp_matches), 'exact' (contains)\n    file_types: Filter by content_type ['yaml', 'json', 'md']\n    paths: Filter by path prefixes\n    case_sensitive: Whether search is case-sensitive (default True)\n    context_lines: Number of lines before/after match to include\n    limit: Maximum number of files to return (default 100)\n    **kwargs: May contain query_engine for dependency injection\n\nReturns:\n    {success: bool, matches: [{file_path, line_number, match_text, context}]}",
          "line_number": 191
        },
        {
          "name": "memory.sql_query",
          "function": "memory_sql_query",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "row_limit",
              "type": "int",
              "required": false,
              "default": "DEFAULT_ROW_LIMIT"
            },
            {
              "name": "timeout_sec",
              "type": "int",
              "required": false,
              "default": "DEFAULT_QUERY_TIMEOUT"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Execute SQL query against agent_memory table with safety controls.\n\nTEA Custom Action: memory.sql_query\n\nArgs:\n    state: Current agent state (must contain project_id)\n    query: SQL SELECT query\n    row_limit: Maximum rows to return (default 1000)\n    timeout_sec: Query timeout in seconds (default 30)\n    **kwargs: May contain query_engine and query_sandbox\n\nReturns:\n    {success: bool, columns: [], rows: [[]], row_count: int}",
          "line_number": 338
        },
        {
          "name": "memory.search_content",
          "function": "memory_search_content",
          "parameters": [
            {
              "name": "field_path",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "operator",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "value",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "file_types",
              "type": "List[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search for files by structured content field values.\n\nTEA Custom Action: memory.search_content\n\nArgs:\n    state: Current agent state (must contain project_id)\n    field_path: JSON path to field (e.g., '$.status', '$.metadata.author')\n    operator: Comparison operator ('=', '!=', 'LIKE', 'IN', 'IS NULL', 'IS NOT NULL')\n    value: Value to compare against (not needed for IS NULL/IS NOT NULL)\n    file_types: Filter by content_type ['yaml', 'json', 'md']\n    limit: Maximum results (default 100)\n    **kwargs: May contain query_engine\n\nReturns:\n    {success: bool, files: [{file_path, field_value, content_type}]}",
          "line_number": 470
        }
      ]
    },
    {
      "file": "secrets_actions.py",
      "namespace": "secrets",
      "actions": [
        {
          "name": "secrets.get",
          "function": "secrets_get",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get a secret value by key.\n\nThis action retrieves a secret from the configured secrets backend.\nIf no secrets backend is configured, falls back to the engine's\nsecrets dictionary (populated from EnvSecretsBackend by default).\n\nArgs:\n    state: Current workflow state (unused but required by action interface)\n    key: Secret key to retrieve\n    default: Default value if secret not found (default: None)\n    **kwargs: Additional parameters (ignored)\n\nReturns:\n    Dictionary with 'value' key containing the secret value or default.\n    Designed to be used with 'output' to store in state.\n\nExample YAML:\n    - uses: secrets.get\n      with:\n        key: API_KEY\n      output: api_key\n\n    - uses: secrets.get\n      with:\n        key: OPTIONAL_KEY\n        default: \"fallback_value\"\n      output: optional_value",
          "line_number": 51
        },
        {
          "name": "secrets.has",
          "function": "secrets_has",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Check if a secret exists.\n\nThis action checks whether a secret key exists in the configured\nsecrets backend. Useful for conditional logic based on optional\nsecrets.\n\nArgs:\n    state: Current workflow state (unused but required by action interface)\n    key: Secret key to check\n    **kwargs: Additional parameters (ignored)\n\nReturns:\n    Dictionary with 'exists' key containing boolean.\n    Designed to be used with 'output' to store in state.\n\nExample YAML:\n    - uses: secrets.has\n      with:\n        key: OPTIONAL_FEATURE_KEY\n      output: has_feature\n\n    - name: use_feature\n      when: \"{{ state.has_feature }}\"\n      run: |\n        # Feature is available\n        pass",
          "line_number": 96
        },
        {
          "name": "actions.secrets_get",
          "function": "secrets_get",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "default",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get a secret value by key.\n\nThis action retrieves a secret from the configured secrets backend.\nIf no secrets backend is configured, falls back to the engine's\nsecrets dictionary (populated from EnvSecretsBackend by default).\n\nArgs:\n    state: Current workflow state (unused but required by action interface)\n    key: Secret key to retrieve\n    default: Default value if secret not found (default: None)\n    **kwargs: Additional parameters (ignored)\n\nReturns:\n    Dictionary with 'value' key containing the secret value or default.\n    Designed to be used with 'output' to store in state.\n\nExample YAML:\n    - uses: secrets.get\n      with:\n        key: API_KEY\n      output: api_key\n\n    - uses: secrets.get\n      with:\n        key: OPTIONAL_KEY\n        default: \"fallback_value\"\n      output: optional_value",
          "line_number": 51
        },
        {
          "name": "actions.secrets_has",
          "function": "secrets_has",
          "parameters": [
            {
              "name": "key",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Check if a secret exists.\n\nThis action checks whether a secret key exists in the configured\nsecrets backend. Useful for conditional logic based on optional\nsecrets.\n\nArgs:\n    state: Current workflow state (unused but required by action interface)\n    key: Secret key to check\n    **kwargs: Additional parameters (ignored)\n\nReturns:\n    Dictionary with 'exists' key containing boolean.\n    Designed to be used with 'output' to store in state.\n\nExample YAML:\n    - uses: secrets.has\n      with:\n        key: OPTIONAL_FEATURE_KEY\n      output: has_feature\n\n    - name: use_feature\n      when: \"{{ state.has_feature }}\"\n      run: |\n        # Feature is available\n        pass",
          "line_number": 96
        }
      ]
    },
    {
      "file": "semtools_actions.py",
      "namespace": "semtools",
      "actions": [
        {
          "name": "semtools.search",
          "function": "semtools_search",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "files",
              "type": "Union[str, List[str]]",
              "required": true,
              "default": null
            },
            {
              "name": "max_distance",
              "type": "float",
              "required": false,
              "default": "0.5"
            },
            {
              "name": "n_results",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "n_lines",
              "type": "int",
              "required": false,
              "default": "0"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "60"
            },
            {
              "name": "check_version",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Semantic search using SemTools CLI.\n\nPerforms local semantic search on text files using model2vec embeddings.\nNo API calls required - runs entirely locally.\n\nArgs:\n    state: Current workflow state\n    query: Semantic search query string\n    files: File path(s) or glob pattern to search in\n    max_distance: Maximum cosine distance (0.0-1.0, lower = more similar).\n                 Default: 0.5. Use 0.3 for stricter matching.\n    n_results: Maximum number of results to return. Default: 10\n    n_lines: Number of context lines around each match. Default: 0\n    timeout: Command timeout in seconds. Default: 60\n    check_version: Whether to verify minimum semtools version. Default: False\n\nReturns:\n    Dictionary with:\n    - success: bool - Whether search completed successfully\n    - matches: List[dict] - List of match objects with file, line, score, text\n    - best_match: dict|None - Highest-scoring match\n    - has_matches: bool - Whether any matches were found\n    - total_matches: int - Number of matches returned\n    - query: str - The search query used\n    - error: str (optional) - Error message if failed\n    - error_type: str (optional) - Error category\n    - install_hint: str (optional) - Installation instructions if CLI missing\n\nExample:\n    >>> result = semtools_search(\n    ...     state={},\n    ...     query=\"error handling exception\",\n    ...     files=\"src/**/*.py\",\n    ...     max_distance=0.3,\n    ...     n_results=5\n    ... )\n    >>> if result[\"has_matches\"]:\n    ...     print(result[\"best_match\"][\"text\"])",
          "line_number": 88
        },
        {
          "name": "actions.semtools_search",
          "function": "semtools_search",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "files",
              "type": "Union[str, List[str]]",
              "required": true,
              "default": null
            },
            {
              "name": "max_distance",
              "type": "float",
              "required": false,
              "default": "0.5"
            },
            {
              "name": "n_results",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "n_lines",
              "type": "int",
              "required": false,
              "default": "0"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "60"
            },
            {
              "name": "check_version",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Semantic search using SemTools CLI.\n\nPerforms local semantic search on text files using model2vec embeddings.\nNo API calls required - runs entirely locally.\n\nArgs:\n    state: Current workflow state\n    query: Semantic search query string\n    files: File path(s) or glob pattern to search in\n    max_distance: Maximum cosine distance (0.0-1.0, lower = more similar).\n                 Default: 0.5. Use 0.3 for stricter matching.\n    n_results: Maximum number of results to return. Default: 10\n    n_lines: Number of context lines around each match. Default: 0\n    timeout: Command timeout in seconds. Default: 60\n    check_version: Whether to verify minimum semtools version. Default: False\n\nReturns:\n    Dictionary with:\n    - success: bool - Whether search completed successfully\n    - matches: List[dict] - List of match objects with file, line, score, text\n    - best_match: dict|None - Highest-scoring match\n    - has_matches: bool - Whether any matches were found\n    - total_matches: int - Number of matches returned\n    - query: str - The search query used\n    - error: str (optional) - Error message if failed\n    - error_type: str (optional) - Error category\n    - install_hint: str (optional) - Installation instructions if CLI missing\n\nExample:\n    >>> result = semtools_search(\n    ...     state={},\n    ...     query=\"error handling exception\",\n    ...     files=\"src/**/*.py\",\n    ...     max_distance=0.3,\n    ...     n_results=5\n    ... )\n    >>> if result[\"has_matches\"]:\n    ...     print(result[\"best_match\"][\"text\"])",
          "line_number": 88
        }
      ]
    },
    {
      "file": "session_actions.py",
      "namespace": "session",
      "actions": [
        {
          "name": "session.create",
          "function": "session_create",
          "parameters": [
            {
              "name": "user_id",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "ttl_hours",
              "type": "int",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Create a new session with expiration.\n\nTEA Custom Action: session.create\n\nAC1: Create new session with expiration:\n- Accepts user_id, ttl_hours (default from config, typically 24)\n- Generates unique session_id\n- Sets expires_at = now() + ttl_hours\n- Creates session metadata doc\n- Returns {success: bool, session_id: str, expires_at: timestamp}\n\nArgs:\n    state: Current agent state\n    user_id: User ID owning this session\n    ttl_hours: Hours until expiration (default from config)\n    **kwargs: Additional arguments\n        metadata_store: MetadataStore instance (optional)\n\nReturns:\n    Dict with success, session_id, expires_at, or error",
          "line_number": 426
        },
        {
          "name": "session.end",
          "function": "session_end",
          "parameters": [
            {
              "name": "session_id",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "End session and archive its memory.\n\nTEA Custom Action: session.end\n\nAC2: Manually end and archive session:\n- Accepts session_id\n- Moves files from sessions/{id}/* to archived-sessions/{id}/*\n- Updates all agent_memory docs for session with archived status\n- Returns {success: bool, archived_files: int}\n\nArgs:\n    state: Current agent state\n    session_id: Session to end\n    **kwargs: Additional arguments\n        metadata_store: MetadataStore instance (optional)\n        blob_storage: BlobStorage instance (optional)\n\nReturns:\n    Dict with success, archived_files, or error",
          "line_number": 512
        },
        {
          "name": "session.archive",
          "function": "session_archive",
          "parameters": [
            {
              "name": "session_id",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "reason",
              "type": "str",
              "required": false,
              "default": "'manual'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Archive session with custom reason.\n\nTEA Custom Action: session.archive\n\nAC3: Manually archive with custom reason:\n- Accepts session_id, reason (string)\n- Same behavior as session.end but with custom archive_reason\n- Used for admin operations or manual cleanup\n\nArgs:\n    state: Current agent state\n    session_id: Session to archive\n    reason: Archive reason (default \"manual\")\n    **kwargs: Additional arguments\n        metadata_store: MetadataStore instance (optional)\n        blob_storage: BlobStorage instance (optional)\n\nReturns:\n    Dict with success, archived_files, or error",
          "line_number": 555
        },
        {
          "name": "session.restore",
          "function": "session_restore",
          "parameters": [
            {
              "name": "session_id",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "new_ttl_hours",
              "type": "int",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Restore archived session.\n\nTEA Custom Action: session.restore\n\nAC7: Restore archived session:\n- Accepts session_id, new_ttl_hours\n- Moves files back from archived-sessions/{id}/* to sessions/{id}/*\n- Sets new expires_at\n- Updates status: \"active\", clears archive fields\n- Returns {success: bool, restored_files: int}\n\nArgs:\n    state: Current agent state\n    session_id: Session to restore\n    new_ttl_hours: New TTL (default from config)\n    **kwargs: Additional arguments\n        metadata_store: MetadataStore instance (optional)\n        blob_storage: BlobStorage instance (optional)\n\nReturns:\n    Dict with success, restored_files, expires_at, or error",
          "line_number": 602
        },
        {
          "name": "session.get",
          "function": "session_get",
          "parameters": [
            {
              "name": "session_id",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get session metadata.\n\nTEA Custom Action: session.get\n\nHelper action to retrieve session state.\n\nArgs:\n    state: Current agent state\n    session_id: Session ID to retrieve\n    **kwargs: Additional arguments\n        metadata_store: MetadataStore instance (optional)\n\nReturns:\n    Dict with success, session data, or error",
          "line_number": 720
        },
        {
          "name": "session.list",
          "function": "session_list",
          "parameters": [
            {
              "name": "user_id",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "status",
              "type": "str",
              "required": false,
              "default": "None"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "List sessions with optional filtering.\n\nTEA Custom Action: session.list\n\nHelper action to list sessions for a user or all sessions.\n\nArgs:\n    state: Current agent state\n    user_id: Filter by user ID (optional)\n    status: Filter by status (\"active\" or \"archived\") (optional)\n    limit: Maximum results (default 100)\n    **kwargs: Additional arguments\n        metadata_store: MetadataStore instance (optional)\n\nReturns:\n    Dict with success, sessions list, or error",
          "line_number": 773
        },
        {
          "name": "session.archive_expired",
          "function": "session_archive_expired",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Archive sessions that have exceeded their TTL.\n\nTEA Custom Action: session.archive_expired\n\nAC4: Archive expired sessions:\n- Queries sessions where expires_at < now() and status = \"active\"\n- Archives each with archive_reason: \"expired\"\n- Returns count of archived sessions\n\nArgs:\n    state: Current agent state\n    **kwargs: Additional arguments\n        metadata_store: MetadataStore instance (optional)\n        blob_storage: BlobStorage instance (optional)\n\nReturns:\n    Dict with archived count and error count",
          "line_number": 835
        }
      ]
    },
    {
      "file": "session_persistence_actions.py",
      "namespace": "session_persistence",
      "actions": [
        {
          "name": "session.load",
          "function": "session_load_action",
          "parameters": [
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "default",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Load session data from the configured session backend.\n\nTEA Custom Action: session.load\n\nRetrieves session data by ID from the engine's session backend.\nReturns default value if session is not found or expired.\n\nArgs:\n    state: Current agent state\n    session_id: Session ID to load. If None, looks for 'session_id' in state.\n    default: Default value if session not found (default: {})\n    **kwargs: Additional arguments\n        _session_backend: SessionBackend instance (injected by engine)\n\nReturns:\n    Dict with loaded session data or default value.\n\nExample YAML:\n    - name: load_session\n      uses: session.load\n      with:\n        session_id: \"{{ state.session_id }}\"\n        default: {\"conversation_history\": []}\n      output: session_data",
          "line_number": 41
        },
        {
          "name": "session.save",
          "function": "session_save_action",
          "parameters": [
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "fields",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "ttl",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Save current state to the session backend.\n\nTEA Custom Action: session.save\n\nPersists state data to the engine's session backend. Can save\nfull state or specific fields only.\n\nArgs:\n    state: Current agent state\n    session_id: Session ID to save to. If None, looks for 'session_id' in state.\n    fields: Optional list of state fields to save. If None, saves entire state\n            (excluding internal fields starting with '_').\n    ttl: Optional TTL in seconds. If None, uses backend default.\n    **kwargs: Additional arguments\n        _session_backend: SessionBackend instance (injected by engine)\n        _session_settings: SessionSettings instance (for default TTL/fields)\n\nReturns:\n    Dict with success status and session_id.\n\nExample YAML:\n    - name: save_session\n      uses: session.save\n      with:\n        session_id: \"{{ state.session_id }}\"\n        fields:\n          - conversation_history\n          - last_question",
          "line_number": 107
        },
        {
          "name": "session.delete",
          "function": "session_delete_action",
          "parameters": [
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Delete a session from the backend.\n\nTEA Custom Action: session.delete\n\nRemoves session data from the storage backend.\n\nArgs:\n    state: Current agent state\n    session_id: Session ID to delete. If None, looks for 'session_id' in state.\n    **kwargs: Additional arguments\n        _session_backend: SessionBackend instance (injected by engine)\n\nReturns:\n    Dict with success status.",
          "line_number": 196
        },
        {
          "name": "session.exists",
          "function": "session_exists_action",
          "parameters": [
            {
              "name": "session_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Check if a session exists.\n\nTEA Custom Action: session.exists\n\nArgs:\n    state: Current agent state\n    session_id: Session ID to check. If None, looks for 'session_id' in state.\n    **kwargs: Additional arguments\n        _session_backend: SessionBackend instance (injected by engine)\n\nReturns:\n    Dict with exists status.",
          "line_number": 233
        }
      ]
    },
    {
      "file": "storage_actions.py",
      "namespace": "storage",
      "actions": [
        {
          "name": "storage.list",
          "function": "storage_list",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "detail",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "max_results",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List files/objects at the given path.\n\nArgs:\n    state: Current state (for template processing)\n    path: Directory/prefix path or URI (e.g., 's3://bucket/prefix/')\n    detail: If True, return full file info; if False, just names (default: False)\n    max_results: Maximum number of results to return (default: None = all)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'files': list, 'success': True} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 57
        },
        {
          "name": "actions.storage_list",
          "function": "storage_list",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "detail",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "max_results",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "List files/objects at the given path.\n\nArgs:\n    state: Current state (for template processing)\n    path: Directory/prefix path or URI (e.g., 's3://bucket/prefix/')\n    detail: If True, return full file info; if False, just names (default: False)\n    max_results: Maximum number of results to return (default: None = all)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'files': list, 'success': True} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 57
        },
        {
          "name": "storage.exists",
          "function": "storage_exists",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if a file/object exists.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., 's3://bucket/key')\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'exists': bool, 'success': True} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 100
        },
        {
          "name": "actions.storage_exists",
          "function": "storage_exists",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Check if a file/object exists.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., 's3://bucket/key')\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'exists': bool, 'success': True} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 100
        },
        {
          "name": "storage.delete",
          "function": "storage_delete",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "recursive",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete a file/object or directory.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., 's3://bucket/key')\n    recursive: If True, delete directory contents recursively (default: False)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'deleted': True, 'success': True, 'path': str} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 127
        },
        {
          "name": "actions.storage_delete",
          "function": "storage_delete",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "recursive",
              "type": "Any",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Delete a file/object or directory.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., 's3://bucket/key')\n    recursive: If True, delete directory contents recursively (default: False)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'deleted': True, 'success': True, 'path': str} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 127
        },
        {
          "name": "storage.copy",
          "function": "storage_copy",
          "parameters": [
            {
              "name": "source",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "destination",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Copy a file/object to another location.\n\nSupports same-provider and cross-provider copies (e.g., S3 to GCS).\n\nArgs:\n    state: Current state (for template processing)\n    source: Source path or URI (e.g., 's3://bucket/src.json')\n    destination: Destination path or URI (e.g., 'gs://bucket/dst.json')\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'copied': True, 'success': True, 'source': str, 'destination': str} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 163
        },
        {
          "name": "actions.storage_copy",
          "function": "storage_copy",
          "parameters": [
            {
              "name": "source",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "destination",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Copy a file/object to another location.\n\nSupports same-provider and cross-provider copies (e.g., S3 to GCS).\n\nArgs:\n    state: Current state (for template processing)\n    source: Source path or URI (e.g., 's3://bucket/src.json')\n    destination: Destination path or URI (e.g., 'gs://bucket/dst.json')\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'copied': True, 'success': True, 'source': str, 'destination': str} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 163
        },
        {
          "name": "storage.info",
          "function": "storage_info",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Get metadata/info about a file/object.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., 's3://bucket/key')\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'info': dict, 'success': True} on success - info contains:\n        - name: Full path name\n        - size: File size in bytes\n        - type: 'file' or 'directory'\n        - modified: Last modified time (if available)\n        - Additional provider-specific metadata\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 232
        },
        {
          "name": "actions.storage_info",
          "function": "storage_info",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Get metadata/info about a file/object.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., 's3://bucket/key')\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'info': dict, 'success': True} on success - info contains:\n        - name: Full path name\n        - size: File size in bytes\n        - type: 'file' or 'directory'\n        - modified: Last modified time (if available)\n        - Additional provider-specific metadata\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 232
        },
        {
          "name": "storage.mkdir",
          "function": "storage_mkdir",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "exist_ok",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Create a directory/prefix.\n\nArgs:\n    state: Current state (for template processing)\n    path: Directory path or URI (e.g., 's3://bucket/prefix/')\n    exist_ok: If True, don't error if directory exists (default: True)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'created': True, 'success': True, 'path': str} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 272
        },
        {
          "name": "actions.storage_mkdir",
          "function": "storage_mkdir",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "exist_ok",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Create a directory/prefix.\n\nArgs:\n    state: Current state (for template processing)\n    path: Directory path or URI (e.g., 's3://bucket/prefix/')\n    exist_ok: If True, don't error if directory exists (default: True)\n    **kwargs: Additional arguments passed to filesystem\n\nReturns:\n    {'created': True, 'success': True, 'path': str} on success\n    {'success': False, 'error': str, 'error_type': str} on failure",
          "line_number": 272
        },
        {
          "name": "storage.native",
          "function": "storage_native",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "operation",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a native filesystem operation not exposed by standard fsspec API.\n\nThis is an escape hatch for provider-specific operations like:\n- S3: put_object_acl, get_object_tagging, etc.\n- GCS: compose, rewrite, etc.\n- Azure: set_blob_metadata, etc.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., 's3://bucket/key')\n    operation: Name of the native filesystem method to call\n    **kwargs: Arguments passed to the native method\n\nReturns:\n    {'result': any, 'success': True, 'operation': str} on success\n    {'success': False, 'error': str, 'error_type': str} on failure\n\nExample:\n    >>> # Set S3 object ACL\n    >>> registry['storage.native'](\n    ...     state={},\n    ...     path=\"s3://my-bucket/file.json\",\n    ...     operation=\"put_object_acl\",\n    ...     ACL=\"public-read\"\n    ... )",
          "line_number": 311
        },
        {
          "name": "actions.storage_native",
          "function": "storage_native",
          "parameters": [
            {
              "name": "path",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "operation",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a native filesystem operation not exposed by standard fsspec API.\n\nThis is an escape hatch for provider-specific operations like:\n- S3: put_object_acl, get_object_tagging, etc.\n- GCS: compose, rewrite, etc.\n- Azure: set_blob_metadata, etc.\n\nArgs:\n    state: Current state (for template processing)\n    path: File path or URI (e.g., 's3://bucket/key')\n    operation: Name of the native filesystem method to call\n    **kwargs: Arguments passed to the native method\n\nReturns:\n    {'result': any, 'success': True, 'operation': str} on success\n    {'success': False, 'error': str, 'error_type': str} on failure\n\nExample:\n    >>> # Set S3 object ACL\n    >>> registry['storage.native'](\n    ...     state={},\n    ...     path=\"s3://my-bucket/file.json\",\n    ...     operation=\"put_object_acl\",\n    ...     ACL=\"public-read\"\n    ... )",
          "line_number": 311
        }
      ]
    },
    {
      "file": "text_actions.py",
      "namespace": "text",
      "actions": [
        {
          "name": "text.insert_citations",
          "function": "text_insert_citations_action",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "required": false,
              "default": "''"
            },
            {
              "name": "references",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'text-embedding-3-large'"
            },
            {
              "name": "api_key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "base_url",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "str",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Wrapper for insert_citations with state access.\n\nInherits provider settings from engine.llm_settings if not explicitly set.\n\nSee insert_citations() for full documentation.",
          "line_number": 387
        },
        {
          "name": "actions.text_insert_citations",
          "function": "text_insert_citations_action",
          "parameters": [
            {
              "name": "text",
              "type": "str",
              "required": false,
              "default": "''"
            },
            {
              "name": "references",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'text-embedding-3-large'"
            },
            {
              "name": "api_key",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "base_url",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "provider",
              "type": "str",
              "required": false,
              "default": "'auto'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Wrapper for insert_citations with state access.\n\nInherits provider settings from engine.llm_settings if not explicitly set.\n\nSee insert_citations() for full documentation.",
          "line_number": 387
        }
      ]
    },
    {
      "file": "textgrad_actions.py",
      "namespace": "textgrad",
      "actions": [
        {
          "name": "learn.textgrad.variable",
          "function": "textgrad_variable",
          "parameters": [
            {
              "name": "name",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "initial_value",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "role_description",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "requires_grad",
              "type": "bool",
              "required": false,
              "default": "True"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Define an optimizable prompt variable (learn.textgrad.variable action).\n\nThis action creates a prompt variable that can be optimized using TextGrad.\nVariables are tracked with version history for state persistence.\n\nArgs:\n    state: Current workflow state\n    name: Unique variable name\n    initial_value: Initial prompt text\n    role_description: Description of the variable's role\n    constraints: List of constraints for optimization\n    requires_grad: Whether variable should be optimized (default: True)\n\nReturns:\n    Dictionary with:\n        - variable: PromptVariable object\n        - variable_name: Name of the variable\n        - textgrad_enabled: Whether TextGrad is active\n\nExample YAML:\n    - name: define_prompt\n      action: learn.textgrad.variable\n      with:\n        name: system_prompt\n        initial_value: \"You are a helpful assistant.\"\n        constraints:\n          - \"Must be polite\"\n          - \"Must be concise\"",
          "line_number": 142
        },
        {
          "name": "learn.textgrad.feedback",
          "function": "textgrad_feedback",
          "parameters": [
            {
              "name": "output",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "evaluation_criteria",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "aspects",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compute textual gradients from output evaluation (learn.textgrad.feedback action).\n\nThis action evaluates an output against criteria and generates structured\nfeedback that can be used as textual gradients for optimization.\n\nArgs:\n    state: Current workflow state\n    output: The output to evaluate\n    evaluation_criteria: List of criteria to evaluate against\n    aspects: Multi-aspect evaluation (accuracy, clarity, safety, etc.)\n    context: Optional context for evaluation\n\nReturns:\n    Dictionary with:\n        - valid: Whether output passes evaluation\n        - feedback: Structured feedback text\n        - scores: Dictionary mapping criteria/aspects to scores (0-1)\n        - gradient_text: Raw textual gradient for optimization\n        - suggestions: List of specific improvement suggestions\n\nExample YAML:\n    - name: evaluate_output\n      action: learn.textgrad.feedback\n      with:\n        output: \"{{ state.response }}\"\n        evaluation_criteria:\n          - \"Response is accurate\"\n          - \"Response is helpful\"\n        aspects:\n          - accuracy\n          - clarity\n          - safety",
          "line_number": 229
        },
        {
          "name": "learn.textgrad.optimize_prompt",
          "function": "textgrad_optimize_prompt",
          "parameters": [
            {
              "name": "variable",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "loss_fn",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "iterations",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Optimize a prompt variable using TextGrad (learn.textgrad.optimize_prompt action).\n\nThis action optimizes a previously defined prompt variable using TextGrad's\ngradient-based approach. It iteratively improves the prompt based on the\nloss function evaluation.\n\nArgs:\n    state: Current workflow state\n    variable: Name of the variable to optimize (from learn.textgrad.variable)\n    loss_fn: Loss function prompt template\n    iterations: Number of optimization iterations (default: from config)\n    constraints: Additional constraints for this optimization\n    context: Optional context for loss function evaluation\n\nReturns:\n    Dictionary with:\n        - optimized_value: Final optimized prompt\n        - initial_value: Original prompt before optimization\n        - variable_name: Name of the optimized variable\n        - iterations_completed: Number of iterations run\n        - improvement_trace: List of improvements per iteration\n        - converged: Whether optimization converged early\n        - version: New version number of the variable\n\nExample YAML:\n    - name: optimize_prompt\n      action: learn.textgrad.optimize_prompt\n      with:\n        variable: system_prompt\n        loss_fn: \"Evaluate if the response is accurate and helpful: {{ state.response }}\"\n        iterations: 3",
          "line_number": 365
        },
        {
          "name": "learn.textgrad.reflection_corrector",
          "function": "textgrad_reflection_corrector",
          "parameters": [
            {
              "name": "variable",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "trigger_threshold",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "optimization_iterations",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Corrector for reflection.loop that uses TextGrad for prompt optimization (AC: 4).\n\nThis action integrates with reflection.loop to automatically trigger\nTextGrad optimization when the reflection loop has failed multiple times.\nIt uses the reflection history as context for gradient computation.\n\nArgs:\n    state: Current workflow state (includes reflection_history)\n    variable: Name of the prompt variable to optimize\n    trigger_threshold: Number of failures before triggering optimization\n    optimization_iterations: Iterations for optimization (default: from config)\n    context: Additional context for optimization\n\nReturns:\n    Dictionary with:\n        - corrected_output: The corrected output (from optimized prompt)\n        - optimization_triggered: Whether optimization was triggered\n        - new_prompt: The optimized prompt (if optimization triggered)\n        - reflection_context_used: Whether reflection history was used\n\nExample YAML (as corrector in reflection.loop):\n    - name: generate_with_learning\n      action: reflection.loop\n      with:\n        generator:\n          action: llm.call\n          prompt: \"{{ state.optimized_prompt }}\"\n        evaluator:\n          type: llm\n          prompt: \"Evaluate quality...\"\n        corrector:\n          action: learn.textgrad.reflection_corrector\n          with:\n            variable: optimized_prompt\n            trigger_threshold: 2\n        max_iterations: 3",
          "line_number": 502
        },
        {
          "name": "textgrad.variable",
          "function": "textgrad_variable",
          "parameters": [
            {
              "name": "name",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "initial_value",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "role_description",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "requires_grad",
              "type": "bool",
              "required": false,
              "default": "True"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Define an optimizable prompt variable (learn.textgrad.variable action).\n\nThis action creates a prompt variable that can be optimized using TextGrad.\nVariables are tracked with version history for state persistence.\n\nArgs:\n    state: Current workflow state\n    name: Unique variable name\n    initial_value: Initial prompt text\n    role_description: Description of the variable's role\n    constraints: List of constraints for optimization\n    requires_grad: Whether variable should be optimized (default: True)\n\nReturns:\n    Dictionary with:\n        - variable: PromptVariable object\n        - variable_name: Name of the variable\n        - textgrad_enabled: Whether TextGrad is active\n\nExample YAML:\n    - name: define_prompt\n      action: learn.textgrad.variable\n      with:\n        name: system_prompt\n        initial_value: \"You are a helpful assistant.\"\n        constraints:\n          - \"Must be polite\"\n          - \"Must be concise\"",
          "line_number": 142
        },
        {
          "name": "textgrad.feedback",
          "function": "textgrad_feedback",
          "parameters": [
            {
              "name": "output",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "evaluation_criteria",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "aspects",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compute textual gradients from output evaluation (learn.textgrad.feedback action).\n\nThis action evaluates an output against criteria and generates structured\nfeedback that can be used as textual gradients for optimization.\n\nArgs:\n    state: Current workflow state\n    output: The output to evaluate\n    evaluation_criteria: List of criteria to evaluate against\n    aspects: Multi-aspect evaluation (accuracy, clarity, safety, etc.)\n    context: Optional context for evaluation\n\nReturns:\n    Dictionary with:\n        - valid: Whether output passes evaluation\n        - feedback: Structured feedback text\n        - scores: Dictionary mapping criteria/aspects to scores (0-1)\n        - gradient_text: Raw textual gradient for optimization\n        - suggestions: List of specific improvement suggestions\n\nExample YAML:\n    - name: evaluate_output\n      action: learn.textgrad.feedback\n      with:\n        output: \"{{ state.response }}\"\n        evaluation_criteria:\n          - \"Response is accurate\"\n          - \"Response is helpful\"\n        aspects:\n          - accuracy\n          - clarity\n          - safety",
          "line_number": 229
        },
        {
          "name": "textgrad.optimize_prompt",
          "function": "textgrad_optimize_prompt",
          "parameters": [
            {
              "name": "variable",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "loss_fn",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "iterations",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Optimize a prompt variable using TextGrad (learn.textgrad.optimize_prompt action).\n\nThis action optimizes a previously defined prompt variable using TextGrad's\ngradient-based approach. It iteratively improves the prompt based on the\nloss function evaluation.\n\nArgs:\n    state: Current workflow state\n    variable: Name of the variable to optimize (from learn.textgrad.variable)\n    loss_fn: Loss function prompt template\n    iterations: Number of optimization iterations (default: from config)\n    constraints: Additional constraints for this optimization\n    context: Optional context for loss function evaluation\n\nReturns:\n    Dictionary with:\n        - optimized_value: Final optimized prompt\n        - initial_value: Original prompt before optimization\n        - variable_name: Name of the optimized variable\n        - iterations_completed: Number of iterations run\n        - improvement_trace: List of improvements per iteration\n        - converged: Whether optimization converged early\n        - version: New version number of the variable\n\nExample YAML:\n    - name: optimize_prompt\n      action: learn.textgrad.optimize_prompt\n      with:\n        variable: system_prompt\n        loss_fn: \"Evaluate if the response is accurate and helpful: {{ state.response }}\"\n        iterations: 3",
          "line_number": 365
        },
        {
          "name": "textgrad.reflection_corrector",
          "function": "textgrad_reflection_corrector",
          "parameters": [
            {
              "name": "variable",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "trigger_threshold",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "optimization_iterations",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Corrector for reflection.loop that uses TextGrad for prompt optimization (AC: 4).\n\nThis action integrates with reflection.loop to automatically trigger\nTextGrad optimization when the reflection loop has failed multiple times.\nIt uses the reflection history as context for gradient computation.\n\nArgs:\n    state: Current workflow state (includes reflection_history)\n    variable: Name of the prompt variable to optimize\n    trigger_threshold: Number of failures before triggering optimization\n    optimization_iterations: Iterations for optimization (default: from config)\n    context: Additional context for optimization\n\nReturns:\n    Dictionary with:\n        - corrected_output: The corrected output (from optimized prompt)\n        - optimization_triggered: Whether optimization was triggered\n        - new_prompt: The optimized prompt (if optimization triggered)\n        - reflection_context_used: Whether reflection history was used\n\nExample YAML (as corrector in reflection.loop):\n    - name: generate_with_learning\n      action: reflection.loop\n      with:\n        generator:\n          action: llm.call\n          prompt: \"{{ state.optimized_prompt }}\"\n        evaluator:\n          type: llm\n          prompt: \"Evaluate quality...\"\n        corrector:\n          action: learn.textgrad.reflection_corrector\n          with:\n            variable: optimized_prompt\n            trigger_threshold: 2\n        max_iterations: 3",
          "line_number": 502
        },
        {
          "name": "actions.textgrad_variable",
          "function": "textgrad_variable",
          "parameters": [
            {
              "name": "name",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "initial_value",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "role_description",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "requires_grad",
              "type": "bool",
              "required": false,
              "default": "True"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Define an optimizable prompt variable (learn.textgrad.variable action).\n\nThis action creates a prompt variable that can be optimized using TextGrad.\nVariables are tracked with version history for state persistence.\n\nArgs:\n    state: Current workflow state\n    name: Unique variable name\n    initial_value: Initial prompt text\n    role_description: Description of the variable's role\n    constraints: List of constraints for optimization\n    requires_grad: Whether variable should be optimized (default: True)\n\nReturns:\n    Dictionary with:\n        - variable: PromptVariable object\n        - variable_name: Name of the variable\n        - textgrad_enabled: Whether TextGrad is active\n\nExample YAML:\n    - name: define_prompt\n      action: learn.textgrad.variable\n      with:\n        name: system_prompt\n        initial_value: \"You are a helpful assistant.\"\n        constraints:\n          - \"Must be polite\"\n          - \"Must be concise\"",
          "line_number": 142
        },
        {
          "name": "actions.textgrad_feedback",
          "function": "textgrad_feedback",
          "parameters": [
            {
              "name": "output",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "evaluation_criteria",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "aspects",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Compute textual gradients from output evaluation (learn.textgrad.feedback action).\n\nThis action evaluates an output against criteria and generates structured\nfeedback that can be used as textual gradients for optimization.\n\nArgs:\n    state: Current workflow state\n    output: The output to evaluate\n    evaluation_criteria: List of criteria to evaluate against\n    aspects: Multi-aspect evaluation (accuracy, clarity, safety, etc.)\n    context: Optional context for evaluation\n\nReturns:\n    Dictionary with:\n        - valid: Whether output passes evaluation\n        - feedback: Structured feedback text\n        - scores: Dictionary mapping criteria/aspects to scores (0-1)\n        - gradient_text: Raw textual gradient for optimization\n        - suggestions: List of specific improvement suggestions\n\nExample YAML:\n    - name: evaluate_output\n      action: learn.textgrad.feedback\n      with:\n        output: \"{{ state.response }}\"\n        evaluation_criteria:\n          - \"Response is accurate\"\n          - \"Response is helpful\"\n        aspects:\n          - accuracy\n          - clarity\n          - safety",
          "line_number": 229
        },
        {
          "name": "actions.textgrad_optimize_prompt",
          "function": "textgrad_optimize_prompt",
          "parameters": [
            {
              "name": "variable",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "loss_fn",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "iterations",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Optimize a prompt variable using TextGrad (learn.textgrad.optimize_prompt action).\n\nThis action optimizes a previously defined prompt variable using TextGrad's\ngradient-based approach. It iteratively improves the prompt based on the\nloss function evaluation.\n\nArgs:\n    state: Current workflow state\n    variable: Name of the variable to optimize (from learn.textgrad.variable)\n    loss_fn: Loss function prompt template\n    iterations: Number of optimization iterations (default: from config)\n    constraints: Additional constraints for this optimization\n    context: Optional context for loss function evaluation\n\nReturns:\n    Dictionary with:\n        - optimized_value: Final optimized prompt\n        - initial_value: Original prompt before optimization\n        - variable_name: Name of the optimized variable\n        - iterations_completed: Number of iterations run\n        - improvement_trace: List of improvements per iteration\n        - converged: Whether optimization converged early\n        - version: New version number of the variable\n\nExample YAML:\n    - name: optimize_prompt\n      action: learn.textgrad.optimize_prompt\n      with:\n        variable: system_prompt\n        loss_fn: \"Evaluate if the response is accurate and helpful: {{ state.response }}\"\n        iterations: 3",
          "line_number": 365
        },
        {
          "name": "actions.textgrad_reflection_corrector",
          "function": "textgrad_reflection_corrector",
          "parameters": [
            {
              "name": "variable",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "trigger_threshold",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "optimization_iterations",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "context",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Corrector for reflection.loop that uses TextGrad for prompt optimization (AC: 4).\n\nThis action integrates with reflection.loop to automatically trigger\nTextGrad optimization when the reflection loop has failed multiple times.\nIt uses the reflection history as context for gradient computation.\n\nArgs:\n    state: Current workflow state (includes reflection_history)\n    variable: Name of the prompt variable to optimize\n    trigger_threshold: Number of failures before triggering optimization\n    optimization_iterations: Iterations for optimization (default: from config)\n    context: Additional context for optimization\n\nReturns:\n    Dictionary with:\n        - corrected_output: The corrected output (from optimized prompt)\n        - optimization_triggered: Whether optimization was triggered\n        - new_prompt: The optimized prompt (if optimization triggered)\n        - reflection_context_used: Whether reflection history was used\n\nExample YAML (as corrector in reflection.loop):\n    - name: generate_with_learning\n      action: reflection.loop\n      with:\n        generator:\n          action: llm.call\n          prompt: \"{{ state.optimized_prompt }}\"\n        evaluator:\n          type: llm\n          prompt: \"Evaluate quality...\"\n        corrector:\n          action: learn.textgrad.reflection_corrector\n          with:\n            variable: optimized_prompt\n            trigger_threshold: 2\n        max_iterations: 3",
          "line_number": 502
        }
      ]
    },
    {
      "file": "tools_actions.py",
      "namespace": "tools",
      "actions": [
        {
          "name": "tools.crewai",
          "function": "tools_crewai",
          "parameters": [
            {
              "name": "tool",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "**params",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a CrewAI tool.\n\nBridges to CrewAI's extensive tool library. The tool parameter can be\na class name (e.g., \"SerperDevTool\") or import path.\n\nArgs:\n    state: Current state dictionary\n    tool: Tool name or import path\n    timeout: Execution timeout in seconds (default: 30.0)\n    **params: Parameters to pass to the tool\n\nReturns:\n    {\"result\": any, \"tool\": str, \"success\": True} on success\n    {\"error\": str, \"error_type\": str, \"tool\": str, \"success\": False} on failure\n\nExample:\n    result = tools_crewai(state, \"SerperDevTool\", query=\"AI news\")",
          "line_number": 763
        },
        {
          "name": "actions.tools_crewai",
          "function": "tools_crewai",
          "parameters": [
            {
              "name": "tool",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "**params",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a CrewAI tool.\n\nBridges to CrewAI's extensive tool library. The tool parameter can be\na class name (e.g., \"SerperDevTool\") or import path.\n\nArgs:\n    state: Current state dictionary\n    tool: Tool name or import path\n    timeout: Execution timeout in seconds (default: 30.0)\n    **params: Parameters to pass to the tool\n\nReturns:\n    {\"result\": any, \"tool\": str, \"success\": True} on success\n    {\"error\": str, \"error_type\": str, \"tool\": str, \"success\": False} on failure\n\nExample:\n    result = tools_crewai(state, \"SerperDevTool\", query=\"AI news\")",
          "line_number": 763
        },
        {
          "name": "tools.mcp",
          "function": "tools_mcp",
          "parameters": [
            {
              "name": "server",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "tool",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "**params",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a tool from an MCP server.\n\nConnects to a Model Context Protocol server and executes the specified tool.\n\nArgs:\n    state: Current state dictionary\n    server: Server configuration dict with 'command', 'args', optionally 'env'\n           Or server name if configured in YAML settings\n    tool: Name of the tool to execute\n    timeout: Execution timeout in seconds (default: 30.0)\n    **params: Parameters to pass to the tool\n\nReturns:\n    {\"result\": any, \"tool\": str, \"server\": str, \"success\": True} on success\n    {\"error\": str, \"error_type\": str, \"tool\": str, \"success\": False} on failure\n\nExample:\n    result = tools_mcp(\n        state,\n        server={\"command\": \"npx\", \"args\": [\"-y\", \"@anthropic/mcp-server-filesystem\"]},\n        tool=\"read_file\",\n        path=\"/tmp/test.txt\"\n    )",
          "line_number": 838
        },
        {
          "name": "actions.tools_mcp",
          "function": "tools_mcp",
          "parameters": [
            {
              "name": "server",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "tool",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "**params",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a tool from an MCP server.\n\nConnects to a Model Context Protocol server and executes the specified tool.\n\nArgs:\n    state: Current state dictionary\n    server: Server configuration dict with 'command', 'args', optionally 'env'\n           Or server name if configured in YAML settings\n    tool: Name of the tool to execute\n    timeout: Execution timeout in seconds (default: 30.0)\n    **params: Parameters to pass to the tool\n\nReturns:\n    {\"result\": any, \"tool\": str, \"server\": str, \"success\": True} on success\n    {\"error\": str, \"error_type\": str, \"tool\": str, \"success\": False} on failure\n\nExample:\n    result = tools_mcp(\n        state,\n        server={\"command\": \"npx\", \"args\": [\"-y\", \"@anthropic/mcp-server-filesystem\"]},\n        tool=\"read_file\",\n        path=\"/tmp/test.txt\"\n    )",
          "line_number": 838
        },
        {
          "name": "tools.langchain",
          "function": "tools_langchain",
          "parameters": [
            {
              "name": "tool",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "**params",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a LangChain tool.\n\nBridges to LangChain's tool ecosystem. The tool parameter can be\na class name (e.g., \"DuckDuckGoSearchRun\") or tool instance.\n\nArgs:\n    state: Current state dictionary\n    tool: Tool name or class name\n    timeout: Execution timeout in seconds (default: 30.0)\n    **params: Parameters to pass to the tool\n\nReturns:\n    {\"result\": any, \"tool\": str, \"success\": True} on success\n    {\"error\": str, \"error_type\": str, \"tool\": str, \"success\": False} on failure\n\nExample:\n    result = tools_langchain(state, \"DuckDuckGoSearchRun\", query=\"AI news\")",
          "line_number": 911
        },
        {
          "name": "actions.tools_langchain",
          "function": "tools_langchain",
          "parameters": [
            {
              "name": "tool",
              "type": "Any",
              "required": true,
              "default": null
            },
            {
              "name": "timeout",
              "type": "Any",
              "required": false,
              "default": "30.0"
            },
            {
              "name": "**params",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Execute a LangChain tool.\n\nBridges to LangChain's tool ecosystem. The tool parameter can be\na class name (e.g., \"DuckDuckGoSearchRun\") or tool instance.\n\nArgs:\n    state: Current state dictionary\n    tool: Tool name or class name\n    timeout: Execution timeout in seconds (default: 30.0)\n    **params: Parameters to pass to the tool\n\nReturns:\n    {\"result\": any, \"tool\": str, \"success\": True} on success\n    {\"error\": str, \"error_type\": str, \"tool\": str, \"success\": False} on failure\n\nExample:\n    result = tools_langchain(state, \"DuckDuckGoSearchRun\", query=\"AI news\")",
          "line_number": 911
        },
        {
          "name": "tools.discover",
          "function": "tools_discover",
          "parameters": [
            {
              "name": "source",
              "type": "Any",
              "required": false,
              "default": "'all'"
            },
            {
              "name": "filter",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "use_cache",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Discover available tools from specified sources.\n\nArgs:\n    state: Current state dictionary\n    source: Source to discover from: \"crewai\", \"mcp\", \"langchain\", \"all\"\n    filter: Optional filter string to narrow results\n    use_cache: Use cached discovery results (default: True)\n    **kwargs: Additional arguments (e.g., mcp_servers for MCP discovery)\n\nReturns:\n    {\n        \"tools\": List[Dict],  # List of tool schemas\n        \"sources\": List[str],  # Sources checked\n        \"count\": int,  # Total tools found\n        \"success\": True\n    }\n\nExample:\n    result = tools_discover(state, source=\"all\")\n    for tool in result['tools']:\n        print(f\"{tool['source']}: {tool['name']}\")",
          "line_number": 990
        },
        {
          "name": "actions.tools_discover",
          "function": "tools_discover",
          "parameters": [
            {
              "name": "source",
              "type": "Any",
              "required": false,
              "default": "'all'"
            },
            {
              "name": "filter",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "use_cache",
              "type": "Any",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Discover available tools from specified sources.\n\nArgs:\n    state: Current state dictionary\n    source: Source to discover from: \"crewai\", \"mcp\", \"langchain\", \"all\"\n    filter: Optional filter string to narrow results\n    use_cache: Use cached discovery results (default: True)\n    **kwargs: Additional arguments (e.g., mcp_servers for MCP discovery)\n\nReturns:\n    {\n        \"tools\": List[Dict],  # List of tool schemas\n        \"sources\": List[str],  # Sources checked\n        \"count\": int,  # Total tools found\n        \"success\": True\n    }\n\nExample:\n    result = tools_discover(state, source=\"all\")\n    for tool in result['tools']:\n        print(f\"{tool['source']}: {tool['name']}\")",
          "line_number": 990
        },
        {
          "name": "tools.clear_cache",
          "function": "tools_clear_cache",
          "parameters": [
            {
              "name": "source",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Clear the tool discovery cache.\n\nArgs:\n    state: Current state dictionary\n    source: Optional source to clear (\"crewai\", \"mcp\", \"langchain\")\n           If None, clears all caches\n\nReturns:\n    {\"cleared\": True, \"source\": str or None}",
          "line_number": 1081
        },
        {
          "name": "actions.tools_clear_cache",
          "function": "tools_clear_cache",
          "parameters": [
            {
              "name": "source",
              "type": "Any",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "dict",
          "docstring": "Clear the tool discovery cache.\n\nArgs:\n    state: Current state dictionary\n    source: Optional source to clear (\"crewai\", \"mcp\", \"langchain\")\n           If None, clears all caches\n\nReturns:\n    {\"cleared\": True, \"source\": str or None}",
          "line_number": 1081
        }
      ]
    },
    {
      "file": "validation_actions.py",
      "namespace": "validation",
      "actions": [
        {
          "name": "validate.extraction",
          "function": "validate_extraction_action",
          "parameters": [
            {
              "name": "entities",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "relationships",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "source_text",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "probes",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "logging",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_name",
              "type": "str",
              "required": false,
              "default": "'unknown'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Validate extracted entities and relationships (AC: 16-18).\n\nRuns all three validation layers:\n1. Structural validation against extraction_schema\n2. Semantic validation using Prolog constraints\n3. LLM grounding using semantic probes\n\nArgs:\n    state: Current workflow state\n    entities: List of extracted entities (or from state['entities'])\n    relationships: List of extracted relationships (or from state['relationships'])\n    source_text: Original source text (or from state['text'])\n    schema: ExtractionSchema config dict (or from engine config)\n    constraints: ValidationConstraints config dict (or from engine config)\n    probes: List of SemanticProbe config dicts (or from engine config)\n    logging: ValidationLoggingConfig dict (or from engine config)\n    agent_name: Agent name for logging\n\nReturns:\n    Dict with:\n        - valid: bool\n        - errors: List of error dicts\n        - validated_at: ISO timestamp",
          "line_number": 44
        },
        {
          "name": "actions.validate_extraction",
          "function": "validate_extraction_action",
          "parameters": [
            {
              "name": "entities",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "relationships",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "source_text",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "probes",
              "type": "Optional[List[Dict[str, Any]]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "logging",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "agent_name",
              "type": "str",
              "required": false,
              "default": "'unknown'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Validate extracted entities and relationships (AC: 16-18).\n\nRuns all three validation layers:\n1. Structural validation against extraction_schema\n2. Semantic validation using Prolog constraints\n3. LLM grounding using semantic probes\n\nArgs:\n    state: Current workflow state\n    entities: List of extracted entities (or from state['entities'])\n    relationships: List of extracted relationships (or from state['relationships'])\n    source_text: Original source text (or from state['text'])\n    schema: ExtractionSchema config dict (or from engine config)\n    constraints: ValidationConstraints config dict (or from engine config)\n    probes: List of SemanticProbe config dicts (or from engine config)\n    logging: ValidationLoggingConfig dict (or from engine config)\n    agent_name: Agent name for logging\n\nReturns:\n    Dict with:\n        - valid: bool\n        - errors: List of error dicts\n        - validated_at: ISO timestamp",
          "line_number": 44
        },
        {
          "name": "validate.generate_prompt",
          "function": "generate_extraction_prompt_action",
          "parameters": [
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "confidence_tracking",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Generate a schema-guided extraction prompt (AC: 23-27).\n\nCreates an extraction prompt that guides the LLM according to\nthe declared ontology (symbolicneural flow).\n\nArgs:\n    state: Current workflow state\n    schema: ExtractionSchema config dict\n    constraints: ValidationConstraints config dict\n    confidence_tracking: Include confidence score instructions\n\nReturns:\n    Dict with:\n        - extraction_prompt: Generated prompt string",
          "line_number": 231
        },
        {
          "name": "actions.validate_generate_prompt",
          "function": "generate_extraction_prompt_action",
          "parameters": [
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "constraints",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "confidence_tracking",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Generate a schema-guided extraction prompt (AC: 23-27).\n\nCreates an extraction prompt that guides the LLM according to\nthe declared ontology (symbolicneural flow).\n\nArgs:\n    state: Current workflow state\n    schema: ExtractionSchema config dict\n    constraints: ValidationConstraints config dict\n    confidence_tracking: Include confidence score instructions\n\nReturns:\n    Dict with:\n        - extraction_prompt: Generated prompt string",
          "line_number": 231
        }
      ]
    },
    {
      "file": "vector_actions.py",
      "namespace": "vector",
      "actions": [
        {
          "name": "memory.vector_search",
          "function": "memory_vector_search",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "top_k",
              "type": "int",
              "required": false,
              "default": "DEFAULT_TOP_K"
            },
            {
              "name": "threshold",
              "type": "float",
              "required": false,
              "default": "DEFAULT_THRESHOLD"
            },
            {
              "name": "content_type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "anchors",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "status",
              "type": "str",
              "required": false,
              "default": "'active'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Semantic search over agent memory using vector similarity.\n\nTEA Custom Action: memory.vector_search\n\nThis action generates an embedding for the query text and performs\nvector similarity search against the configured VectorIndex.\n\nArgs:\n    state: Current agent state (must contain project_id)\n    query: Natural language search query\n    top_k: Maximum number of results (default 10, max 100)\n    threshold: Minimum similarity score 0.0-1.0 (default 0.0)\n    content_type: Filter by content type (yaml, json, md)\n    anchors: Filter by anchors (any match)\n    status: Filter by status (default \"active\")\n    **kwargs: Additional arguments\n        vector_index: VectorIndex instance (optional, uses state if not provided)\n        embedding_fn: Function to generate embeddings (optional)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"file_path\": str,\n                \"content\": str,\n                \"similarity_score\": float,\n                \"anchors\": list,\n                \"summary\": str,\n                ...\n            },\n            ...\n        ],\n        \"count\": int,\n        \"query_tokens\": int\n    }\n\n    On error:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }",
          "line_number": 168
        },
        {
          "name": "memory.vector_search_by_embedding",
          "function": "memory_vector_search_by_embedding",
          "parameters": [
            {
              "name": "embedding",
              "type": "List[float]",
              "required": true,
              "default": null
            },
            {
              "name": "top_k",
              "type": "int",
              "required": false,
              "default": "DEFAULT_TOP_K"
            },
            {
              "name": "threshold",
              "type": "float",
              "required": false,
              "default": "DEFAULT_THRESHOLD"
            },
            {
              "name": "content_type",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "status",
              "type": "str",
              "required": false,
              "default": "'active'"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Search using a pre-computed embedding vector.\n\nTEA Custom Action: memory.vector_search_by_embedding\n\nUseful when you already have an embedding (e.g., from a document)\nand want to find similar documents.\n\nArgs:\n    state: Current agent state (must contain project_id)\n    embedding: Pre-computed embedding vector\n    top_k: Maximum number of results\n    threshold: Minimum similarity score\n    content_type: Filter by content type\n    status: Filter by status\n    **kwargs: Additional arguments\n        vector_index: VectorIndex instance (optional)\n\nReturns:\n    Same format as memory.vector_search",
          "line_number": 349
        },
        {
          "name": "memory.vector_load_data",
          "function": "memory_vector_load_data",
          "parameters": [
            {
              "name": "source",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Load vector data from a Parquet file or URL.\n\nTEA Custom Action: memory.vector_load_data\n\nArgs:\n    state: Current agent state\n    source: Path or URL to Parquet file\n    **kwargs: Additional arguments\n        vector_index: VectorIndex instance (optional)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"loaded\": True,\n        \"source\": str,\n        \"row_count\": int\n    }\n\n    On error:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }",
          "line_number": 478
        },
        {
          "name": "memory.vector_build_index",
          "function": "memory_vector_build_index",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Build or rebuild the vector search index.\n\nTEA Custom Action: memory.vector_build_index\n\nArgs:\n    state: Current agent state\n    **kwargs: Additional arguments\n        vector_index: VectorIndex instance (optional)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"built\": True,\n        \"stats\": dict\n    }\n\n    On error:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }",
          "line_number": 552
        },
        {
          "name": "memory.vector_stats",
          "function": "memory_vector_stats",
          "parameters": [
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Get statistics about the vector index.\n\nTEA Custom Action: memory.vector_stats\n\nArgs:\n    state: Current agent state\n    **kwargs: Additional arguments\n        vector_index: VectorIndex instance (optional)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"stats\": {\n            \"dimensions\": int,\n            \"row_count\": int,\n            \"index_built\": bool,\n            ...\n        }\n    }\n\n    On error:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }",
          "line_number": 615
        },
        {
          "name": "memory.embed",
          "function": "memory_embed",
          "parameters": [
            {
              "name": "content",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "EMBEDDING_MODEL"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Generate embedding for content.\n\nTEA Custom Action: memory.embed\n\nArgs:\n    state: Current agent state\n    content: Text to embed (max ~8000 tokens)\n    model: OpenAI embedding model (default: text-embedding-3-small)\n    **kwargs: Additional arguments\n        embedding_fn: Custom embedding function (optional)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"embedding\": [float, ...],\n        \"model\": str,\n        \"tokens\": int,\n        \"dimensions\": int\n    }\n\n    On error:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }",
          "line_number": 710
        },
        {
          "name": "memory.embed_batch",
          "function": "memory_embed_batch",
          "parameters": [
            {
              "name": "contents",
              "type": "List[str]",
              "required": true,
              "default": null
            },
            {
              "name": "skip_on_error",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Generate embeddings for multiple content strings.\n\nTEA Custom Action: memory.embed_batch\n\nArgs:\n    state: Current agent state\n    contents: List of strings to embed\n    skip_on_error: If True, continue on individual errors\n    **kwargs: Additional arguments\n        embedding_fn: Custom embedding function (optional)\n\nReturns:\n    {\n        \"success\": True,\n        \"embeddings\": [result_dict, ...],\n        \"total\": int,\n        \"succeeded\": int,\n        \"failed\": int,\n        \"total_tokens\": int\n    }",
          "line_number": 821
        },
        {
          "name": "memory.backfill_embeddings",
          "function": "memory_backfill_embeddings",
          "parameters": [
            {
              "name": "batch_size",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "100"
            },
            {
              "name": "project_id",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "dry_run",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Backfill embeddings for documents missing them.\n\nTEA Custom Action: memory.backfill_embeddings\n\nUses MetadataStore and BlobStorage backends injected via kwargs.\n\nArgs:\n    state: Current agent state\n    batch_size: Number of docs to process at once (1-50)\n    limit: Maximum total docs to process (1-500)\n    project_id: Filter by project_id\n    dry_run: If True, report counts but don't generate embeddings\n    **kwargs: Additional arguments\n        metadata_store: MetadataStore instance (required)\n        blob_storage: BlobStorage instance (required)\n        embedding_fn: Custom embedding function (optional)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"processed\": int,\n        \"succeeded\": int,\n        \"failed\": int,\n        \"skipped\": int,\n        \"total_tokens\": int,\n        \"remaining\": int\n    }\n\n    On error:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }",
          "line_number": 883
        }
      ]
    },
    {
      "file": "web_actions.py",
      "namespace": "web",
      "actions": [
        {
          "name": "web.scrape",
          "function": "web_scrape",
          "parameters": [
            {
              "name": "url",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "formats",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "only_main_content",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "wait_for",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30000"
            },
            {
              "name": "actions",
              "type": "Optional[List[Dict]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "extract_schema",
              "type": "Optional[Dict]",
              "required": false,
              "default": "None"
            },
            {
              "name": "extract_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mobile",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "include_tags",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "exclude_tags",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Scrape a URL and extract LLM-ready content via Firecrawl API.\n\nArgs:\n    state: Current workflow state\n    url: URL to scrape\n    formats: Output formats to request. Options: \"markdown\", \"html\",\n            \"links\", \"screenshot\", \"extract\". Default: [\"markdown\"]\n    only_main_content: Extract only main content, removing navigation,\n                      headers, footers, etc. Default: True\n    wait_for: Time in milliseconds to wait for page to load\n    timeout: Request timeout in milliseconds. Default: 30000\n    actions: Browser actions to perform before scraping. Each action is\n            a dict with keys like: type, selector, text, milliseconds.\n            Supported types: click, type, scroll, wait, screenshot\n    extract_schema: JSON Schema for structured data extraction.\n                   Uses LLM to extract data matching the schema.\n    extract_prompt: Natural language prompt for data extraction.\n                   Alternative to schema-based extraction.\n    mobile: Use mobile viewport for scraping. Default: False\n    include_tags: HTML tags to include (e.g., [\"article\", \"main\"])\n    exclude_tags: HTML tags to exclude (e.g., [\"nav\", \"footer\"])\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"url\": str,\n        \"markdown\": str,           # Clean LLM-ready markdown\n        \"html\": str,               # If \"html\" in formats\n        \"links\": List[str],        # If \"links\" in formats\n        \"screenshot\": str,         # Base64 if \"screenshot\" in formats\n        \"extract\": dict,           # If extract_schema/prompt provided\n        \"metadata\": {\n            \"title\": str,\n            \"description\": str,\n            \"language\": str,\n            \"statusCode\": int\n        }\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str  # configuration, rate_limit, timeout, api_error\n    }\n\nExample:\n    >>> result = web_scrape(\n    ...     state={},\n    ...     url=\"https://example.com/article\",\n    ...     formats=[\"markdown\", \"links\"],\n    ...     only_main_content=True\n    ... )\n    >>> if result['success']:\n    ...     print(result['markdown'][:500])",
          "line_number": 95
        },
        {
          "name": "actions.web_scrape",
          "function": "web_scrape",
          "parameters": [
            {
              "name": "url",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "formats",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "only_main_content",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "wait_for",
              "type": "Optional[int]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "30000"
            },
            {
              "name": "actions",
              "type": "Optional[List[Dict]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "extract_schema",
              "type": "Optional[Dict]",
              "required": false,
              "default": "None"
            },
            {
              "name": "extract_prompt",
              "type": "Optional[str]",
              "required": false,
              "default": "None"
            },
            {
              "name": "mobile",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "include_tags",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "exclude_tags",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Scrape a URL and extract LLM-ready content via Firecrawl API.\n\nArgs:\n    state: Current workflow state\n    url: URL to scrape\n    formats: Output formats to request. Options: \"markdown\", \"html\",\n            \"links\", \"screenshot\", \"extract\". Default: [\"markdown\"]\n    only_main_content: Extract only main content, removing navigation,\n                      headers, footers, etc. Default: True\n    wait_for: Time in milliseconds to wait for page to load\n    timeout: Request timeout in milliseconds. Default: 30000\n    actions: Browser actions to perform before scraping. Each action is\n            a dict with keys like: type, selector, text, milliseconds.\n            Supported types: click, type, scroll, wait, screenshot\n    extract_schema: JSON Schema for structured data extraction.\n                   Uses LLM to extract data matching the schema.\n    extract_prompt: Natural language prompt for data extraction.\n                   Alternative to schema-based extraction.\n    mobile: Use mobile viewport for scraping. Default: False\n    include_tags: HTML tags to include (e.g., [\"article\", \"main\"])\n    exclude_tags: HTML tags to exclude (e.g., [\"nav\", \"footer\"])\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"url\": str,\n        \"markdown\": str,           # Clean LLM-ready markdown\n        \"html\": str,               # If \"html\" in formats\n        \"links\": List[str],        # If \"links\" in formats\n        \"screenshot\": str,         # Base64 if \"screenshot\" in formats\n        \"extract\": dict,           # If extract_schema/prompt provided\n        \"metadata\": {\n            \"title\": str,\n            \"description\": str,\n            \"language\": str,\n            \"statusCode\": int\n        }\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str  # configuration, rate_limit, timeout, api_error\n    }\n\nExample:\n    >>> result = web_scrape(\n    ...     state={},\n    ...     url=\"https://example.com/article\",\n    ...     formats=[\"markdown\", \"links\"],\n    ...     only_main_content=True\n    ... )\n    >>> if result['success']:\n    ...     print(result['markdown'][:500])",
          "line_number": 95
        },
        {
          "name": "web.crawl",
          "function": "web_crawl",
          "parameters": [
            {
              "name": "url",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "max_depth",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "formats",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "only_main_content",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "include_paths",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "exclude_paths",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "allow_external_links",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "poll_interval",
              "type": "float",
              "required": false,
              "default": "2.0"
            },
            {
              "name": "max_poll_time",
              "type": "float",
              "required": false,
              "default": "300.0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Crawl a website recursively via Firecrawl API.\n\nStarts a crawl job and polls for completion. Returns content from\nall discovered pages up to the specified limit.\n\nArgs:\n    state: Current workflow state\n    url: Starting URL to crawl\n    max_depth: Maximum link depth to crawl. Default: 2\n    limit: Maximum number of pages to crawl. Default: 10\n    formats: Output formats for each page. Default: [\"markdown\"]\n    only_main_content: Extract only main content. Default: True\n    include_paths: URL patterns to include (e.g., [\"/blog/*\"])\n    exclude_paths: URL patterns to exclude (e.g., [\"/admin/*\"])\n    allow_external_links: Follow links to external domains. Default: False\n    poll_interval: Seconds between status checks. Default: 2.0\n    max_poll_time: Maximum seconds to wait for completion. Default: 300.0\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"pages\": [\n            {\n                \"url\": str,\n                \"markdown\": str,\n                \"metadata\": {...}\n            },\n            ...\n        ],\n        \"total_pages\": int,\n        \"job_id\": str\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample:\n    >>> result = web_crawl(\n    ...     state={},\n    ...     url=\"https://docs.example.com\",\n    ...     max_depth=2,\n    ...     limit=20,\n    ...     include_paths=[\"/docs/*\"]\n    ... )\n    >>> if result['success']:\n    ...     for page in result['pages']:\n    ...         print(f\"Crawled: {page['url']}\")",
          "line_number": 316
        },
        {
          "name": "actions.web_crawl",
          "function": "web_crawl",
          "parameters": [
            {
              "name": "url",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "max_depth",
              "type": "int",
              "required": false,
              "default": "2"
            },
            {
              "name": "limit",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "formats",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "only_main_content",
              "type": "bool",
              "required": false,
              "default": "True"
            },
            {
              "name": "include_paths",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "exclude_paths",
              "type": "Optional[List[str]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "allow_external_links",
              "type": "bool",
              "required": false,
              "default": "False"
            },
            {
              "name": "poll_interval",
              "type": "float",
              "required": false,
              "default": "2.0"
            },
            {
              "name": "max_poll_time",
              "type": "float",
              "required": false,
              "default": "300.0"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Crawl a website recursively via Firecrawl API.\n\nStarts a crawl job and polls for completion. Returns content from\nall discovered pages up to the specified limit.\n\nArgs:\n    state: Current workflow state\n    url: Starting URL to crawl\n    max_depth: Maximum link depth to crawl. Default: 2\n    limit: Maximum number of pages to crawl. Default: 10\n    formats: Output formats for each page. Default: [\"markdown\"]\n    only_main_content: Extract only main content. Default: True\n    include_paths: URL patterns to include (e.g., [\"/blog/*\"])\n    exclude_paths: URL patterns to exclude (e.g., [\"/admin/*\"])\n    allow_external_links: Follow links to external domains. Default: False\n    poll_interval: Seconds between status checks. Default: 2.0\n    max_poll_time: Maximum seconds to wait for completion. Default: 300.0\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"pages\": [\n            {\n                \"url\": str,\n                \"markdown\": str,\n                \"metadata\": {...}\n            },\n            ...\n        ],\n        \"total_pages\": int,\n        \"job_id\": str\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample:\n    >>> result = web_crawl(\n    ...     state={},\n    ...     url=\"https://docs.example.com\",\n    ...     max_depth=2,\n    ...     limit=20,\n    ...     include_paths=[\"/docs/*\"]\n    ... )\n    >>> if result['success']:\n    ...     for page in result['pages']:\n    ...         print(f\"Crawled: {page['url']}\")",
          "line_number": 316
        },
        {
          "name": "web.search",
          "function": "web_search",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "num_results",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'sonar'"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "60"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Perform web search via Perplexity API.\n\nUses Perplexity's sonar model family to search the web and return\nstructured results with citations.\n\nArgs:\n    state: Current workflow state\n    query: Search query string\n    num_results: Maximum number of results to return. Default: 10\n    model: Perplexity model to use. Options:\n           - \"sonar\" (default): Fast, basic search\n           - \"sonar-pro\": Enhanced search\n           - \"sonar-deep-research\": Multi-step retrieval with reasoning\n    timeout: Request timeout in seconds. Default: 60 (increase for deep-research)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"title\": str,\n                \"url\": str,\n                \"snippet\": str,\n                \"position\": int\n            },\n            ...\n        ],\n        \"query\": str,\n        \"total_results\": int,\n        \"answer\": str  # Perplexity's synthesized answer\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample:\n    >>> result = web_search(\n    ...     state={},\n    ...     query=\"latest AI developments 2025\",\n    ...     num_results=5\n    ... )\n    >>> if result['success']:\n    ...     print(result['answer'])\n    ...     for r in result['results']:\n    ...         print(f\"- {r['title']}: {r['url']}\")",
          "line_number": 562
        },
        {
          "name": "actions.web_search",
          "function": "web_search",
          "parameters": [
            {
              "name": "query",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "num_results",
              "type": "int",
              "required": false,
              "default": "10"
            },
            {
              "name": "model",
              "type": "str",
              "required": false,
              "default": "'sonar'"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "60"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Perform web search via Perplexity API.\n\nUses Perplexity's sonar model family to search the web and return\nstructured results with citations.\n\nArgs:\n    state: Current workflow state\n    query: Search query string\n    num_results: Maximum number of results to return. Default: 10\n    model: Perplexity model to use. Options:\n           - \"sonar\" (default): Fast, basic search\n           - \"sonar-pro\": Enhanced search\n           - \"sonar-deep-research\": Multi-step retrieval with reasoning\n    timeout: Request timeout in seconds. Default: 60 (increase for deep-research)\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"results\": [\n            {\n                \"title\": str,\n                \"url\": str,\n                \"snippet\": str,\n                \"position\": int\n            },\n            ...\n        ],\n        \"query\": str,\n        \"total_results\": int,\n        \"answer\": str  # Perplexity's synthesized answer\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str\n    }\n\nExample:\n    >>> result = web_search(\n    ...     state={},\n    ...     query=\"latest AI developments 2025\",\n    ...     num_results=5\n    ... )\n    >>> if result['success']:\n    ...     print(result['answer'])\n    ...     for r in result['results']:\n    ...         print(f\"- {r['title']}: {r['url']}\")",
          "line_number": 562
        },
        {
          "name": "web.ai_scrape",
          "function": "web_ai_scrape",
          "parameters": [
            {
              "name": "url",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "prompt",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "output_schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "60"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "cache",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Extract structured data from a URL using ScrapeGraphAI.\n\nUses AI-powered extraction to parse web pages and return data\nmatching a provided Pydantic/JSON schema. Supports optional LTM\ncaching to avoid redundant API calls (TEA-BUILTIN-008.7).\n\nArgs:\n    state: Current workflow state\n    url: URL to scrape\n    prompt: Natural language prompt describing what to extract\n    output_schema: JSON Schema dict for structured output (inline)\n    schema: Schema configuration with optional `uses` for Git refs\n           or fsspec URIs (s3://, gs://, az://, https://, file://)\n           Supports merging via list of references.\n    timeout: Request timeout in seconds. Default: 60\n    max_retries: Maximum retry attempts for rate limits/server errors. Default: 3\n    cache: Optional cache configuration dict (TEA-BUILTIN-008.7):\n           - enabled: bool (default: False) - Enable caching\n           - ttl_days: int (default: 60) - Cache TTL in days\n           - ttl_hours: int - Cache TTL in hours (overrides ttl_days)\n           - ttl_seconds: int - Cache TTL in seconds (overrides ttl_hours)\n           - key_strategy: str - Cache key generation strategy:\n               * \"url\" (default): Hash of URL only\n               * \"url+schema\": Hash of URL + schema\n               * \"url+prompt\": Hash of URL + prompt\n               * \"url+prompt+schema\": Hash of URL + prompt + schema (most unique)\n           - skip_cache: bool (default: False) - Force fresh scrape, ignore cache\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"data\": {...},           # Extracted data matching schema\n        \"url\": str,\n        \"schema_used\": {...},    # Final merged schema\n        \"_cache_hit\": bool,      # True if result came from cache\n        \"_cache_key\": str,       # Cache key used\n        \"_cache_created_at\": str # ISO timestamp if cache hit\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str  # configuration, api_error, schema_error,\n                           # timeout, rate_limit, dependency, authentication\n    }\n\nSchema Sources (via Story 008.2):\n    - Inline dict via output_schema parameter\n    - Git refs: \"owner/repo@ref#path/to/schema.json\"\n    - Git full URLs: \"git+https://...\" or \"git+ssh://...\"\n    - fsspec URIs: s3://, gs://, az://, https://, file://\n\nSchema Merging (via Story 008.3):\n    When schema.uses is a list, schemas are merged with kubectl-style\n    semantics (last wins). First schema = lowest priority.\n\nCaching (via Story 008.7):\n    When cache.enabled=True, results are stored in Long-Term Memory\n    with configurable TTL. Cache key strategies determine uniqueness:\n    - \"url\": Same URL always returns cached result\n    - \"url+prompt\": Different prompts for same URL get separate cache entries\n    - \"url+schema\": Different schemas for same URL get separate cache entries\n    - \"url+prompt+schema\": Most granular - all three must match\n\nExample:\n    >>> result = web_ai_scrape(\n    ...     state={},\n    ...     url=\"https://example.com/products\",\n    ...     prompt=\"Extract all products with prices\",\n    ...     output_schema={\n    ...         \"type\": \"object\",\n    ...         \"properties\": {\n    ...             \"products\": {\n    ...                 \"type\": \"array\",\n    ...                 \"items\": {\n    ...                     \"type\": \"object\",\n    ...                     \"properties\": {\n    ...                         \"name\": {\"type\": \"string\"},\n    ...                         \"price\": {\"type\": \"string\"}\n    ...                     }\n    ...                 }\n    ...             }\n    ...         }\n    ...     },\n    ...     cache={\"enabled\": True, \"ttl_days\": 30}\n    ... )\n    >>> if result['success']:\n    ...     for product in result['data']['products']:\n    ...         print(f\"{product['name']}: {product['price']}\")",
          "line_number": 913
        },
        {
          "name": "actions.web_ai_scrape",
          "function": "web_ai_scrape",
          "parameters": [
            {
              "name": "url",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "prompt",
              "type": "str",
              "required": true,
              "default": null
            },
            {
              "name": "output_schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "schema",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "timeout",
              "type": "int",
              "required": false,
              "default": "60"
            },
            {
              "name": "max_retries",
              "type": "int",
              "required": false,
              "default": "3"
            },
            {
              "name": "cache",
              "type": "Optional[Dict[str, Any]]",
              "required": false,
              "default": "None"
            },
            {
              "name": "**kwargs",
              "type": "Any",
              "required": false,
              "default": null
            }
          ],
          "return_type": "Dict[str, Any]",
          "docstring": "Extract structured data from a URL using ScrapeGraphAI.\n\nUses AI-powered extraction to parse web pages and return data\nmatching a provided Pydantic/JSON schema. Supports optional LTM\ncaching to avoid redundant API calls (TEA-BUILTIN-008.7).\n\nArgs:\n    state: Current workflow state\n    url: URL to scrape\n    prompt: Natural language prompt describing what to extract\n    output_schema: JSON Schema dict for structured output (inline)\n    schema: Schema configuration with optional `uses` for Git refs\n           or fsspec URIs (s3://, gs://, az://, https://, file://)\n           Supports merging via list of references.\n    timeout: Request timeout in seconds. Default: 60\n    max_retries: Maximum retry attempts for rate limits/server errors. Default: 3\n    cache: Optional cache configuration dict (TEA-BUILTIN-008.7):\n           - enabled: bool (default: False) - Enable caching\n           - ttl_days: int (default: 60) - Cache TTL in days\n           - ttl_hours: int - Cache TTL in hours (overrides ttl_days)\n           - ttl_seconds: int - Cache TTL in seconds (overrides ttl_hours)\n           - key_strategy: str - Cache key generation strategy:\n               * \"url\" (default): Hash of URL only\n               * \"url+schema\": Hash of URL + schema\n               * \"url+prompt\": Hash of URL + prompt\n               * \"url+prompt+schema\": Hash of URL + prompt + schema (most unique)\n           - skip_cache: bool (default: False) - Force fresh scrape, ignore cache\n\nReturns:\n    On success:\n    {\n        \"success\": True,\n        \"data\": {...},           # Extracted data matching schema\n        \"url\": str,\n        \"schema_used\": {...},    # Final merged schema\n        \"_cache_hit\": bool,      # True if result came from cache\n        \"_cache_key\": str,       # Cache key used\n        \"_cache_created_at\": str # ISO timestamp if cache hit\n    }\n\n    On failure:\n    {\n        \"success\": False,\n        \"error\": str,\n        \"error_type\": str  # configuration, api_error, schema_error,\n                           # timeout, rate_limit, dependency, authentication\n    }\n\nSchema Sources (via Story 008.2):\n    - Inline dict via output_schema parameter\n    - Git refs: \"owner/repo@ref#path/to/schema.json\"\n    - Git full URLs: \"git+https://...\" or \"git+ssh://...\"\n    - fsspec URIs: s3://, gs://, az://, https://, file://\n\nSchema Merging (via Story 008.3):\n    When schema.uses is a list, schemas are merged with kubectl-style\n    semantics (last wins). First schema = lowest priority.\n\nCaching (via Story 008.7):\n    When cache.enabled=True, results are stored in Long-Term Memory\n    with configurable TTL. Cache key strategies determine uniqueness:\n    - \"url\": Same URL always returns cached result\n    - \"url+prompt\": Different prompts for same URL get separate cache entries\n    - \"url+schema\": Different schemas for same URL get separate cache entries\n    - \"url+prompt+schema\": Most granular - all three must match\n\nExample:\n    >>> result = web_ai_scrape(\n    ...     state={},\n    ...     url=\"https://example.com/products\",\n    ...     prompt=\"Extract all products with prices\",\n    ...     output_schema={\n    ...         \"type\": \"object\",\n    ...         \"properties\": {\n    ...             \"products\": {\n    ...                 \"type\": \"array\",\n    ...                 \"items\": {\n    ...                     \"type\": \"object\",\n    ...                     \"properties\": {\n    ...                         \"name\": {\"type\": \"string\"},\n    ...                         \"price\": {\"type\": \"string\"}\n    ...                     }\n    ...                 }\n    ...             }\n    ...         }\n    ...     },\n    ...     cache={\"enabled\": True, \"ttl_days\": 30}\n    ... )\n    >>> if result['success']:\n    ...     for product in result['data']['products']:\n    ...         print(f\"{product['name']}: {product['price']}\")",
          "line_number": 913
        }
      ]
    }
  ]
}