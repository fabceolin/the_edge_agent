# NOTE: This example uses Python scripting and is NOT compatible with the Rust runtime.
# For cross-runtime compatibility, use language: lua or language: prolog.

name: perplexity-research-agent
description: Deep research agent using Perplexity API (YAML configuration version)

variables:
  perplexity_model: sonar
  max_search_results: 5

state_schema:
  user_query: str
  search_query: str
  search_results: str
  formatted_report: str

nodes:
  # Node 1: Refine user query for better search
  - name: refine_query
    language: python
    run: |
      # In a real implementation, you might use an LLM here
      # For now, just use the user query as-is
      return {"search_query": state["user_query"]}

  # Node 2: Search using Perplexity API (OpenAI-compatible)
  - name: search_with_perplexity
    language: python
    run: |
      import os
      try:
          from openai import OpenAI
      except ImportError:
          return {
              "search_results": f"Simulated search results for: {state['search_query']}\n\n"
                               "This is a placeholder. Install openai package for real results."
          }

      # Get API key from environment
      api_key = os.getenv("PERPLEXITY_API_KEY")
      if not api_key:
          return {
              "search_results": "Error: PERPLEXITY_API_KEY environment variable not set"
          }

      # Initialize OpenAI-compatible client for Perplexity
      client = OpenAI(
          api_key=api_key,
          base_url="https://api.perplexity.ai"
      )

      # Perform search
      messages = [
          {
              "role": "system",
              "content": "You are a research assistant. Provide comprehensive, well-researched answers."
          },
          {
              "role": "user",
              "content": state["search_query"]
          }
      ]

      # Call Perplexity API
      response = client.chat.completions.create(
          model="sonar",
          messages=messages
      )

      # Extract result
      result_text = response.choices[0].message.content

      return {"search_results": result_text}

  # Node 3: Format the results as a report
  - name: format_report
    language: python
    steps:
      - name: Create header
        run: |
          from datetime import datetime
          title = "# Research Report"
          query_line = f"## Query: {state['user_query']}"
          date_line = f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
          search_line = f"**Search Query**: {state['search_query']}"
          header = f"{title}\n{query_line}\n{date_line}\n{search_line}\n\n---\n\n"
          return {"header": header}

      - name: Combine with results
        run: |
          report = state["header"] + "\n## Findings\n\n" + state["search_results"]
          return {"formatted_report": report}

  # Node 4: Display results
  - name: display_results
    language: python
    run: |
      print("\n" + "=" * 80)
      print(state["formatted_report"])
      print("=" * 80 + "\n")
      return {}

  # Node 5: Save to file (optional)
  - name: save_to_file
    uses: file.write
    with:
      path: ./output/research_{{ state.user_query[:30] }}.md
      content: "{{ state.formatted_report }}"

# Implicit flow: refine_query -> search_with_perplexity -> format_report -> display_results -> save_to_file -> __end__

config:
  raise_exceptions: true
  interrupt_before: []
  interrupt_after: []
