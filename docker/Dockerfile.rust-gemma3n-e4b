# TEA-DIST-002: Docker Rust LLM Image - Gemma 3N E4B
#
# Full TEA Rust image with bundled Gemma 3N E4B GGUF model (~7.5GB).
# Self-contained for offline LLM inference without external model downloads.
#
# Build: docker build -t tea:rust-gemma3n-e4b -f docker/Dockerfile.rust-gemma3n-e4b .
# Size: ~8.5GB target
#
# Model: google/gemma-3n-e4b
# Source: Ollama Registry (https://ollama.com/library/gemma3n)
#
# OPTIMIZATION: Uses cargo-chef for efficient Docker layer caching.
# Dependency layers are cached separately from source code changes.

# =============================================================================
# Stage 1: Chef - Prepare dependency recipe
# =============================================================================
FROM rust:latest AS chef
RUN cargo install cargo-chef --locked
WORKDIR /build

# =============================================================================
# Stage 2: Planner - Analyze dependencies
# =============================================================================
FROM chef AS planner
COPY rust/ ./
RUN cargo chef prepare --recipe-path recipe.json

# =============================================================================
# Stage 3: Builder - Build dependencies (cached) then source
# =============================================================================
FROM chef AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    pkg-config \
    libssl-dev \
    cmake \
    libclang-dev \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /build

# Build dependencies first (this layer is cached if dependencies don't change)
COPY --from=planner /build/recipe.json recipe.json
RUN cargo chef cook --release --recipe-path recipe.json \
    --features "memory,trace,data,web,rag,llm,code,agent,reflection,reasoning,planning,a2a,ltm-duckdb,scryer,llm-local"

# Now copy source and build (only this layer rebuilds on code changes)
COPY rust/ ./
RUN cargo build --release --features "memory,trace,data,web,rag,llm,code,agent,reflection,reasoning,planning,a2a,ltm-duckdb,scryer,llm-local"

RUN ls -lh target/release/tea && \
    ./target/release/tea --version

# =============================================================================
# Stage 2: Runtime - Image with bundled model
# =============================================================================
FROM debian:trixie-slim

RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates \
    libssl3t64 \
    libgomp1 \
    curl \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* \
    && rm -rf /var/cache/apt/archives/*

# Copy the compiled binary
COPY --from=builder /build/target/release/tea /usr/local/bin/tea

# Download model directly in final stage to avoid layer duplication
# This saves ~7.5GB of overlay storage during build
RUN mkdir -p /opt/tea-models && \
    curl -L -o /opt/tea-models/gemma3n-e4b.gguf \
    "https://registry.ollama.ai/v2/library/gemma3n/blobs/sha256:38e8dcc30df4eb0e29eaf5c74ba6ce3f2cd66badad50768fc14362acfb8b8cb6" && \
    ls -lh /opt/tea-models/ && \
    test -f /opt/tea-models/gemma3n-e4b.gguf

# Verify binary
RUN tea --version

# Set environment variables for model
ENV TEA_MODEL_PATH="/opt/tea-models/gemma3n-e4b.gguf"
ENV TEA_MODEL_NAME="gemma3n-e4b"

# Create non-root user for security
RUN useradd -m -s /bin/bash tea \
    && mkdir -p /home/tea/.tea \
    && chown -R tea:tea /home/tea

USER tea
WORKDIR /work

# Labels for container registry
LABEL org.opencontainers.image.source="https://github.com/fabceolin/the_edge_agent"
LABEL org.opencontainers.image.description="TEA Rust Gemma3N-E4B - Full features with bundled Gemma 3N E4B model"
LABEL org.opencontainers.image.licenses="MIT"
LABEL tea.variant="rust-gemma3n-e4b"
LABEL tea.runtime="rust"
LABEL tea.model="gemma3n-e4b"
LABEL tea.model.size="7.5GB"
LABEL tea.model.source="ollama"

ENTRYPOINT ["tea"]
CMD ["--help"]
